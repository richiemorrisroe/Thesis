\documentclass[apa]{article}
\title{Placebo: is deception always necessary?}
\usepackage{graphicx}
\begin{document}
\maketitle
\section{Introduction}
\label{sec:introduction}

<<importdata, echo=FALSE, results=hide>>=
setwd("./ExperimentDataforR/FullStudy/")
expmeasures <- read.csv("explicitmeasuresfixed.csv")
expmeasures[,"LOTR"] <- with(expmeasures, LOTR/6)
vasscores <- read.csv("VASscores.csv"       )
optiat <- read.csv("optiatres.csv" )
tcqiat <- read.csv("tcqiatres.csv")
setwd("../..")

@ 



The commonly accepted model of the placebo effect claims that deception is necessary for the effect to occur. The entire theory behind randomised controlled trials suggests that because participants respond to the mere administration of medicines, then a control (the placebo) is needed to allow the true effect of the drug to be established. A necessary part of this design is that the participants in the placebo group need to believe that there is a chance that they are getting the real treatment. 

Some theorists have argued against this \cite{Deventer2008,Evans2002}, and a recent randomised controlled trial has shown that compared to no treatment, an open placebo can perform significantly better \cite{kaptchuk2010placebos}.

The Kaptchuk \textit{et al} study, which was a three week randomised controlled trial using IBS patients showed that administration of an open-label placebo was associated with a significant improvement (d=0.79) at in the IBS Global Improvement Scale (the major outcome measure) the 21 day endpoint for the study. Similar trends were observed for other symptom severity ($d=0.53$) and quality of life ($d=0.40$)

However, this study did not include a deceptive placebo condition, and so is not a true test of the theory that deception is unnecessary for the placebo response. This study aims to rectify this gap in the literature by putting this theory to the test. 


Additionally,  consistent single-study predictors of the placebo have been difficult to find in  experimental work for over 50 years \cite{Shapiro1997}. Many personality characteristics have been tested to see if they can predict the response, but none have proved replicable \cite{Shapiro1997} Some have argued that placebo responders do not even exist \cite{Kaptchuk2008a}.

The placebo is a badly defined but widely used concept in medicine \cite{Kaptchuk1998,Macedo2003}. Many theories have been put forth to account for its effects, including expectancy, conditioning, emotional change, motivation and meaning \cite{Stewart-Williams2004b, Moerman2006,Vodouris1989,Geers2005a}. However, none of these theories can account for all of the effects observed in research. The conditioning approach gained a following at first \cite{Voudouris1985}  but was quickly opposed by the response expectancy theory of Kirsch \cite{Kirsch1985,Kirsch1997}. 

Some argue that the reasons for this lack of theoretical clarity is that the construct referred to as the placebo response is a collection of disparate underlying specific mechanisms \cite{Benedetti2008}. While certainly there are many different underlying biological processes through which placebo effects are mediated \citep[see][]{Sauro2005} for a meta-analytic review of the effects of endogenous opioids, and the work of Scott and Zubieta \citeauthor*{Scott2007} for some work on the interactions of dopamine with placebo responding, this does not necessitate splitting the construct into multiple independent facets, especially given that there is evidence that these systems can interact with one another. 

The general format for inducing conditioned placebo responses developed by Voudouris and still in use today in the research of Benedetti \cite{Benedetti2006c} and others, consists of three stages. This method is independent of the particular means through which pain is induced, which is often left to the researcher. The only real requirement is that painful stimuli must be controllable by the reducer. 

Firstly, the participant (typically either healthy volunteers or hospital patients) has their pain thresholds calibrated and is given a number of blocks of painful stimuli. Secondly, the participants are either given the same stimuli again following the application of placebo cream or the pain is reduced for the second stage after the application of the cream. The second group are then said to be conditioned by this stage. In the third block, pain is increased again for the conditioned participants, and they typically show a much larger placebo response than those who are merely given verbal suggestions of analgesia.  

These results were believed by their developer to argue in favour of a conditioned approach to the placebo effect. However, an experiment by Montgomery and Kirsch \cite{Montgomery1997} demonstrated that these conditioned effects resulted solely from expectancies,and after a regression in which expectancies were partialled out, there were no significant effects arising from conditioning. This would seem to argue that expectancies are the prime method through which conditioning has an impact, and indeed this is the position of Kirsch. 

Kirschs \cite{Kirsch1985,Kirsch1997} theory of response expectancies specifies that these are the expectation of a non-volitional response, and he argues that they play a role in placebos and hypnosis. This theory relies upon the measurement of self report expectancies to determine this, and this seems to be somewhat incompatible to what happened in the Montgomery and Kirsch research described in the paragraph above. 

In the experiment that confirmed the effects of expectancies, there were two groups who received lowered stimuli to condition them. One of these groups was informed of this pairing, and the other was not. Contrary to the predictions of the conditioned response model, those in the informed pairing group did not show an enhanced placebo effect, while those in the uninformed pairing group did. This seems to indicate that the effects of the conditioning procedure were inhibited by awareness. In other words, what occurred here was an example of implicit learning (learning without conscious awareness) \cite{Wittenbrink2007,DeHouwer2007}.

A third theory, much less influential is the motivational theory \cite{Jensen1991,Geers2005a} which suggests that placebo responses are driven by the participants desire for relief. This would fit with both the observations of Beecher \cite{beecher1955powerful} on the beaches of Normandy, and with the documented fact that placebo responses tend to be larger in clinical populations, who could be assumed to possess a greater desire for relief \cite{Klinger2007a,Amanzio2001}. 

 

This conclusion is further reinforced by the work of Shiv and Carmon \cite{Shiv2005a} on the placebo effects of energy drinks given to participants at a lowered price. This series of three experimental studies at a University using a student population (n=125)  used an outcome measure of the number of problems solved in a specified period, and it demonstrated that those who believed that the price had been discounted solved less puzzles than those who received the drink at the normal price. Additionally, there was an interaction effect between expectancy levels and price conditions, whereby participants with high expectancies showed a much greater mean score between the price conditions.  The interesting feature of this research (for the purposes of this paper) is that when participants attention was drawn to the discounted price (Study 2, n=193 ), the difference between the groups was reduced. This would seem to argue in favour of an implicit learning situation in this experiment also. 

It is worth noting that the results of Shiv \textit{et al} are equally as supportive of a motivational account of placebo, given that one would expect motivation to be lowered by being aware that the drinks were discounted, and thus presumably less valued. 

\section{Implicit Measures }
\label{sec:impl-meas}

One line of evidence which supports the idea that implicit measures may prove useful in the prediction of placebo  is the finding that the Implicit Association Test (IAT) is better at predicting spontaneous behaviour than explicit measures \cite{Conner2005,Hofmann2005}. The IAT outperforms explicit measures in some domains \cite{Greenwald2009} and these domains tend to be where there is little conscious deliberation or reflection upon the matter concerned. The placebo effect is the example \textit{par excellance} of a undeliberated and spontaneous phenomenon, as no one chooses to have such a response, as far as is known. Thus, we can take this as supporting evidence that implicit measures should predict the placebo response more effectively. 

\subsection{Priming and Placebo}
\label{sec:priming-placebo}

Implicit learning is a phenomenon which has come to the attention of psychologists in the past three decades. The research mostly focused on learning of patterns in random letter fragments and in the effects of sub-threshold sounds and pictures upon participants in a research setting.

Additionally, methods of priming such as the scrambled sentence task \cite{bargh1996automaticity} have been shown to impact behaviour in social situations, and additionally in the placebo effect \cite{Jensen1991,Geers2005a}. However, these effects have not been shown in the area of placebo analgesia, and so this study aimed to conceptually replicate this finding so that larger, more reliable placebo effects can be induced by researchers in the field. 

%The investigation of these implicit attitudes was given a huge leap forward by the development of the Implicit Association Test \cite{Greenwald1998}. This computer administered instrument requests the participants to classify words into either trait (race, gender etc) or evaluative (self versus other, pleasant versus unpleasant) categories. This categorisation is performed separately at first, and then one of each type is paired together. This pairing is then reversed in the final step. The test works primarily based on reaction time (in milliseconds) and the assumption underlying the test is that items which are associated are easier to classify together, and that the difference in mean response latencies for classifications of each concept reflect the attitude towards that object. 

Additionally, it appears from recent research that optimism can act as a predictor of the placebo effect \citet{Geers2005}, at least in some circumstances \cite{Hyland2007}. For this reason, the Life Orientation Test (revised) \cite{Scheier1994} was administered to participants in this study. 


To review the research findings above, it appears that there is some evidence that the placebo effect may be influenced by factors outside conscious awareness, that deception may not be necessary and that priming manipulations may prove both effective and practical ways of increasing the size of the effect. 

This study had one major and two minor hypotheses:
\begin{itemize}
\item Firstly, that placebo effects would be equally as large in the open placebo condition as the deceptive placebo condition;

\item Secondly, that implicit measures would add predictive validity to the assessment of placebo response;

\item Finally, that priming (by means of a scrambled sentence task) would increase the size of the placebo effect in both conditions. 
\end{itemize}


% So, reviewing these research findings, we can note that in some cases, the placebo effect seems to emerge without conscious awareness. The Shiv \textit{et al} (2005) study showed that when participants awareness was drawn to the discounted price, the effects disappeared. A similar phenomenon occurred in the Montgomery and Kirsch (1997) \cite{Montgomery1997} study. These research findings suggest that the placebo response is at least partially determined by factors outside conscious awareness, and therefore the rationale for deception is less strong, given that if the placebo effect occurs entirely outside conscious awareness then the awareness or 





One issue that needs to be raised is the issue of inducing placebo responses in participants of the study. The traditional method used is conditioning, but this requires a much more complicated design. A more feasible method for the induction of placebo responses is that of priming, which has been demonstrated to work in recent years \cite{Geers2005a,Jensen1991}. The priming method used in this study was a scrambled sentence task. 


 A secondary aim of this paper was to assess the usefulness of an implicit measure (the Implicit Association Test) in the prediction of placebo response in healthy volunteers \cite{Greenwald1998}. Finally, this paper additionally aimed to replicate the findings of Geers \textit{et al} who showed that placebo responses could be increased by priming of participants towards goals of co-operation \cite{Geers2005a}. This finding has not yet been demonstrated in the placebo response to pain, a deficit which this study aims to rectify. 


% In conclusion, I believe that the expectancies underlying the placebo effect can be measured by means of the Implicit Association Test for the following reasons. Firstly, some placebo effects seem to require a lack of conscious awareness in order to occur. Secondly, the placebo effect seems to be best modelled as a spontaneous phenomenon and the IAT has been shown to predict these kinds of behaviours better. Thirdly, the placebo and the IAT task seem to share some common patterns of neurological activation. 

% Therefore the hypotheses of this paper are as follows:
% \begin{itemize}
% \item Implicit measures of treatment credibility and optimism will be associated with the placebo response to pain in healthy volunteers.

% \item Explicit measures of treatment credibility and optimism will also be associated with the placebo response

% \item Physiological measures (skin conductance) will be a useful predictor of the placebo response
% \end{itemize}

<<packages, echo=FALSE, results=hide>>=
load("tcq2.rda")
load("homdata.rda")
load("credtotals.rda")
require(psych)
require(xtable)
require(arm)
require(ggplot2)
require(reshape2)
require(eRm)
require(ltm)
require(boot)
require(plyr)
require(caret)
require(survival)
source("func.R")

@ 

<<iatsort, echo=FALSE, results=hide>>=
tcqiatsorted <- tcqiat[,c("Participant", "Date", "Time", "Block", "Accurate", "Actual", "Real", "Truth", "Fake", "Illusory", "Inaccurate", "Lies", "Cream", "Injections", "Pills", "Surgery", "Acupuncture", "FlowerEssence", "Homeopathy", "Reiki", "Correct", "BlockTime")]
optiatsorted <- optiat[, c("Participant", "Date", "Time", "Block", "Me", "Mine", "Myself", "Theirs", "Them", "Themselves", "Better", "Happy", "Improving", "Succeeding", "Disimproving", "Failing", "Sad", "Worse", "Correct", "BlockTime")]
@ 


<<optiatscore, echo=FALSE, results=hide>>=
optiatsorted[,"Block"] <- with(optiatsorted, gsub(":", "", x=Block))
optiatscore.mean <- calcIatScores(optiatsorted,Code="Participant", method="mean", words=c("Me", "Mine", "Myself", "Theirs", "Them", "Themselves", "Better", "Happy", "Improving", "Succeeding", "Disimproving", "Failing", "Sad", "Worse"))
names(optiatscore.mean)[1] <- "Participant"
names(optiatscore.mean)[6] <- "OptIAT.Mean"
optstimblock3 <- optiatscore.mean[,grep("Block3.", x=names(optiatscore.mean))]
optstimblock5 <- optiatscore.mean[,grep("Block5.", x=names(optiatscore.mean))]
optiatscore.median <- calcIatScores(optiatsorted,Code="Participant", method="median", words=c("Me", "Mine", "Myself", "Theirs", "Them", "Themselves", "Better", "Happy", "Improving", "Succeeding", "Disimproving", "Failing", "Sad", "Worse"))
names(optiatscore.median)[1] <- "Participant"
names(optiatscore.median)[6] <- "OptIAT.Median"
optiatscore <- merge(optiatscore.mean[c(1,6)], optiatscore.median[,c(1,6)], by="Participant")
part.opt <- unique(optiat[,"Participant"])
part.tcq <- unique(tcqiat[,"Participant"])
optstimblock3 <- as.data.frame(cbind(part.opt, optstimblock3))
optstimblock5 <- as.data.frame(cbind(part.opt, optstimblock5))
@ 

<<tcqtestblocks, echo=FALSE, results=hide>>=
tcqiatsorted[,"Block"] <- with(tcqiatsorted, gsub(":", "", x=Block))
tcqiatscore.mean <- calcIatScores(tcqiatsorted, Code="Participant", method="mean", words=c("Accurate", "Actual", "Real", "Truth", "Fake", "Illusory", "Inaccurate", "Lies", "Cream", "Injections", "Pills", "Surgery", "Acupuncture", "FlowerEssence", "Homeopathy", "Reiki"))
tcqstimblock3 <- tcqiatscore.mean[,grep("Block3.", x=names(tcqiatscore.mean))]
tcqstimblock5 <- tcqiatscore.mean[,grep("Block5.", x=names(tcqiatscore.mean))]
names(tcqiatscore.mean)[1] <- "Participant"
names(tcqiatscore.mean)[6] <- "TCQIAT.Mean"
tcqiatscore.median <- calcIatScores(tcqiatsorted, Code="Participant", method="median", words=c("Accurate", "Actual", "Real", "Truth", "Fake", "Illusory", "Inaccurate", "Lies", "Cream", "Injections", "Pills", "Surgery", "Acupuncture", "FlowerEssence", "Homeopathy", "Reiki"))
names(tcqiatscore.median)[1] <- "Participant"
names(tcqiatscore.median)[6] <- "TCQIAT.Median"
tcqiatscore <- merge(tcqiatscore.mean[,c(1,6)], tcqiatscore.median[,c(1,6)], by="Participant")
part.tcq <- unique(tcqiat[,"Participant"])
tcqstimblock3 <- as.data.frame(cbind(part.tcq, tcqstimblock3))
tcqstimblock5 <- as.data.frame(cbind(part.tcq, tcqstimblock5))
IATscores <- merge(optiatscore, tcqiatscore, by="Participant")
@ 

\section{Methodology}
\label{sec:methodology}

\subsection{Participants}
\label{sec:participants}

All participants were students at the University of the lead researcher (RM). There were 111 participants (62 female, 47 male, 1 did not report gender). The ages of the participants ranged from 17-62 ($x\hat = 22.94$, $sd=6.901$).
All participants volunteered to take part in the study, as an incentive a draw was held where one randomly selected participant won a smartphone. The study was approved by the Social Research Ethics Committee of the lead researchers university. 

\subsection{Materials and Methods}
\label{sec:materials-methods}


The major materials used in this study were as follows:
\begin{itemize}
\item A moisturising cream in a plain pharmaceutical container as a placebo

\item A manual blood pressure gauge and hand exerciser used to induce pain via the submaximal torniuqet technique \cite{moore1979submaximal}. 

\item One IAT for treatment credibility - this IAT used real and fake words as the stimuli, along with names of various treatments \footnote{see Appendix A for details of measures developed for this study} 

\item Another IAT for optimism which used a self/other distinction along with words related to optimism and pessimism

\item The Life Orientation Test, Revised \cite{Scheier1994}

\item The Mindful Attention Absorption Scale \cite{brown2003benefits}

\item A Treatment Credibility Questionnaire (18 items) developed by the researcher (Morrisroe et al, forthcoming). This instrument showed extremely high reliability ($\alpha=0.9$) in two development studies. 

\item The majority ($n=79$) of participants completed a scrambled sentence taks, to prime them to cooperate with the experimental procedure. 
\end{itemize}

\subsection{Experimental Procedure}
\label{sec:exper-proc}

All participants were met at the entrance to the building by the primary researcher. They were given the informed consent documentation, and after they signed it, they completed three questionnaires (the MAAS, the LOT-R and the TCQ).The majority of participants ($n=79$) also completed a scrambled sentence task which aimed to prime them for cooperation in the experiment \footnote{see Appendix A for details}. Such manipulations have been shown to be effective at increasing the size of placebo effects \cite{Geers2005a}.  

Following this, they completed both an Optimism IAT and the Treatment Credibility IAT, where order of administration was counterbalanced across participants.  Next, the participants sat down next to the Biopac physiological monitoring data, and baseline data was recorded for five minutes (300 seconds).  Then, a blood pressure gauge was wrapped around the upper part of the non-dominant hand of the participant, and they were asked to squeeze a hand exerciser twenty times for two seconds each time. One minute after this, and every minute thereafter, participants were asked to rate their pain on a VAS from 1 to 10. If the participant was in the treatment or placebo group, then when they rated their pain as 7 or higher, the placebo cream was applied. The experiment continued until the participant either decided to withdraw, their pain rating reached 10 or 45 minutes elapsed  from when the bandage was applied. ECG and EDA recordings were taken at 1000Hz using the Biopac equipment and VAS ratings were recorded on paper by the experimenter. 

Participants in the treatment group were told that the cream was a potent painkiller, recently approved, which would take effect almost immediately. Participants in the placebo group were told that they were receiving a placebo and that placebos have been clinically proven to reduce pain, and that it would take effect almost immediately. 

\subsection{Statistical Analysis}
\label{sec:statistical-analysis}

All statistical analysis was carried out in R \cite{RDevelopmentCoreTeam2011} version 2.15.1, in Ubuntu 12.04. Packages used included ggplot2 , plyr, caret, psych \cite{Revelle2010}, eRm, ltm and arm \cite{gelman2007data}. After basic demographic statistics were calculated, analysis of variance was used to ensure that all participants were comparable on variables before treatment. No discrepancies were found. Following preliminary graphing, IAT scores were calculated using both means and medians to ensure that outliers did not impact the IAT scores (the correlation between these two measures was greater than 0.9, so only the means are reported in this paper). 

Some authors \cite{Blanton2006a,Blanton2006} have argued that the Implicit Association test is confounded by general processing speed. Therefore, correlations between the critical (Blocks 3 and 5) and non-critical blocks were examined to determine if this was the case for the sample at hand. 

Finally, logistic regression analyses were used to determine the results of the major hypotheses of the study. 

% Next, a cross-validation approach was used to assess the usefulness of the variables as predictors of the placebo response. This approach \cite{friedman2009elements} involves building a statistical model on 9/10ths of the data and testing its accuracy on the remaining tenth. This process was carried out ten times, to provide a better measure of predictive accuracy. This method prevents models which do extremely well on the experimenter\'s dataset, but then fail to generalise to other datasets. In this study, support vector machines, naive bayes and random forest algorithms as well as logistic regression were used. 

\section{Results}
\label{sec:results}

At baseline, none of the three groups (Deceptive Placebo (n=35), Open Placebo (n=39) and No treatment (n=36) differed on any of the variables of interest. This allowed analysis to proceed without adjusting for potential confounders. 

<<selfreportcleanup, echo=FALSE, results=hide>>=
names(vasscores)[1] <-  "Participant"
names(expmeasures)[1] <- "Participant"
vasscores.test <- vasscores[,1:8]
expmeasurescomp <- merge(vasscores.test, expmeasures)
Iatandexpmeasures <- merge(expmeasurescomp, IATscores, by="Participant")
iatexp <- Iatandexpmeasures[,14:23]
Iatandexpmeasures[,"meanconv"] <- with(Iatandexpmeasures, (Pill+Cream+Inj)/3)
Iatandexpmeasures[,"meanalt"] <- with(Iatandexpmeasures, (Acu+Hom+Rei)/3)
Iatandexpmeasures[,"convaltcomp"] <- with(Iatandexpmeasures, meanconv -meanalt)
## Iatandexpmeasures[,"Date"] <- with(Iatandexpmeasures, dmy(Date))
@
 
<<survivaldata, echo=FALSE, results=hide>>=
painratings <- vasscores[,c(9:53)]
napainratings <- apply(painratings,1, function (x)  sum(is.na(x)))
lengthsurv <- 46-napainratings
Iatandexpmeasures[,"lengthsurv"] <- lengthsurv
censor <- Iatandexpmeasures[,"Censored"]
censor2 <- ifelse(censor==c("No", "Left"), 1, 0)
Surv.data <- Surv(lengthsurv,censor2)
@ 




<<paints, echo=FALSE, results=hide>>=
painratings <- vasscores[,c(1,2,8:53)]
painratings.temp <- painratings[,2:length(painratings)]
painratings.trans <- t(painratings.temp)
colnames(painratings.trans) <- as.character(t(painratings[,1]))
painratings.trans <- as.data.frame(painratings.trans)
painratings.trans[,"Time"] <- 1:nrow(painratings.trans)
pain.cond <- ddply(painratings, .(Condition), summarise, PainRatings=apply(painratings[,4:48], 2, mean, na.rm=TRUE))
pain.cond.m <- melt(pain.cond, id.vars="Condition")
@ 

<<meanpaingroup, echo=FALSE, results=hide>>=
deceptivepain <- painratings[with(painratings,Condition=="Treatment"),]
placebopain <- painratings[with(painratings,Condition=="Placebo"),]
notreatpain <- painratings[with(painratings,Condition=="No Treatment"),]
meandeceptivepain <- apply(deceptivepain[,4:48], 2, mean, na.rm=TRUE)
meanplacebopain <- apply(placebopain[,4:48], 2, mean, na.rm=TRUE)
meannotreatpain <- apply(notreatpain[,4:48], 2, mean, na.rm=TRUE)
meangrouppainratings <- cbind(meandeceptivepain, meanplacebopain, meannotreatpain)
meangrouppainratings <- as.data.frame(meangrouppainratings)
meangrouppainratings[,"Time"] <- 1:45
meddeceptivepain <- apply(deceptivepain[,4:48], 2, median, na.rm=TRUE)
medplacebopain <- apply(placebopain[,4:48], 2, median, na.rm=TRUE)
mednotreatpain <- apply(notreatpain[,4:48], 2, median, na.rm=TRUE)
medpainratings <- as.data.frame(cbind(meddeceptivepain, medplacebopain, mednotreatpain))
medpainratings[,"Time"] <- 1:45
dec.pain.mean <- apply(deceptivepain[,4:48], 2, mean, na.rm=TRUE)
open.pain.mean <- apply(placebopain[,4:48], 2, mean, na.rm=TRUE)
notreat.pain.mean <- apply(notreatpain[,4:48], 2, mean, na.rm=TRUE)

painbycond <- data.frame(Deceptive=dec.pain.mean, Open=open.pain.mean, NoTreat=notreat.pain.mean)
painbycond[,"Time"] <- 1:45
painbycond.m <- melt(painbycond, id.vars="Time")
@

Firstly, the implicit measures were examined to ensure that they were not contaminated by general processing speed or by method variance. 


\begin{figure}
<<tcqmeanresp, echo=FALSE, figure=TRUE, eps=TRUE, pdf=TRUE>>=
tcqiat.mean.resp <- ddply(tcqiatsorted, .(Block, Participant), summarise, Correlations=mean(BlockTime, na.rm=TRUE))
## tcqmeanresp.pl <- ggplot(tcqiat.mean.resp, aes(x=MeanResponseTime))+geom_histogram+facet_grid(.~Block)
## print(tcqmeanresp.pl)
tcqiat.mean.m <- melt(tcqiat.mean.resp, id.vars=c("Participant", "Block"))
tcq.iat.c <- dcast(tcqiat.mean.m, Participant+...~Block)
tcq.block.corr.pl <- plotmatrix(tcq.iat.c[,3:length(tcq.iat.c)])+geom_smooth(method="lm")
print(tcq.block.corr.pl)
@    
\caption{Correlations between Block Scores for Treatment Credibility IAT with linear regression smooth line}
\label{fig:tcqblockcorr}
\end{figure}

As can be seen from Figure \ref{fig:tcqblockcorr}, the correlations are relatively low between most of the blocks, though somewhat higher between blocks 3 and 5. Table \ref{tab:tcqcormat} gives the exact Kendall\'s $\tau$ between each of the blocks. As can be seen the correlations hover between 0.3 and 0.4, which is in line with expectations prior to the experiment. 

<<cormatrixtcqiat, echo=FALSE, results=tex>>=
cormat <- corr.test(tcq.iat.c[,3:length(tcq.iat.c)], method="kendall")[["r"]]
print(xtable(cormat, label="tab:tcqcormat", caption="Correlations between the blocks of the treatment credibility IAT (Kendall\'s $\tau$. All correlations are significant at the p<0.001 level"))

@ 
 

Next, the same process is repeated for the Optimism IAT. 
\begin{figure}
<<tcqmeanresp, echo=FALSE, figure=TRUE>>=
optiat.mean.resp <- ddply(optiatsorted, .(Block, Participant), summarise, Correlations=mean(BlockTime, na.rm=TRUE))
optiat.mean.m <- melt(optiat.mean.resp, id.vars=c("Participant", "Block"))
opt.iat.c <- dcast(optiat.mean.m, Participant+...~Block)
opt.block.corr.pl <- plotmatrix(opt.iat.c[,3:length(opt.iat.c)])+geom_smooth(method="lm")
print(opt.block.corr.pl)

@    
   \caption{Correlations between Block Scores for Optimism IAT with linear regression smooth line}
   \label{fig:optblockcorr}
 \end{figure}

As shown in Figure \ref{fig:optblockcorr}, the correlations between blocks are moderate, though highest in blocks 3 and 5, as was seen for the Treatment Credibility IAT. Table \ref{tab:optcormat}. The correlations are a little higher than for the Treatment Credibility IAT, but still within an acceptable range. Its interesting to note that (with the exception of Block 5), the correlations are strongest between adjacent blocks, and drop off as the blocks move further apart, suggesting that the autocorrelation theory has some merit. 

<<cormatrixtcqiat, echo=FALSE, results=tex>>=
opt.cormat <- corr.test(opt.iat.c[,3:length(tcq.iat.c)], method="kendall")[["r"]]
print(xtable(opt.cormat, label="tab:optcormat", caption="Correlations between the blocks of the Optimism IAT (Kendalls $\tau$. All correlations are significant at the p<  0.001 level"))
@ 

The next question with regard to the IAT's is whether or not the non-critical blocks (that is, Blocks 1, 2 and 4) will be correlated. Given that these were administered in counterbalanced order and there was a small gap between them one would expect there to be much lower correlations between these blocks of the IAT's. These correlations (if present) should provide an index of general processing speed, and may be useful as predictor variables for some of the other measures. 

<<opttcqiatcorr, echo=FALSE, results=hide>>=
curnames <- names(opt.iat.c)
curnames.bl <- curnames[3:length(curnames)]
curnames.bl2 <- paste("Opt", curnames.bl, sep="")
curnames.d <- c(curnames[1:2], curnames.bl2)
names(opt.iat.c) <- curnames.d
curnames.tcq <- names(tcq.iat.c)
curnames.bl.tcq <- curnames[3:length(curnames)]
curnames.bl2.tcq <- paste("TCQ", curnames.bl.tcq, sep="")
curnames.d.tcq <- c(curnames.tcq[1:2], curnames.bl2.tcq)
names(tcq.iat.c) <- curnames.d.tcq
iat.block.merge <- merge(tcq.iat.c, opt.iat.c, by="Participant")
iat.block.merge2 <- iat.block.merge[, -1*c(2, 5,7,8,11,13)]

@ 

\begin{figure}
  
<<corriattcqopt, echo=FALSE, figure=TRUE>>=
corr.iat.tcq.opt.pl <- plotmatrix(iat.block.merge2[,2:length(iat.block.merge2)])+geom_smooth(method="lm")

print(corr.iat.tcq.opt.pl)
@   
  \caption{Correlations between Non Critical Blocks of Optimism and Treatment Credibility IAT}
  \label{fig:corriattcqopt}
\end{figure}

As can be seen from Figure \ref{fig:corriattcqopt}, there were correlations between the two IAT's. These correlations, while significant, were quite low ($r\bar =0.20$) which equates to about 4\% of the variance. Therefore the two explicit measures can be safely be regarded as not being contaminated by method variance \cite{Mierke2003}. 


\subsection{Testing the Major Hypotheses}
\label{sec:test-major-hypoth}

Before moving on to the main thrust of the analyses, the differences between the three groups were examined in terms of pain using t-tests. 

<<tsimport, echo=FALSE, results=hide>>=
physfiles <- fileImport("ExperimentDataforR/FullStudy/PhysMeasures/Richie1ps", pattern=".txt$")
gsr.mat <- listToDf(physfiles, 1)

ecg.mat <- listToDf(physfiles, 2)
gsrnames <- colnames(gsr.mat)
ecgnames <- colnames(ecg.mat)
names(gsr.mat) <- NULL
gsr.mat.df <- as.data.frame(gsr.mat) #plot these, look at mean gsr scores as a predictor of placebo use variances also
gsr.mean <- colSums(gsr.mat.df, na.rm=TRUE)
gsr.mean2 <- as.data.frame(gsr.mean)
gsrnames2 <- gsub("GSR", "", gsrnames)
gsr.mean2[,"Participant"] <- gsrnames2
gsr.mat.t <- t(gsr.mat)
names(ecg.mat) <- NULL
ecg.mat.t <- t(ecg.mat)
gsr.df <- as.data.frame(gsr.mat.t)
gsr.df[,"Participant"] <- gsrnames2
Iatandexpmeasures.phys <- merge(Iatandexpmeasures, gsr.mean2, by="Participant")
@

<<meanpain, echo=FALSE, results=hide>>=
painmean <- rowMeans(vasscores[,9:53], na.rm=TRUE)
painmeanpart <- data.frame(Participant=vasscores[,"Participant"], painmean)
Iatandexpmeasures.pain <- merge(Iatandexpmeasures.phys, painmeanpart, by="Participant")
@ 

Surprisingly enough, there was no significant mean difference between the Deceptive and No Treatment conditions ($t=0.353, df=53.553, p=0.7255$). However, the differences between the Open Placebo group and the Deceptive Treatment group ($t=-3.422, df=53.27, p=0.0012, d=1.11$) were,  as was the difference between the  Open Placebo group and the No Treatment group ($t=-3.9154, df=87.996, p=0.0001, d=1.22$). These effects both qualify as large effects by Cohen's standards, and would match those effects found earlier in the field and assessed through meta-analysis \cite{Sauro2005}.

This would seem to support the major hypothesis of this study, that deception is not necessary to induce a placebo response. 



\begin{figure}
  
<<placprimeplot, echo=FALSE, figure=TRUE>>=
plac.prime.pl <- ggplot(Iatandexpmeasures, aes(x=PlacResp))+geom_histogram()+facet_grid(.~Prime)
print(plac.prime.pl)
@   
  \caption{Proportion of Placebo Response in Primed and Non-Primed Conditions}
  \label{fig:placprimeplot}
\end{figure}


The priming manipulation appeared to achieve its intended purpose, as Figure \ref{fig:placprimeplot} shows. A logistic regression (shown in Table \ref{tab:primelogreg}) demonstrated that the effects of priming were significant (p=0.0452), validating hypothesis three of the study. 

<<primelogreg, echo=FALSE, results=tex>>=
prim.logreg <- glm(PlacResp~Prime, data=Iatandexpmeasures, family=binomial(link="logit"))
print(xtable(prim.logreg, label="tab:primelogreg", caption="Results of Logistic Regression of Priming on Placebo Response"))
@ 




Next, the relationship between the placebo response and proposed predictors were examined graphically. 



\begin{figure}
<<ggplothistcond, echo=FALSE, fig=TRUE, eps=TRUE, pdf=TRUE>>=
plac.resp.hist.pl <- ggplot(Iatandexpmeasures, aes(x=TCQIAT.Mean))+geom_density()+facet_grid(.~PlacResp)
print(plac.resp.hist.pl)
@   
  \caption{TCQIAT.Mean against Pain Responses Over Time}
  \label{fig:histresp}
\end{figure}

From Figure \ref{fig:histresp}, it can be seen that the participants who did respond to placebo had marginally higher Treatment Credibility IAT scores than those who did not. 

\begin{figure}
<<optiatcorrsurv, echo=FALSE, figure=TRUE, eps=TRUE, pdf=TRUE>>=
optiat.histcond <- ggplot(Iatandexpmeasures, aes(x=OptIAT.Mean))+geom_density()+facet_grid(.~PlacResp)
print(optiat.histcond)
@   
  \caption{Density Plots of Optimism IAT Scores by Condition}
  \label{fig:opthistcond}
\end{figure}


Again, it can be seen from Figure \ref{fig:opthistcond} that those who responded to placebo had higher optimism IAT scores than  those who did not, suggesting that something about the IAT is predictive of placebo response. 


\begin{figure}
<<ggplotplacyesno, echo=FALSE, fig=TRUE, eps=TRUE, pdf=TRUE>>=
placrespyes <- painratings[with(painratings,PlacResp=="Yes"),]
placrespno <- painratings[with(painratings,PlacResp=="No"),]
placresyesmean <- apply(placrespyes[4:48], 2, mean, na.rm=TRUE)
placresnomean <- apply(placrespno[4:48], 2, mean, na.rm=TRUE)
placresyesnopain <- as.data.frame(cbind(placresyesmean, placresnomean))
placresyesnopain$Time <- 1:45
placres.melt <- melt(placresyesnopain, id="Time")
placresplot <- ggplot(placres.melt, aes(x=Time, y=value, group=variable, colour=variable))+geom_line() +geom_smooth(method="loess")
print(placresplot)
@   
  \caption{Pain Ratings of Participants by Response to Placebo Across Time. Straight line is a loess smoother, the jagged line represents the actual pain levels}
  \label{fig:placyesno}
\end{figure}

A number of findings are apparent from the plot above in Figure \ref{fig:placyesno}. The placebo effect was approximately equivalent to a 15\% decrease in pain (read from the graph at the point the no response participants pain reached seven). This is a relatively large effect, and adds confidence to the significant results for modelling reported below. In addition, the participants who responded to placebo tended to remain in the experiment for a longer period of time (which is intuitively obvious). 


\begin{figure}
  \centering
<<ggplot3wayinter, echo=FALSE, fig=TRUE, eps=TRUE, pdf=TRUE>>=
iatexpno <- Iatandexpmeasures[with(Iatandexpmeasures,PlacResp=="No"),]
iatexpyes <- Iatandexpmeasures[with(Iatandexpmeasures,PlacResp=="Yes"),]
iatexpno <- na.omit(iatexpno)
iatexpyes <- na.omit(iatexpyes)
iatexpyesno <- rbind(iatexpyes, iatexpno)
lotriatplot <- ggplot(iatexpyesno, aes(x=LOTR, y=TCQIAT.Mean , colour=PlacResp, size=OptIAT.Mean))+geom_point()## +geom_smooth(method="glm", formula="PlacResp~TCQIAT.Mean*OptIAT.Mean*LOTR")
print(lotriatplot)
@  
  \caption{Three Way Interaction Plot between Treatment Credibility IAT, Optimism IAT and LOTR against Placebo Response}
  \label{fig:3wayinter}
\end{figure}


The plot above in Figure \ref{fig:3wayinter} indicates that there appears to be a non linear interaction between the Treatment Credibility IAT, the Optimism IAT and the LOT-R. It can be seen that extremely high scores on both the treatment credibility questionnaire and the Life Orientation test appear to be associated with not responding to placebo, while moderate levels of all three variables appear to provide the greatest likelihood of placebo response. % This nonlinear relationship was the impetus to widen the kinds of models fit to the data than is common in this area. 



<<placmodlog, echo=FALSE, results=tex>>=
placmod.int <- glm(PlacResp~TCQIAT.Mean*OptIAT.Mean*LOTR, data=Iatandexpmeasures.phys, family=binomial(link="logit"))
print(xtable(summary(placmod.int), label="tab:placmodint", caption="Logistic Regression of IAT measures and optimism on the Placebo Response"))                   
@ 

From Table \ref{tab:placmodint}, it can be seen that the effect of the treatment credibility IAT was significant, validating the major hypothesis of the study. The effect of explicit Optimism was marginally significant, as were the two and three way interactions between these variables. 

Next, the mean GSR for each participant was included in the model, and allowed to interact with all the other variables in the model. 

<<placmodint2, echo=FALSE, results=tex>>=
placmod.int2 <- glm(PlacResp~(TCQIAT.Mean*OptIAT.Mean*LOTR*gsr.mean)^2, data=Iatandexpmeasures.phys, family=binomial(link="logit"))
print(xtable(summary(placmod.int2), label="tab:placmodint2", caption="Regression of IAT on Treatment Credibility IAT, Optimism IAT, Optimism and Mean GSR"))
@ 

As shown in Table \ref{tab:placmodint2}, the addition of the mean GSR for each participant causes the individual effects of Optimism, Optimism IAT and the Treatment Credibility IAT to become significant, al;ong with the two and three way interactions. The psuedo $R^2$ of this model is equal to 0.39, which is quite good. However, given the lack of individual significance for the predictors (with the exception of mean GSR), this would suggest that there are some problems with the model. Indeed, an examination of variance inflation factors for this model shows that multicollinearity is a major issue for this model, which suggests that the significance of these coefficients should be regarded as suspect. 

% To control for possible unequal error variance and to tease out the effects by Condition, a generalised linear mixed model was fitted using the previous model as a baseline. Condition and Prime were included as random effects, to assess if their effects differed across conditions. 

## <<glmm1, echo=FALSE, results=tex>>=
## placmodlmer.pain <- lmer(PlacResp~+TCQIAT.Mean*OptIAT.Mean*LOTR+(1|Prime)+(1|Condition), data=Iatandexpmeasures.pain, family=binomial(link="logit"), control=list(maxIter=10000))

@ 
 
% \subsection{Machine Learning and the Placebo Response}
% \label{sec:mach-learn-plac}

% Firstly, the data was split into two random sections, with 80\% of the data utilised to train the models, and 20\% held aside for a final validation of any model's predictive power. Additionally, the training set was split into ten sections, of which nine were used to train the model and one was used to test the model. This allowed for the accuracy of each model to be ranked in a preliminary fashion, though the ultimate arbiter was performance on the training set. 


<<testandtrain, echo=FALSE, results=hide>>=
require(doMC)
registerDoMC(2)
iatandexpfull <- Iatandexpmeasures.phys[,c("PlacResp", "Age", "Gender", "Prime", "LOTR", "MAAS", "Pill", "Cream", "Inj", "Acu", "Hom", "Rei", "OptIAT.Mean", "OptIAT.Median", "TCQIAT.Mean", "TCQIAT.Median", "meanconv", "meanalt", "convaltcomp", "lengthsurv", "gsr.mean")]
iatandexpfull <- na.omit(iatandexpfull)
iatandexpfull.train.ind <- with(iatandexpfull,createDataPartition(PlacResp, p=0.8, list=FALSE))
iatandexpfull.train <- iatandexpfull[iatandexpfull.train.ind,]
iatandexpfull.test <- iatandexpfull[-iatandexpfull.train.ind,]
mytrain <- trainControl(method="repeatedcv", number=10, repeats=25)
@ 


<<rftrain, echo=FALSE, results=hide, eval=FALSE>>=
randf.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv+gsr.mean, data=iatandexpfull.train, method="rf", trControl=mytrain, tuneGrid=data.frame(.mtry=1:10), preProcess=c("center", "scale"))
randf.pred <- predict(randf.train, iatandexpfull.test)
@ 

<<nbtrain, echo=FALSE, results=hide,eval=FALSE>>=
## nb.train <- train(PlacResp~., data=iatandexpfull.train, method="nb", trControl=mytrain, tuneGrid=data.frame(.usekernel=c(TRUE, FALSE), .fL=0:1), preProcess=c("center", "scale"))
nb.train.pc <- train(PlacResp~., data=iatandexpfull.train, method="nb", trControl=mytrain, tuneGrid=data.frame(.usekernel=c(TRUE, FALSE), .fL=0:1), preProcess=c("center", "scale", "pca"))
@

<<knntrain, echo=FALSE, results=hide,eval=FALSE>>=
knn.train.pc <- train(PlacResp~. , data=iatandexpfull.train, method="knn", trControl=mytrain, tuneGrid=data.frame(.k=1:10), 
                   preProcess=c("center", "scale", "pca"))
knn.train <- train(PlacResp~. , data=iatandexpfull.train, method="knn", trControl=mytrain, tuneGrid=data.frame(.k=1:10), 
                   preProcess=c("center", "scale"))
@ 

<<svmtrain, echo=FALSE, results=hide,eval=FALSE>>=
## svm.train.pc <- train(PlacResp~., data=iatandexpfull.train, method="svmLinear", trControl=mytrain, tuneGrid=data.frame(.C=1:10), preprocess=c("center", "scale", "pca"))
## svm.train <- train(PlacResp~., data=iatandexpfull.train, method="svmLinear", trControl=mytrain, tuneGrid=data.frame(.C=1:10), preprocess=c("center", "scale"))
@ 

<<svmradialtrain, echo=FALSE, results=hide,eval=FALSE>>=
#svmradial.train <- train(PlacResp~., data=iatandexpfull.train, method="lssvmRadial", trControl=mytrain, preProcess=c("center", "scale", "pca"))
@ 

<<gbmtrain, echo=FALSE, results=hide,eval=FALSE>>=
gbm.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+LOTR+meanconv+gsr.mean, data=iatandexpfull.train, method="gbm", trControl=mytrain, tuneGrid=data.frame(.shrinkage=(c(0.1, 0.01, 0.001)), .n.trees=c(50,100,150), .interaction.depth=3))
@ 

<<gamboost, echo=FALSE, results=hide,eval=FALSE>>=
gamboost.train <- train(PlacResp~., data=iatandexpfull.train, method="gamboost", trControl=mytrain, tuneGrid=data.frame(.mstop=1:100, .prune=1:100))
@ 

<<glmboost, echo=FALSE, results=hide, eval=FALSE>>=
glmboost.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv+gsr.mean, data=iatandexpfull.train, method="glmboost", trControl=mytrain, tuneGrid=data.frame(.mstop=1:100, .prune=1:100))
@ 

% <<logitboost, echo=FALSE, results=hide,eval=FALSE>>=
% logitboost.train <-  train(PlacResp~., data=iatandexpfull.train, method="logitBoost", trControl=mytrain, tuneGrid=data.frame(.nIter=seq(10,500, by=10)))
@ 

<<glmnettrain, echo=FALSE, results=hide, eval=FALSE>>=
glmnet.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv+gsr.mean, data=iatandexpfull.train, method="glmnet", trControl=mytrain, tuneGrid=data.frame(.alpha=seq(0.1, 1, by=0.1), .lambda=seq(0.1, 1.0, by=0.1)))
@ 

<<blackboost, echo=FALSE, results=hide,eval=FALSE>>=
blackboost.train <- train(PlacResp~., data=iatandexpfull.train, method="blackboost", trControl=mytrain, tuneGrid=data.frame(.mstop=c(50,100,150), .maxdepth=1:3))
@ 

<<glmstep, echo=FALSE, results=hide,eval=FALSE>>=
glmstep.train <- train(PlacResp~., data=iatandexpfull.train, method="glmStepAIC", trControl=mytrain)
@ 

<<rpart, echo=FALSE, results=hide, eval=FALSE>>=
rpart.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv+gsr.mean, data=iatandexpfull.train, method="rpart", trControl=mytrain, tuneGrid=data.frame(.cp=seq(0.01, 1, by=0.01)))
@ 

<<rpart2, echo=FALSE, results=hide,eval=FALSE>>=
rpart2.train <- train(PlacResp~., data=iatandexpfull.train, method="rpart2", trControl=mytrain, tuneGrid=data.frame(.maxdepth=1:10), preProcess=c("center", "scale"))
@ 

<<svmRadialCost, echo=FALSE, results=hide,eval=FALSE>>=
## svmradialcost.train.pc <- train(PlacResp~., data=iatandexpfull.train, method="svmRadialCost", trControl=mytrain, tuneGrid=data.frame(.C=1:10), preProcess=c("center", "scale", "pca"))
## svmradialcost.train <- train(PlacResp~., data=iatandexpfull.train, method="svmRadialCost", trControl=mytrain, tuneGrid=data.frame(.C=1:10), preProcess=c("center", "scale"))
@ 

<<ctree, echo=FALSE, results=hide, eval=FALSE>>=
ctree2.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv+gsr.mean, data=iatandexpfull.train, method="ctree2", trControl=mytrain, tuneGrid=data.frame(.maxdepth=seq(0,10, by=1)), preProcess=c("center", "scale"))
@ 

<<evtree, echo=FALSE, results=hide,eval=FALSE>>=
evtree.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv+gsr.mean, data=iatandexpfull.train, method="evtree", trControl=mytrain, tuneGrid=data.frame(.alpha=1:10), preProcess=c("center", "scale"))
@ 

<<cforest, echo=FALSE, results=hide, eval=FALSE>>=
cforest.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv+gsr.mean, data=iatandexpfull.train, method="cforest", trControl=mytrain, tuneGrid=data.frame(.mtry=seq(1, 101, by=10)))
@ 

<<rferns, echo=FALSE, results=hide,eval=FALSE>>=
## rferns.train <- train(PlacResp~., data=iatandexpfull.train, method="rFerns", trControl=mytrain, tuneGrid=data.frame(.depth=1:10) )
@ 

<<pred, echo=FALSE, results=hide, eval=FALSE>>=
## nb.pc.pred <- predict(nb.train.pc, iatandexpfull.test)
## nb.pred <- predict(nb.train, iatandexpfull.test)
## knn.pc.pred <- predict(knn.train.pc, iatandexpfull.test)
## knn.pred <- predict(knn.train, iatandexpfull.test)
## svm.pc.pred <- predict(svm.train.pc, iatandexpfull.test)
## svm.pred <- predict(svm.train, iatandexpfull.test)
## gamboost.pred <- predict(gamboost.train, iatandexpfull.test)
glmboost.pred <- predict(glmboost.train, iatandexpfull.test)
## logitboost.pred <- predict(logitboost.train, iatandexpfull.test)
glmnet.pred <- predict(glmnet.train, iatandexpfull.test)
## blackboost.pred <- predict(blackboost.train, iatandexpfull.test)
## glmstep.pred <- predict(glmstep.train, iatandexpfull.test)
rpart.pred <- predict(rpart.train, iatandexpfull.test)
## rpart2.pred <- predict(rpart2.train, iatandexpfull.test)
## svmradialcost.pred <- predict(svmradialcost.train, iatandexpfull.test)
ctree.pred <- predict(ctree2.train, iatandexpfull.test)
## evtree.pred <- predict(evtree.train, iatandexpfull.test)
cforest.pred <- predict(cforest.train, iatandexpfull.test)
## rferns.pred <- predict(rferns.train, iatandexpfull.test)
@ 

<<confusionmats, echo=FALSE, results=hide, eval=FALSE>>=
randf.acc <-confusionMatrix(randf.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
## nb.acc <-confusionMatrix(nb.pc.pred, iatandexpfull.test[,"PlacResp"])
## knn.pc.acc <- confusionMatrix(knn.pc.pred, iatandexpfull.test[,"PlacResp"])
## knn.acc <- confusionMatrix(knn.pred, iatandexpfull.test[,"PlacResp"])
## svm.pc.acc <-confusionMatrix(svm.pc.pred, iatandexpfull.test[,"PlacResp"])
## svm.acc <- confusionMatrix(svm.pred, iatandexpfull.test[,"PlacResp"])
## gamboost.acc <- confusionMatrix(gamboost.pred, iatandexpfull.test[,"PlacResp"])
glmboost.acc <- confusionMatrix(glmboost.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
glmnet.acc <- confusionMatrix(glmnet.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
## blackboost.acc <- confusionMatrix(blackboost.pred, iatandexpfull.test[,"PlacResp"])
## glmstep.acc <- confusionMatrix(glmstep.pred, iatandexpfull.test[,"PlacResp"])
rpart.acc <- confusionMatrix(rpart.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
## rpart2.acc <- confusionMatrix(rpart2.pred, iatandexpfull.test[,"PlacResp"])
## svmradial.acc <- confusionMatrix(svmradialcost.pred, iatandexpfull.test[,"PlacResp"])
ctree.acc <- confusionMatrix(ctree.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
## evtree.acc <- confusionMatrix(evtree.pred, iatandexpfull.test[,"PlacResp"])
cforest.acc <- confusionMatrix(cforest.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
## rferns.acc <- confusionMatrix(rferns.pred, iatandexpfull.test[,"PlacResp"])
@ 


% After an extensive model fitting process, only three models reached acceptable levels of accuracy. The first of these was the penalised regression methods used in the glmnet package. This model achieved an overall accuracy of 0.667 ($95CI 0.3489 - 0.9008$). The next algorithm which achieved acceptable accuracy was the conditional random forest model. These models form ensembles of trees to reduce variability and increase predictive power. This model achieved an accuracy of 0.75 ($95CI 0.4281 - 0.9451$), with a sensitivity of 0.8750 and a specificity of 0.5. This would suggest that while the model was excellent at predicting those who would respond to placebo, it was much worse (chance level) at predicting those who would not. The final model which was selected as being acceptably accurate was a glmboost algorithm which averages the results of many generalised linear models (logistic regression, in this case) and weights observations which were categorised incorrectly more highly in the next iteration of the algorithm. This model achieved an accuracy of 0.875, ($95CI 0.4281 -- 0.9421$), with a sensitivity of 0.875 and a specificity of 0.5. Again, note that the negative examples of placebo response are harder to predict than are the positive ones. The probability of correctly classifying a positive placebo response was 0.7778, while the probability of classifying a lack of placebo response was 0.6667. 



\section{Discussion}
\label{sec:discussion}


The major contribution of this  study is that replicates the findings of Kapthcuk  \textit{et al} \cite{kaptchuk2010placebos} that people given an open placebo will experience a placebo response. This finding, in fact, is made even stronger by this study as even when the administration of a deceptive placebo is a possibility, the open placebo still proves more effective. 

Secondly, there appears to be a relationship between implicit measures (assessed using the IAT) and the placebo response. However, this effect needs to be replicated due to issues with multicollinearity making it difficult to establish whether the effect is real or not. A study with greater power would be necessary to definitively establish whether or not this effect is real. 

There are a number of potential explanations for this unusual finding. Firstly, the primary researcher's thesis was on placebo, and he possessed a strong belief that placebos actually work. This could easily have been communicated to the participants. Secondly, the experimenter was not comfortable with the deception involved in informing participants that they were about to receive a real drug, and this may have been communicated to the participants also. Additionally, the lack of any side-effects or sensory stimulus from the inert cream used in this study may have cued participants to believe that they were not getting a particularly good medication. It may also have been the case that the use of the term ``recently approved'' for the deceptive placebo administration may have had an impact on the response to this placebo.

Finally, it appears that priming manipulations are as effective in placebo analgesia as they have been in other areas of social cognition. This is important as priming is a less resource intensive way to increase the size of placebo effects, which is important if these effects are to be studied effectively. 

Some limitations of this study included that it was conducted entirely on students, and to that extent may not be widely generalisable. Secondly, the experimenter was not blinded, so demand characteristics cannot be ruled out as an explanation \footnote{that being said, the experimenter was as surprised as anyone when the results were analysed}. Another limitation of this study was the relatively small sample size. Although 111 participants took part in the study, only 66 of them were in either the Open or Deceptive Placebo group, which bounded the potential number of placebo responders. 





\bibliography{placebo2,IAT2,statistics,healthoptmind2,probabilities2}

\end{document}
