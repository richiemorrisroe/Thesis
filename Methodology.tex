
\section{Introduction}

This research assessed the differential contributions of implicit and explicit expectancy measures of treatment credibility and optimism to the prediction of the placebo response. 
In this section, the theory underlying this primary research question will be explicated, and followed by a description of the methods used to answer the primary research question. 


This chapter will consist of the following sections, representing the core parts of the thesis.

\begin{enumerate}
\item The theoretical model of the thesis will be explained;
\item The approach taken towards the development of the IAT(s) will be described;
\item The methods used for the development and validation of the scales used in the research will be described.
\end{enumerate}

\section{Introduction to the theory}

Theories form an indispensible part of science. They represent an attempt to generalise beyond particular forms of evidence and data and to derive some kinds of overall principles which lie behind the oberved events. The major theories behind the placebo and implicit measures were reviewed in  chapter~\ref{cha:literature-review}, and in this chapter provides  an attempt to synthesise all of this information into a coherent whole. The major building blocks of this theory are as follows:
\begin{enumerate}
\item The results of implicit measures point towards there being at least two systems of attitude assessment and evaluation of stimuli in the human mind~\footnote{perhaps more, but certainly at least two}
\item The placebo effect is typically conceptualised as a conscious phenomenon, in spite of the experimental evidence
\item There appear to be feedback loops between bodily sensations and conscious perception --- embodied cognition
\item These feedback loops are evidence against an additive model of drug-placebo interactions
\item Despite the centrality of such conceptions to the placebo effect, no theory has as yet incorporated these findings into their theories
\end{enumerate}

This section will briefly review the evidence in favour of the above propositions, then will elucidate how these could be combined into our theories of placebo and implicit measures, and will provide some testable hypotheses regarding the theory, in the spirit of Popperian falsification.

\subsection{Implicit Measures and Dual Process Models of Mind}

The notion of dual process models of mind is an old one, dating back within psychology to at least the time of Freud, and possibly before. However, the modern conception of dual process models is much more recent, and developed as a result of work with implicit measures and through the findings of cognitive psychology. Essentially, the modern theory suggests that there are two prevalant systems of reasoning inherent to humans, a slow, rational, conscious system, and a fast, frugal and implicit system~\cite{Kahneman2002}. 

These two systems activate under different conditions and seem to perform different functions. One of the major hypotheses emerging from this theory is that under conditions of attentional strain, the implicit system takes over. This is, as we have seen in the previous chapter, borne out by much of the research into implicit attitudes. Asendorpf~\cite{Asendorpf2002} demonstrated what came to be called the double-dissociation effect, where implicit measures (of shyness, in this case) were more predictive of performance on a spontaneous speaking task (the Trier Social Stress Test), while explicit measures were more predictive of considered, deliberate behaviour. These findings have been further replicated by other authors   Thus, we can take the results of the IAT research and that into other implicit measures as pointers towards the operation of this system. The major point to take from this section is that this system appears to exist, and yet has only been touched upon in one or two articles over the past decade~\cite{Geers2005}. 

\subsection{Conscious Conceptions of the Placebo}

The dominant model within placebo research at present is the response expectancy theory of Kirsch~\cite{Kirsch1985, Kirsch1997a}. This theory conceptualises the placebo as resulting from response expectancies, which are defined as ``the conscious expectation of a non-volitional response''. This theory has had some success, displacing the then prevalent theory of conditioned placebo responses~\cite{Vuodouris1985}. That being said, the very definition of placebo and its nature as occuring as a result of deception and belief that one is getting a real drug would seem to suggest that non-conscious systems must be centrally involved in the mediation between awareness and the documented physical responses. 

Indeed, the Geers et al study cited earlier~\cite{Geers2005} demonstrated that semantic priming (by means of a scrambled sentence task) was an independent predictor of the response to a sleep placebo, which given that semantic priming does not effect conscious awareness, implies that such priming (and the implicit system more generally) can affect the response to placebos and indeed biologically active treatments. Another study which points in the same direction is the Shiv et al 2005~\cite{Shiv2005a} study which demonstrated an effect of price of an energy drink effecting the number of puzzles solved by participants in a particular time. This effect disappeared when participants attention was drawn to it, which again implies that implicit systems were involved. Despite this evidence, the dominant theoretical framework remains untouched. In this chapter, I will propose a new model that incorporates both of these systems, and makes a number of predictions that will be either supported or rejected by further research. 

\subsection{Embodied Cognition and Placebo}
\label{sec:embod-cogn-plac}
Another issue with most of the conceptions of placebo current in today's research is that they are almost exclusively cognitive, a point made most forcefully by anthropology researchers~\cite{Thompson2009}  %Add this citation. 
This seems strange, given that it rests on assumptions regarding the relationship of mind and body which all of the evidence of placebo would seem to argue against. It seems that the theoretical perspective within the field is that the mind may effect the body, but not vice versa. Unortunately, this is an untenable proposition, as recent work on haptic cognition (where judgements made by participants are affected by the sensory input they are receiving at the time), published in Science in 2010~\cite{ackerman2010incidental}
Indeed, more recent work by Cwir~\cite{Cwir2011} suggests that awareness of one's own interioceptive processes appears to be a good predictor of people's ability to predict the emotional states of others. 


There has been a resurgance of interest in mind-brain-body feedback loops within psychology and the social sciences more generally of late. It seems that the dominant cognitive model of mind (a kind of splendid isolation for the brain) is being slowly worn down by experimental evidence. An approach such as this was also proposed by Meissner et al in 2009~\cite{Meissner2009} in their meta-analysis which showed large placebo effects in some areas and none in others. They suggested that the larger placebo effects may have occurred in some conditions but not in others as the places in which placebo effects occurred tended to be those systems which have large nervous system connections with the brain, as opposed to communication typically mediated through hormones, which are many orders of magnitude slower. One problem with Meissner's theory is that he posits that placebo effects do not occur in clinical trials when the outcome of interest is a hormone such as cortisol. 

However, the field of psycho-neuro-immunology has noted that psychological variables (specifically optimism) exert major influences on antibody responses~\cite{Carver2010}. There is also some emerging evidence that mindfulness based treatments appear to affect antibody responses, at least in cancer patients~\cite{Ledesma2009}. This begs the question of why the analysis by Meisnner et al found no such effect. One explanation for these conflicting findings would be that the placebo effect is mostly determined by current state effects, while those effects investigated by psycho-neuro-immunology are the result of certain trait like characteristics of individuals.  

One psychological state which may have an impact on placebo responses, and should if my theory is to hold is that of mindfulness. Mindfulness is often defined as a moment to moment awareness of somatic and mental processes. If, as the results of Geers et al suggest, somatic focus can increase the size of placebo effects, then mindfulness (or another proxy marker for somatic focus) should also moderate the size of observed placebo responses. If my hypothesis is correct, then mindfulness should be correlated with placebo response, as well as acting as a mediator between implicit and explicit measures of the same construct. 

If attention is conceptualised as a limited resource, then placebo effects should be enhanced by placing participants in conditions of low stimulation while the treatment is applied. However, this hypothesis causes problems when we considered (below) the effects of participant provider interaction, which appear to account for a significant portion of observed placebo effects. 

\subsection{Additive Models of Placebo}

The common approach throughout clinical trials, and the study of placebo effects more generally, is that the effects of drug and placebo are additive. This assumption leads nicely to the principle that placebo effects and drug effects can be seperated precisely. However, not all of the evidence points in this direction, and (apart from statistical convenience) there exists no apriori reason why this should be the case. Indeed, the work of Geers et al on somatic focus would seem to suggest that paying attention to somatic experience can increase the size of placebo effects. This could be occuring because of a feedback loop whereby a treatment is applied, the patient's awareness of the treatment leads them to attend to sensations related to the treatment, which engages the body's own healing systems, which then increase the size of the effect over and above what would have happened without awareness of treatment on the part of the patient. It is perhaps for this reason that drugs which are adminstered by an automated process are less effective~\cite{benedetti2003}. 

Another issue to consider in terms of mathematical models of placebo response is the phenomenon of the active placebo. This is where an active drug is offered as a treatment for which it has no efficacy, and nonetheless this treatment produces larger healing effects than a typical sugar pill placebo. I would argue that this phenomenon occurs because the side-effects of the drug produce  a feedback loop whereby the treatment has a somatic impact, which alerts participants to the treatment, causes them to accept it as more credible, and thus activates the body's own healing systems. This process, over time, could easily create a conditioned response to the original non-effective treatment, and loops such as this could be responsible for the observations regarding the efficacy of conditioned placebo responses. In this sense, I am making the argument that conditioned placebo responses are turned into expectancies over time.   


\subsection{Social Aspects of Placebo}

In addition to the possibility of feedback loops between awareness and sensation contributing to the placebo effect, the role of the provider needs to be emphasised also. In most drug research, the treatment itself (the pill or cream) is only one element of the context in which the healing process takes place. In addition to the internal factors which shape response to treatment (optimism and expectancies more generally), there are also important social and environmental factors. For instance, the classic work of Gracely et al~\cite{Gracely1985} demonstrated that the effects of the awareness of the provider can have a large impact on the outcome. In this study, half of the dentists were informed that half of the patients would receive placebo. 

In fact, all of them received the real painkiller. However, compared to the group treated by dentists who had not been told that placebo was a possibility, the other patients reported significantly higher pain. Indeed, a systematic review of healthcare interventions~\cite{DiBlasi2001} provided evidence that provider characteristics accounted for a large proportion of the observed placebo effects. More recently~\cite{Kaptchuk2008}, a three armed randomised control trial of acupuncture demonstrated that healing rates were greatly increased when the provider spent more time with the patient discussing symptoms and treatment (45 minutes as opposed to 15). This would seem to suggest that one of the reasons so many people use alternative treatments is not the efficacy of the particular form of treatment, but rather the chance to discuss their symptoms in detail and for longer periods of time.

Another important feature of the context in which healing takes place is the social context, that is the shared beliefs and rituals that make up a culture. For instance, Valium has a powerful resonance in our culture, immortalised in songs by the Rolling Stones are glorified through media et al. Therefore, even when participants have never experienced the drug itself, they bring pre-conceived notions of what it can do, and apply these to their perception of treatment, which alters its efficacy. However, research using the open-hidden paradigm (discussed previously) would seem to suggest that Valium lacks any efficacy when participants are not aware that they are taking it~\cite{benedetti2003}. An additional example of this effect can be seen in the prescription of antibiotics for viral infections. Even though doctors know that they will have no effect, they are still administered to patients. It would be extremely interesting to give people placebo antibiotics and assess its impact on viral infections, relative to the efficacy of true antibiotics. My thesis would suggest that real antibiotics should be slightly more effective, given their obvious side-effects, but that this difference in standardized means would be less than 0.1. 

% A final (in my estimation) variable on placebo response would seem to be the effects of environmental factors. There is some evidence that suggests that a view of nature decreases healing times, and that patients in rooms with sunlight recover faster than those in rooms 
% without a direct view of the sun. It can be argued that this is as a result of environmental influences activating particular implicit cognitions relating to health. It is worth noting that the sun has been consistently associated with health in many cultures across the globe, and that people's mood tends to improve when the weather is nice. 

\subsection{Towards an Embodied Conception of Placebo}

Bearing the previous sections in mind, we can now move forward into proposing a new model for placebo, the embodied placebo model. This model, while not rejecting outright the findings of the expectancy theory, aims to enhance it with more recent research. Indeed, this theory is also compatible with that of conditioned placebo, and also with the motivational concordance approach of Hyland et al~\cite{Hyland2007}.  
The essential features of the embodied model of placebo are as follows. Placebo effects are those healing effects which arise from the perception of treatment or caring on the part of the health care provider or from the context surrounding the treatment. They are mediated by four different kinds of factors.


\begin{itemize}
\item They are mediated by implicit cognitions operating extremely quickly
\item They are also mediated by awareness of the treatment and cognitive models of its efficacy
\item In addition, these implicit and explicit attitudes are enhanced or degrenated by somatic sensations
\item They are mediated by the communications exchanged between the participant and the provider
\item They are also mediated by the system of communications surrounding both the participant and the provider
\item Finally, they are mediated by somatic feedback from the surrounding environment.
\end{itemize}
 
In an effort to advance research and to conclusively demonstrate either my own percipience or ignorance, I shall argue that my theory makes a number of key predictions. 

\begin{itemize}
\item First, that placebo effects will be correlated with scores on implicit measures
\item Second, that there will be an interaction effect between explicit and implicit attitudes on placebo response
\item Third, that increased interioceptive awareness (or mindfulness) will increase the size of placebo responses
\item Fourth, that the implicit attitudes of the practitioner will exert a significant impact on the placebo response of the patient~\footnote{due to researcher constraints, this hypothesis was not tested in this thesis}
\item Fifth, that adding sensory input to a placebo treatment will increase its effectiveness. 
\item Sixth, that the extent of changes in physiological measures in participants will be correlated (as a lagged variable) with the size of the next reported pain rating.
\end{itemize}

However, in order to properly test this model, it needs to be compared to other plausible models. The first of these is the model of Krisch, noted in his 1985 paper. This model claims that expectancies are the major (indeed only) mediating factor between consciousness and placebo responses. Therefore, this model suggests that there is a direct effect of expectancies on placebo response, that physiological outcomes should not be predictive of the placebo response and that any effect of optimism and mindfulness will be mediated through expectancies. This model claims that there will be no direct effects of any other explicit or implicit measure on placebo response, but that they will all be mediated by expectancies. 

Another, equally plausible model is that optimism is the driver of the placebo response, and that the effects of expectancies are mediated by levels of optimism. This model would suggest that both implicit and explicit optimism will have direct effects on the placebo response, and that expectancies (implicit and explicit) shall be mediated by levels of optimism. This model makes no predictions about the effects between physiological response variables and placebo response. 

Additionally, a variation of each of these models would suggest that either explicit or implicit expectancies could be the sole mediator of the placebo response, and these models were also tested. 

Finally, a null model where none of the variables measured had a relationship to the placebo response was examined, to ensure that the other models were at least somewhat more plausible. 

These models will be examined using a structural equation modelling approach, which should allow for an efficient and accurate test of my theory, even if the population is not particularly representative. 


\section{Development of the IAT}

Qualitative analysis was an essential part of this project.  
It gave insight and data into the development of the IAT's used in the final part of the research project.
The methods used for qualitative analysis here involved a thematic analysis~\cite{braun2006using} of the interviews conducted was carried out to develop themes both for the repertory grid and for the IAT.

Qualitative research typically relates to the analysis of interviews and other texts derived from people. It differs fundamentally from quantitative analysis in that it aims for a deep understanding of particular individuals, while quantitative analysis aims for a broad understanding of the sample as a whole. 

The biggest problem with qualitative analysis is that it cannot scale to the level of large scale surveys, as it requires significant amounts of researcher time per participant while quantitative surveys have a cost of development in time, but the marginal cost of administering the survey to a new participant is essentially zero (assuming distribution over the internet).

The issue of reflexivity is crucial to qualitative research (and also appears in quantitative research, though rarely as openly)~\cite{rosenthal1967covert, rosenthal1969interpersonal}.

Reflexivity refers to the impact of the researcher's prior conceptions and approaches have on the course of the interviews~\cite{finlay2002outing}.

This is extremely obvious in the choice of the major questions to be asked in the interviews, but it can occur in subtle ways during the interviews also (for example in the use of language by the interviewer) and in the quality of communication or rapport experienced by the researcher in the course of the interview. Reflexivity is also critical during the analysis, as the researcher must be aware of their own biases and ensure that this affects the analysis as little as possible, or at least report where the problems arose for them.

\subsection{Thematic Analysis}

The thematic analysis's primary purpose was to look for common patterns in the conceptualisation of health, sickness and treatment. As such, it was felt that the best approach would be to develop the codes from the transcipts themselves. 
This is called an inductive approach to coding~\cite{haberman1979analysis}. Following transcription, each interview was coded line by line by the primary researcher, and codes were developed throughout this process.

After all the interviews had been transcribed, the codes were pruned and amalgamated to reduce redundancy, and this process was repeated. This second coding lead to a number of new codes and insights which had been missed the first time, and the text was again coded for a third time following the development of these new codes.

Then, the document was coded a fourth time, but on this run through the aim was to look at higher level patterns that emerged from the text. Following this coding procedure, a process of chunking of codes was carried out. This involved looking at how codes fit together and grouping them under a number of thematic headings. The original texts and recordings were referred back to at this point to ensure that the themes were representative of the original data, and finally the themes were written up to record the results of this exercise.


\subsection{Repertory Grids}

The use of repertory grids in this research was as a bridge between the qualitative analysis carried out and the quantitative side of the research. 

Repertory grids were developed by George Kelly as an aid to therapy~\cite{kelly2003psychology} although it has been used in many diverse situations in the ensuing years. 
Repertory grids were developed out of Kelly's theory of cognitive consistency, an active area of research which fell out of favour following the discovery of cognitive dissonance by Festinger in 1947~\cite{greenwald2002}.

The premise of the technique is simple. Firstly, participants are supplied with a list of important people in their life, such as their mother, an older sibling and a teacher whom they liked or disliked. They write down the names they have chosen for each person, and then they compare the people in groups of 3. For each group (or sort), they are asked to describe how two of them are similiar and also how one of them is different in a word or short phrase. These words or phrases can then be analysed both quantitively or qualitatively.

The primary method of quantitative analysis was through factor analysis.

The approach taken in this project was as follows. In one of the surveys carried out on the UCC population (TCQ version 1) participants were asked to rank the most important people in their life who were related to healthcare. 
This data was then sorted and ranked, and a list of the most common people used was compiled into a health related repertory grid. 
This was then administered to a small sample (N=17) to test the instrument. 
 The results of this testing are described in Chapter~\ref{cha:devel-impl-meas}, in Section~\ref{sec:development-iat}.

\subsection{Development of IAT}
\label{sec:development-iat}

The plan for the development of the IAT was to use the constructs obtained from the repertory grids to develop useful stimuli for the IAT.\
Unfortunately, as Chapter~\ref{cha:devel-impl-meas} describes, this portion of the research did not lead to a successful outcome, for reasons described in that chapter.

Therefore the IAT was developed from both the important figures which arose from the repertory grid, the qualitative interviews, and to match the explicit measure of treatment credibility. The optimism IAT was developed along similar lines to most other Implicit Association tests, in that the survey for this measure was used as a base. 


\section{Quantitative Research Methodology}
\label{sec:quant-rese-meth}
This section  will describe the methods employed for each part of the thesis and provide a rationale for why these methods were used in the thesis.
A mostly quantitative approach was taken for the following reasons. Firstly, the placebo  is a very noisy phenomenon~\cite{Singer2005}, subject to many sources of error and bias (cf Section~\ref{sec:concept-placebo}). Secondly, in order to predict the placebo effect, there needs to be some kind of measure, and these are typically metered in numerical terms. 

This thesis consisted of three main parts. Firstly, following a thorough literature review, the major constructs associated with the placebo effect and implicit measures were identified. These constructs were then administered to large samples of the population from which the experimental participants were drawn. This procedure was carried out for two reasons. In the first case, this was so that the population means could be estimated more precisely and thus the experimental sample compared on these measures.  

The second reason was so that more sophisticated models could be developed for person responses (which typically require larger samples than are common to experimental studies) and could then be applied to the experimental sample. This approach marries two of the strengths of psychological research; the latent variable approach common in psychometric research; and  the use of rigorous experimental design to determine relationships between constructs (through Structural Equation Modeling). This thesis aimed to use both of these strengths in combination to gain insight into the causes underlying the response to placebo in healthy volunteers.

The second major part of the thesis was the development of the implicit association tests (IAT's) and the explicit measure of expectancies (treatment credibility questionnaire) used in the experimental portion of the research. 

The third, and final, part of the thesis was the testing of these measures in an experimental setting using a placebo analgesia design examining the response to ischemic pain in healthy volunteers. 


In this section, the statistical techniques used to answer the primary research questions shall be discussed. Only methodology used throughout the thesis will be described in this section. Therefore, methods of cross-validation will be described, as will the regression models utilised, followed by the treatment of missing data and finally Structural Equation Modeling shall be discussed. The methods employed only within one chapter are discussed in the appropriate chapter. 

\subsection{Problems of Sample Inference}
\label{sec:probl-sample-infer}
\subsubsection{The Problem}

In every statistical approach, the core is the development of inferential tools to reduce our uncertainty about the events under study\cite{gelman2010philosophy}. Given that we typically lack infinite resources, sampling from populations in a randomised manner is used to approximate the quantities of interest~\cite{venables2002modern}.

However, we are rarely interested in the specific sample we have recruited; we tend to want to infer properties of the population  from which they are drawn.  In non technical terms, given a sample and some analyses, we develop a model which we hope will predict the behaviour of future samples (and indeed the population).

This approach toward inference is often operationalised in the creation of a model, whether based on the results of a linear regression or factor analysis. Typically, we aim to maximise the amount of the response variable  in a sample of size $n$ explained given some number of parameters. It is trivial to see that as the number of parameters ($p$) increases, so does the fit --- in the limit, this would involve the fitting of a model with a seperate parameter for each observation. 

Clearly, such a model will violate the principles of parsimony and clarity that we aim for in our science. However, even when $p$ is less than $n$, we still run the risk of overfitting a model to our data. Overfitting is said to have occurred when we model features of the data that are essentially random~\cite{friedman2009elements}. 

Because factor analysis is lenient towards mis-specified models and tends to model error as well as signal, many psychometric theories have faltered on the rock of replication~\cite{fabrigar1999evaluating}. SEM is often used as a panacaea for such problems. However, a Structural Equation Model is only as good as the data and theory behind it, and if the factor analysis models noise, so too will an SEM on the same data set (to a lesser extent, of course). This may account for the poor replicability of psychological theories based on the results of factor analysis and SEM.\@

\subsubsection{A Solution}

The typical scientific approach to this problem is simple --- replication. Replication, preferably by independent researchers, is supposed to ensure that models eventually tend towards the minimum of parameters for the maximum of explanatory power.

Indeed, many fit indices penalise complex models over simple models. This suffices for some research, but there are cases where such an approach does not prove useful. Replication can also be fraught with difficulties, as it takes time, effort and additionally makes the assumption that population quantities are stable over time. Nonetheless, it is the ideal solution. However, given that replication is not incentivised by the scientific community (and may be unethical in some situations), researchers in the field of machine learning have come up with a novel approach which appears to improve predictive accuracy and can also aid in the development of theoretical understanding~\cite{friedman2009elements}.

\subsubsection{Cross-Validation}

The solution proposed by those machine learning researchers is at once simple and elegant, while also extremely practical. The technique is known as cross-validation, and its application routine in commercial and scientific data-mining settings. However, it does not appear to have found much favour within psychology as of yet (with some notable exceptions ~\cite{dawes1979robust}).

The basic premises of the techniques are:
\begin{itemize}
\item All models are wrong;
\item Models are best tested on independent samples;
\item Independent samples are sometimes hard to come by;
\item Therefore, datasets should be split into training and test sets, where the model is developed on the training set, and its accuracy assessed on the test set.
\end{itemize}

This approach seems to improve predictive accuracy by an order of  magnitude, especially when applied to large data sets ~\cite{breiman2001statistical}.  

The principle of training and test sets has since been generalised to $k$-fold cross validation where the dataset is split into $k$ random pieces, and all but one of these are used to estimate a model, while the other is used as a test. This procedure is repeated $k$ times, and the results are averaged to form the best model (for that sample of data, at least). Some authorities argue that this procedure should be repeated at least $k$ more times, to control for the effects of random sampling\cite{friedman2009elements}.

Another variation on the central approach is leave-one-out cross validation, where given a sample of $n$ observations, fit a model on $n-1$ and test on the other, a total of n times. This approach, while taking the technique to its logical extreme is not of particular usefulness to us at this point, as large inter-personal variability between individual participants typically observed in psychological data would tend to reduce its efficacy\cite{friedman2009elements}. % However, given that it is the most efficient means of cross-validation for smaller samples, this method was employed in the experimental portion of the research. 

In essence, cross-validation is an extremely valuable technique which has been mostly ignored in psychology. It is the opinion of this researcher  that this technique is useful, and it will be applied consistently to this research.

\subsubsection{Regression Models}
\label{sec:regress-models}

At the heart of typical psychological modelling practice lies the general linear model.This model underlies such familiar techniques as correlation, regression and analysis of variance (ANOVA)\cite{gelman2007data}. These techniques are based on the idea of fitting a straight line (or plane, in the case of multiple predictor variables) to the observed data and using this line to make inferences about the relationships between variables of interest.

% The models are typically fitted by a least squares method. Least squares is a criterion which suggests that in order to fit the best line, the average vertical distance between the points should be minimised. Least squares is the heart of many social and physical science modelling techniques, and is formalised in the Gauss-Markov theorems which prove that if the assumptions of the model are met, then in the limit, the least squares approach is the most efficient unbiased estimator~\cite{friedman2009elements}.

% The assumptions of the general linear model are as follows:
% \begin{enumerate}
% \item The residuals of the model (the distances between the predicted line and the observed values) should be approximated by the standard normal distribution
% \item The variables should have constant variance across their entire range (homoscedasity)
% \item The residuals should be independent of the response variable

% \item The residuals should be uncorrelated with one another
% \end{enumerate}

The general linear model has been elaborated greatly over the last century, and has been applied to the approximation of relationships that do not meet all of the assumptions noted above\cite{gelman2007data}. The introduction of link functions (non linear functions designed to transform the response variable into a form suitable for the model)  led to the development of generalised linear models, which allow the same computational techniques to be used to fit and test models for data which does not fit the requirements of the standard linear model~\cite{mccullagh1989generalized}.

For example, logistic regression is a technique for prediction of binary
 variables using a link function. Poisson regression is used to model data
 which is bounded by zero and positive infinity. A number of quasi methods
 are available for both of these techniques which allow models to be fitted
 to  data which has an excess of zeros (so called zero inflated models)~\cite{gelman2007data,venables2002modern}. In another direction, the
 requirement for the residuals to be uncorrelated has been relaxed to allow for the development of multilevel or mixed models, which allow the residuals of particular groups to be correlated with one another~\cite{gelman2007data}. 

A number of major issues arise with the use of the general linear model with psychological data. Firstly, the requirement of normal errors can sometimes be difficult to satisfy. This follows from the manner in which the normal distribution appears to behave. When it was originally discovered (by Gauss)~\cite{stigler1986history} it was used to model the combination of many small, independent variables. It tends to work well as an approximation when there are many independent variables affecting the results of an analysis.

However, when psychological tests (such as self report instruments) are developed, the aim is to remove as many of these small influences as possible so that the measure taps one construct with clarity. This will often lead to a non-normal distribution of errors. Non parametric approaches are an alternative to the General Linear Model, but these often lack the power of their parametric alternatives.The central limit theorem assures us that, in the limit, any distribution of means will converge to a normal, and in practice perhaps as little as 100 observations may suffice for a t-test~\cite{venables2002modern}. 

A more serious problem (in terms of the impact on outcomes) is heteroscedasity, where the response variables (or the predictor) do not possess constant variance across their range\cite{gelman2007data}. This can seriously affect the models built as points with less variance will be much more influential than those with a higher variance. The assumption of homosecdasity can be checked by formal statistical tests, but often graphical tests of this assumption are much more revealing, as they can show where the model fails as well as whether or not it fails.

In this research, linear, logistic and mixed models were utilised throughout all of the quantitative research in order to test hypotheses. 

% \subsection{Treatment of Missing Data}


% Missing values can often cause a problem with large datasets. The
% traditional approach has been to delete either pairwise (dropping
% all variables that have missing values for a particular analysis)
% or listwise (dropping all variables that have any missing values)~\cite{graham2009missing}.
% Both of these methods are flawed. The pairwise method can cause issues
% with interpretation in that different analyses will be based on different
% samples and degrees of freedom. The list wise method is slightly better,
% but it does make the assumption that the missing values are missing
% completely at random (MCAR) which is often not tenable~\cite{graham2009missing}.

% The primary deficiency in the above methods is their granularity. Both methods discard information in a rather crude way. Another approach towards the treatment of missing data is mean imputation, where any missing value is recoded as the mean for the sample. Predictive mean matching is a similiar approach which substitutes the mean for the respective variable containing missing data instead.

% The major issue with mean (or median) imputation methods is that they artificially reduce the variance of the data. As more of the sample tends towards missing, this can lead to artificially centred variables which when used in model building, provide inaccurate standard errors and lead to poor inferences.

% A more principled approach to missing data was developed by Donald Rubin, a statistician concerned with the problem of missing data in large public datasets such as the census or longitudinal studies\cite{little1987statistical}. The basic idea is quite simple, and has been generalised far beyond its original goal of filling in data in large representative samples.

% The method is multiple imputation, and the process is as follows. Firstly, the missing values in the dataset are predicted given the information available in the non-missing values. Some noise is added (typically Gaussian, though bootstrapping from the observed distribution is often a better approach)~\cite{gelman2007data} in order to prevent the computer from predicting the same values again, and these new values are then used as the starting predictors for the next imputation. This process is done a number of times (3--15, depending on the proportion of missing data)~\cite{graham2009missing,little1987statistical} and then the the analysis is run on each of these datasets seperately, and the estimates are combined at the end of this process. The major advantage of this method is by looking at the variance of the estimated parameters, we can approximate the uncertainty which surrounds our method of imputation. In addition, the repeated draws ensure that chance features of the data do not lead to erronuous conclusions, as could easily be the case if a single imputation method were used.

% Throughout this research, multiple imputation was used where the amount of missing data was substantial. Typically, between five and fifteen simulated data sets were created, and models run seperately across all imputed datasets, with inferences combined at the end. The multiple imputations were then analysed and tested to ensure that the simulated data was an accurate reflection of the non-missing data, in line with best practice~\cite{abayomi2008diagnostics}.

\subsection{Psychometric Analyses}
\label{sec:psych-analys-meth}

The kinds of psychometric models employed in this thesis were threefold, Factor Analysis, Item Response Theory (IRT) and Structural Equation Modeling (SEM). The general approach taken was to use FA and IRT to develop models, and utilise SEM techniques to test these proposed structures. Below, these three methodologies are introduced, following which the general plan of analysis for Chapters~\ref{cha:health-for-thesis} and~\ref{cha:tcq-thesis} are described. 

\subsubsection{Factor Analysis}
\label{sec:factor-analysis}
Factor analysis has a long history in psychology, and is now over one hundred years old. It is the most commonly used latent variable modelling technique in psychology, and more pages of \textit{Psychometrika} have been devoted to it than to any other technique \cite{henson2006use}.  Despite this, there are still a number of issues and controversies which surround the technique \cite{sass2010comparative}.  Essentially, factor analysis is an attempt to approximate a correlation matrix with a smaller matrix of parameters.  These hypothesised latent variables tend to be called factors or components.

 % One of the first controversies surrounding factor analysis is the dispute between Factor Analysis proper and Principal Components Analysis\cite{henson2006use}. The major difference between Factor Analysis and Principal Components Analysis is that in Factor Analysis, only the variance common across observed response to items (or communalities) is analysed, while in PCA, all of the variance (including variance only found in one item, or unique variance) is analysed. PCA tends to work better for data reduction, and indeed this is the reason why it was developed \cite{borsboom2006attack}.

% Throughout this research only factor analytic methods were used for psychometric purposes, as Factor Analysis provides a true latent variable modelling approach, while Principal Components Analysis  (PCA) does not. 

The most critical issue surrounding factor analysis concerns determination of the number of factors to extract\cite{zwick1986comparison}.  This is an important issue, as theory and practice are likely to be held back if an incorrect choice is made.  The issue is not that there are no criteria on which to base a principled decision, but rather that the different criteria often do not agree, and it is thus ultimately left to the informed opinion of the researcher which factor solution is to be preferred.  All of the decision criteria will be reviewed in turn, and their advantages and disadvantages will be discussed \cite{henson2006use}. 

\begin{quotation}
  "Solving the number of factors problem is
     easy, I do it everyday before breakfast.  But knowing the right
     solution is harder" (Kaiser, 1954).
\end{quotation}

The choice of criterion for retention of factors is extremely important in applied work. This is because if an incorrect number of factors are extracted, then the predictions for the experimental portion of the research will be biased, and thus will not prove as useful as the method could otherwise be. 

The first, and most popular, criterion is surprisingly the least useful \cite{zwick1986comparison}. This rule is called eigenvalues greater than one criterion and recommends keeping all factors whose eigenvalues are greater than one. The rationale behind this approach is that eigenvalues less than one explain less of the variance in the matrix than one item, and as such should not be retained.  More recent research appears to put the minimal criterion for retention of eigenvalues at approximately 0.7 \cite{henson2006use}. 

The second criterion often used is the scree plot technique, which was popularised by Raymond Cattell . This criterion recommends that the eigenvalues of all factors should be plotted against their number, and only factors before the drop off in eigenvalues should be used. As the process of factor analysis ensures that the first factor will have the largest eigenvalue, followed by the second and so forth, this criterion looks for the point where the eigenvalues are very close to one another.  This criterion has a number of advantages.  It is available in all statistical packages, it can be used without any special training and it tends to give results which are somewhat, if not totally accurate \cite{zwick1986comparison}.  Its major disadvantage is that it relies upon the interpretation of the researcher, but given the strong emphasis on interpretation throughout factor analytic literature this should not be regarded as too much of a handicap.

The next criterion which can be used is that of parallel analysis. Parallel analysis is a Monte Carlo (simulation)  technique which simulates a data matrix of equal size and shape to the matrix under study, and calculates the eigenvalues of these simulated matrix against those of the real matrix \cite{horn1965rationale}. All factors are retained up to the point where the simulated eigenvalues are greater than the true eigenvalues.  Parallel analysis is one of the better techniques for assessment of the number of factors to extract , and it can often give very accurate results \cite{zwick1986comparison}.

Its major disadvantage is that it tends not to be available in many statistical packages, and that it can often over factor the data-set. Additionally, many tools simulate the new data from a normal distribution, the requirements of which are not often met in practice in psychological instruments \cite{micceri1989unicorn}.  It does produce some of the most accurate results in simulation studies so it is a useful tool in practice \cite{zwick1986comparison}. 

Another useful criterion is that of the Minimum Average Partial Criterion (MAP) which extracts factors from the data-set until only random variance is extracted \cite{revelle1979very}. Again, this is an accurate criterion  \cite{zwick1986comparison} which is little used as it is not available in popular statistical programs. The only problem that  has been found with this criterion is that it tends to under-extract factors. %However, with the use of this criterion as a lower bound, and parallel analysis as an upper bound, then the decision of how many factors to retain can be made much easier. [I found this, but the literature does not appear to have as many examples of it] This leads on to the major point and issue with much factor analytic research today, whereby one decision rule is used to the exclusion of all others. Many researchers have recommended the use of multiple decision criteria, but this does not appear to be am approach utilised by many in the literature \cite{henson2006use}\cite{sass2010comparative}. However, this is the approach which has been taken in this research. This work will use parallel analysis, the MAP criterion and examination of scree plots to ensure that all relevant factor solutions are examined.

However, the ultimate test of a factor solution (without using other methodologies, such as Structural Equation Modelling) \cite{joreskog1978structural} is its theoretical clarity and interpretability, and this will be the first test used for all proposed factor solutions.

Another area of dispute amongst researchers in the factor analytic field is which method of rotation to use\cite{sass2010comparative}. As the eigenvalues are only defined up to an arbitrary constant, these rotations do not have any substantive impact on the factor matrix, except that they can make it easier to interpret (which is normally very useful). 

Rotations are commonly applied to factor solutions in order to reduce items loading on multiple factors, and to aid in the discovery of simple structure \cite{henson2006use}. Rotations can be divided into two classes, orthogonal and oblique \cite{sass2010comparative}. Orthogonal rotations return uncorrelated factors, while oblique rotations allow the factors to be correlated. Given that most psychological measures are correlated with one another, one would expect oblique rotations to be more common. However, the default appears to be orthogonal rotations, as they are apparently easier to interpret \cite{henson2006use}. Oblique rotations were applied throughout this research, as if the factors are truly uncorrelated, then the oblique rotation will show that, while the converse is not true for orthogonal rotations.

In conclusion, factor analysis is an extremely useful technique which has been widely applied in psychology. It is available in most software packages, is typically easy to interpret and can be carried out with a small number of items (more than 200 is often sufficient). The major problems with factor analysis are the necessity of determining how many factors to extract, which is a difficult and often subjective decision, and additionally if scores are required then many methods exist, again without any clear rationale for choosing one over the other.  

\subsubsection{Item Response Theory}
\label{sec:item-response-theory}
Item response theory (IRT) is often called model-based measurement\cite{fischer1995rasch}, (also referred to as Rasch modelling), is a newer approach to analysing self report data, developed both by the Danish mathematician Rasch in work for the Danish army, and also seperately by Lord and Novick in the US, while working for the Educational Testing Service (ETS) in the 1950's and 60's \cite{van1997handbook}.
The fundamental premise of IRT is that the properties and scores on a psychometric test can be modelled as functions of both the items on the test and the ability of the people taking the test.

The IRT approach suggests that conditional on both the ability of the person and the difficulty of the test, the responses of each participant can be predicted probabilistically. As the latent ability of the participant rises, they tend to choose alternative responses which are more reflective of this latent ability. One example of this might be an item for extraversion ``I am always the life and soul of the party``, those respondents who had a higher latent score on the extraversion construct would tend to choose the agree or strongly agree options (on a typical five point scale). 

For instance, if one was modelling extraversion using a set of ten items, the participants who scored highest in extraversion would be most likely to respond strongly agree to the items.  IRT also assumes a property called local independence, which states that conditional on the ability measured by the test, the scores of each participant are independent of one another.

There are a number of different approaches taken to IRT~\cite{van1997handbook,fischer1995rasch}. The  Rasch models are the simplest, and have a number of extremely appealing mathematical properties. These models assume that only one trait is measured by the items, that all items are equally predictive of the trait, and that there is no guessing~\cite{van1997handbook}.

Because of these assumptions, it is possible to seperate out person abilities and item difficulties perfectly. However, another approach (normally referred to as a two parameter model, or IRT proper) claims that items are differentially predictive of the ability being measured, in a manner analogous to different strength of loadings of items on a construct in factor analysis. Another model the three parameter model~\cite{lord1968statistical}, allows for correct responses through a process of guessing, but this model is not normally applied to polytomous items~\cite{van1997handbook,Mair2010}.

% The Rasch model is the simplest of these three general forms of models, and will be discussed first, followed by a discussion of two parameter models, after which I discuss three parameter models. Finally, this section ends with a discussion about non-parametric item response theory and multidimensional IRT.

In general, IRT models are represented by the logistic function, and are estimated iteratively through procedures of numerical optimisation (maximum likelihood \cite{fischer1995rasch}). The function used to describe the data is a logistic one, where ability is estimated from the probability of answering the question correctly \footnote{a probit model was used for many years as the logistic function was harder to estimate, and the difference between these two functions after scaling are minimum}.

The difficulty of an item is conventionally defined as the ability of participants who answer the question with 50\% accuracy. The parameter $\alpha$ is defined as the difficulty of the item. In two parameter models, another parameter $\beta$ is defined and is used for the discrimination of the item (the slope of the curve). In the more complex 3 parameter model, $\theta$ is used to measure guessing (the probability of a correct answer given low ability)\cite{van1997handbook}.

IRT was developed in the context of ability tests, and this leads to much of the vocabulary fitting uneasily within personality psychology. For instance, in the context of a credibility questionnaire about various treatments, the questions on homeopathy are categorised as most difficult (see Chapter \ref{cha:tcq-thesis}). This does not mean that they are harder to answer, just that the probability of a respondent endorsing them is lower than the probability of a participant endorsing a similiar item on the efficacy of painkilling pills.

% Below, all of the models will be described in terms of their original use in ability testing, and the section concludes with a discussion of polytomous IRT, which is the methodology exclusively relied upon in this research as this research focused on personality  rather than ability testing.

It is important to note that the names of the models are slightly deceiving, while they are called 1, 2 \& 3 parameter models, they actually involve the estimation of 1,2, or 3 parameters for each item. This normally means that a test of ${1,2,3\ldots, n}$ items will require the estimation of either ${n, 2n, 3n}$ parameters. The parameters are estimated by an iterative maximum likelihood approach, as so issues of optimisation and ensuring that a global maximum has been found are often important in practical applications\cite{gill2002bayesian}.


\subsubsection{Structural Equation Modelling}
\label{sec:struct-equat-model}

Structural equation modelling is regarded by many as an adjunct technique for evaluating the results of particular factor solutions~\cite{fabrigar1999evaluating}. However, it is actually a far more general techniques to test the relationships between both manifest and latent variables, and even to establish causality in some cases~\cite{pearl1998graphs}. The factor analytic procedure is full of interpretative procedures where no principled choice can be made, and structural equation modelling (hereafter SEM) is an attempt to compensate for some of these deficiences.

SEM was developed by Joreskog in the 1970's~\cite{joreskog1978structural}. It provides a means of testing hypothesised relationships between latent and manifest variables. In practice, the result of a factor analysis is regarded as a measurement model of the data.

This is combined with a structural model (which describes how the latent variables relate to one another and to the manifest variables). The two of these models are then used to construct a covariance matrix which is then compared with the observed data, and a number of indices of model misfit are calculated. Foremost among these is the $\chi^2$, which estimates the degree of model misfit. The desired result is a p-value of greater than 0.05, which shows that the two matrices are not significantly different.

However, the $\chi^2$ is extremely sensitive to sample size, and tends to be rejected in almost every case~\cite{henson2006use}, given that the sample sizes needed for accurate factor analysis and structural equation modelling tend to be quite large. As a result of this, many other fit indices have been developed. Foremost amongst these are the Non Normed Fit Index (NNFI), which is also known as the Tucker Lewis Index~\cite{bentler1990comparative}, the Root Mean Square Error of Approximation (RMSEA)~\cite{rigdon1996cfi} and the Bayesian Information Criterion (BIC)~\cite{schwarz1978estimating} and the Aikike Information Criterion (AIC)~\cite{akaike1974new}. These all have different strengths and weaknesses and are typically used in a complementary way. All of these fit indices incorporate explicit penalisation, which aids in avoiding overly-complex models. The $\chi^2$ it also has some penalisation (based on degrees of freedom) but it is typically not strong enough to prevent over-fitting. Some authors argue that this focus on other fit measures apart from $\chi^2$ is a way to avoid determining better models, but such a view is controversial in the field at present~\cite{barrett2007structural}.

% There are also some requirements for SEM models which are typically not met for much psychological and psychometric data. These are as follows:
% \begin{itemize}
% \item The distribution must be approximated well by the first and second order moments.
% \item Sample size is required to be large (>300)
% \item The covariance matrix must be strictly positive definite across the entire parameter space.
% \end{itemize}

The multivariate normality assumption made by the procedure is often difficult to meet in practice. However, there are a number of distribution free methods in SEM, of which the most common is a Weighted Least Squares approach. This proceeds similarly to a Weighted Least Squares approach in linear regression, where points are assigned weights depending on how closely they meet the assumptions of the model. Sample size, by contrast, is typically easy to increase (at least for non-clinical populations).

Identification of the model is one issue in practice, though as Joreskog notes, this can often be achieved by fixing a number of parameters to 0 or 1 (the inter-factor variances are often scaled in this fashion)~\cite{joreskog1978structural}.

Another, more theoretical issue is that no set of data is uniquely determined by an SEM model. This is known as the problem of rotation in factor analysis~\cite{maccallum2000applications}. Given a covariance matrix $W$, and a set of data $D$, there are many solutions which provide the same fit indices of the model to the data. This can lead to a similiar problem as occurs to factor analysis, where the researcher must make a choice between models which are quantitatively identical. One approach for resolving this problem was discussed above, in Section~\ref{sec:probl-sample-infer}.

In this thesis, SEM was applied extensively to test the theorised relationships between variables, and the experimental chapter includes a number of tests of theoretically derived models (c.f. Chapter~\ref{cha:primary-research}).

\subsection{General Analytical Approach}
\label{sec:gener-analyt-appr}




Factor solutions were extracted using principal axis methods primarily, and maximum likelihood methods where these did not converge.

Primarily, direct oblimin methods of rotations were utilised, but promax rotations were also applied to ensure that the proposed structure was not overly sensitive to the methods of rotation used.

After the various factor structures were obtained, they were plotted and analysed for interpretability. Communalities and uniquenesses were assessed to ensure that there was no over or under factoring in the solutions. %% Communalities were then graphed against the number of factors extracted and the methods of extraction to provide a simple graphical guide to the usefulness of each solution.

Following this procedure of extraction and interpretation, Structural Equation Modelling was applied to each of the proposed factor solutions using the OpenMx package for R. The optimal factor solution was chosen using the AIC of each fitted model, along with the RMSEA of the proposed solutions, and these solutions were then evaluated for their performance on unseen data. 


Following the investigation of structure with the methods of classical test theory, the scales were analysed using Rasch models and item response theory. Firstly, mokken analyses were run, in order to check the assumptions of monotonicity, local independence and to assess how many sub-scales the analysis should be carried out on. 

Following this, three successively more complicated IRT models were fitted to each sub-scale (Rasch, one parameter and two parameter). Graded Response Models were used for IRT estimation as these are appropriate for ordinal data. 


Linear regressions were run to examine the differential effects of each of the correlated variables. Stepwise selection on the training set was carried out, along with lasso, ridge and least angle regression methods. The performance of each of these methods was then assessed on held-out data (from Sample One, using ten fold cross-validation as previously described). 

In the case of the first samples in Chapters~\ref{cha:health-for-thesis} and~\ref{cha:tcq-thesis}, some of the second was used as a heldout data set. For the second sample, the entire dataset was split into three or four splits, and the cross validation procedure carried out for each. The splits were kept quite large (approximately 300 non missing observations) to allow for psychometric models to be fit to each split seperately, and then to be tested on the remaining data. In Chapter~\ref{cha:health-for-thesis} a backtesting approach was used, whereby the most successful models from the second sample were assessed for their performance on the first sample. For Chapter~\ref{cha:tcq-thesis}, this was not possible due to the revision of the instrument, and so a booststrapping procedure was used instead.


\section{Conclusions}
\label{sec:conclusions}


In this chapter, the theory underlying the work of this thesis has been explained, and the methods used to answer the primary research questions have been addressed. To recap, the IAT's were developed through a process of qualitative analysis and using the explicit measures as a base. Additionally, due to the complexity of many psychometric models, cross-validation techniques were employed to provide unbiased estimates of their likelihood. % Multiple imputation was used on the survey data where the proportion of missing data was greater than 10\%, and
Structural Equation Modeling was employed pervasively throughout the thesis to test particular factor analytic and regression model structures.



% \section{Preliminary Research}

%  Sampling from a population can be difficult, especially as the requirement of randomness needs to be satisfied. However, even if the surveys and measures are sent to a random selection of participants, who responds will almost certainly not be random, as people may only respond to surveys which are salient to them, and ignore the others. This is especially true in a University environment where many surveys are sent out to either random samples or the entire student population regularly. Some of the issues and concerns around sampling for each of the different pieces of research are discussed below.





%%% mode: latex
%%% TeX-master: "PlaceboMeasurementByMultipleMethods"
%%% End:

