
\section{Introduction}
n
In this section, the methods used in this thesis will be described, along with the procedures for all of the research carried out, followed by a description of the statistical analyses used in this research.
All of the methods employed here have their primary focus the following aim - to assess the differential contribution of self report, implicit and physiological measures to the prediction of the placebo response.

This chapter will consist of the following sections, representing the core parts of the thesis.

\begin{enumerate}
\item The theoretical model of the thesis will be explained
\item The approach taken towards the development of the IAT(s) will be described, using interviews and repertory grids
\item The methods used for the development and validation of the scales used in the research will be described
\end{enumerate}

\section{Introduction to the theory}

Theories form an indispensible part of science. They represent an attempt to generalise beyond particular forms of evidence and data and to derive some kinds of overall principles which lie behind the oberved events. The major theories behind the placebo and implicit measures were reviewed in  chapter \ref{cha:literature-review}, and in this chapter provides  an attempt to synthesise all of this information into a coherent whole. The major building blocks of this theory are as follows:
\begin{enumerate}
\item The results of implicit measures point towards there being at least two systems of attitude assessment and evaluation of stimuli in the human mind \footnote{perhaps more, but certainly at least two}
\item The placebo effect is typically conceptualised as a conscious phenomenon, in spite of the experimental evidence
\item There appear to be feedback loops between bodily sensations and conscious perception - embodied cognition
\item These feedback loops are evidence against an additive model of drug-placebo interactions
\item Despite the centrality of such conceptions to the placebo effect, no theory has as yet incorporated these findings into their theories
\end{enumerate}

This section will briefly review the evidence in favour of the above propositions, then will elucidate how these could be combined into our theories of placebo and implicit measures, and will provide some testable hypotheses regarding the theory, in the spirit of Popperian falsification.

\subsection{Implicit Measures and Dual Process Models of Mind}

The notion of dual process models of mind is an old one, dating back within psychology to at least the time of Freud , and possibly before. However, the modern conception of dual process models is much more recent, and developed as a result of work with implicit measures and through the findings of cognitive psychology. Essentially, the modern theory suggests that there are two prevalant systems of reasoning inherent to humans, a slow, rational, conscious system, and a fast, frugal and implicit system \cite{Kahneman2002}. 

These two systems activate under different conditions and seem to perform different functions. One of the major hypotheses emerging from this theory is that under conditions of attentional strain, the implicit system takes over. This is, as we have seen in the previous chapter, borne out by much of the research into implicit attitudes. Asendorpf \cite{Asendorpf2002} demonstrated what came to be called the double-dissociation effect, where implicit measures (of shyness, in this case) were more predictive of performance on a spontaneous speaking task (the Trier Social Stress Test), while explicit measures were more predictive of considered, deliberate behaviour. These findings have been further replicated by other authors   Thus, we can take the results of the IAT research and that into other implicit measures as pointers towards the operation of this system. The major point to take from this section is that this system appears to exist, and yet has only been touched upon in one or two articles over the past decade \cite{Geers2005}. 

\subsection{Conscious Conceptions of the Placebo}

The dominant model within placebo research at present is the response expectancy theory of Kirsch \cite{Kirsch1985, Kirsch1997a}. This theory conceptualises the placebo as resulting from response expectancies, which are defined as ``the conscious expectation of a non-volitional response''. This theory has had some success, displacing the then prevalent theory of conditioned placebo responses \cite{Vuodouris1985}. That being said, the very definition of placebo and its nature as occuring as a result of deception and belief that one is getting a real drug would seem to suggest that non-conscious systems must be centrally involved in the mediation between awareness and the documented physical responses. 

Indeed, the Geers et al study cited earlier \cite{Geers2005} demonstrated that semantic priming (by means of a scrambled sentence task) was an independent predictor of the response to a sleep placebo, which given that semantic priming does not effect conscious awareness , implies that such priming (and the implicit system more generally) can affect the response to placebos and indeed biologically active treatments. Another study which points in the same direction is the Shiv et al 2005 \cite{Shiv2005a} study which demonstrated an effect of price of an energy drink effecting the number of puzzles solved by participants in a particular time. This effect disappeared when participants attention was drawn to it, which again implies that implicit systems were involved. Despite this evidence, the dominant theoretical framework remains untouched. In this chapter, I will propose a new model that incorporates both of these systems, and makes a number of predictions that will be either supported or rejected by further research. 

\subsection{Embodied Cognition and Placebo}

Another issue with most of the conceptions of placebo current in today's research is that they are almost exclusively cognitive, a point made most forcefully by anthropology researchers  %Add this citation. 
This seems strange, given that it rests on assumptions regarding the relationship of mind and body which all of the evidence of placebo would seem to argue against. It seems that the theoretical perspective within the field is that the mind may effect the body, but not vice versa. Unortunately, this is an untenable proposition, as recent work on haptic cognition (where judgements made by participants are affected by the sensory input they are receiving at the time), published in Science in 2010 %get this paper too
Indeed, more recent work by Cwir \cite{Cwir2011} suggests that awareness of one's own interioceptive processes appears to be a good predictor of people's ability to predict the emotional states of others. 



Indeed, there has been a resurgance of interest in mind-brain-body feedback loops within psychology and the social sciences more generally of late. It seems that the dominant cognitive model of mind (a kind of splendid isolation for the brain) is being slowly worn down by experimental evidence. An approach such as this was also proposed by Meissner et al in 2009 \cite{Meissner2009} in their meta-analysis which showed large placebo effects in some areas and none in others. They suggested that the larger placebo effects may have occurred in some conditions but not in others as the places in which placebo effects occurred tended to be those systems which have large nervous system connections with the brain, as opposed to communication typically mediated through hormones, which are many orders of magnitude slower. One problem with Meissner's theory is that he posits that placebo effects do not occur in clinical trials when the outcome of interest is a hormone such as cortisol. Howevever, the field of psycho-neuro-immunology has noted that psychological variables (specifically optimism) exert major influences on antibody responses. There is also some emerging evidence that mindfulness based treatments appear to affect antibody responses, at least in cancer patients. This begs the question of why the analysis by Meisnner et al found no such effect. One explanation for these conflicting findings would be that the placebo effect is mostly determined by current state effects, while those effects investigated by psycho-neuro-immunology are the result of certain trait like characteristics of individuals.  

One psychological state which may have an impact on placebo responses, and should if my theory is to hold is that of mindfulness. Mindfulness is often defined as a moment to moment awareness of somatic and mental processes. If, as the results of Geers et al suggest, somatic focus can increase the size of placebo effects, then mindfulness (or another proxy marker for somatic focus) should also moderate the size of observed placebo responses. One problem with this hypothesis is that it is not clear in which direction the relationship should go. One could make an argument that mindfulness could reduce the size of placebo effects, as if the feedback loop is made conscious, then it should lose its power, or one could argue that higher levels of trait mindfulness would allow for more attention to be placed on the somatic sensation, thus increasing the placebo response. This presents a difficulty in the attempt to lay out bold conjectures and then attempt to refute them. 

If attention is conceptualised as a limited resource, then placebo effects should be enhanced by placing participants in conditions of low stimulation while the treatment is applied. However, this hypothesis causes problems when we considered (below) the effects of participant provider interaction, which appear to account for a significant portion of observed placebo effects. 

\subsection{Additive Models of Placebo}

The common approach throughout clinical trials, and the study of placebo effects more generally, is that the effects of drug and placebo are additive. This assumption leads nicely to the principle that placebo effects and drug effects can be seperated precisely. However, not all of the evidence points in this direction, and (apart from statistical convenience) there exists no apriori reason why this should be the case. Indeed, the work of Geers et al on somatic focus would seem to suggest that paying attention to somatic experience can increase the size of placebo effects. This could be occuring because of a feedback loop whereby a treatment is applied, the patient's awareness of the treatment leads them to attend to sensations related to the treatment, which engages the body's own healing systems, which then increase the size of the effect over and above what would have happened without awareness of treatment on the part of the patient. It is perhaps for this reason that drugs which are adminstered by an automated process are less effective \cite{Benedetti2003}. 

Another issue to consider in terms of mathematical models of placebo response is the phenomenon of the active placebo. This is where an active drug is offered as a treatment for which it has no efficacy, and nonetheless this treatment produces larger healing effects than a typical sugar pill placebo. I would argue that this phenomenon occurs because the side-effects of the drug produce  a feedback loop whereby the treatment has a somatic impact, which alerts participants to the treatment, causes them to accept it as more credible, and thus activates the body's own healing systems. This process, over time, could easily create a conditioned response to the original non-effective treatment, and loops such as this could be responsible for the observations regarding the efficacy of conditioned placebo responses. In this sense, I am making the argument that conditioned placebo responses are turned into expectancies over time.   


\subsection{Social Aspects of Placebo}

In addition to the possibility of feedback loops between awareness and sensation contributing to the placebo effect, the role of the provider needs to be emphasised also. In most drug research, the treatment itself (the pill or cream) is only one element of the context in which the healing process takes place. In addition to the internal factors which shape response to treatment (optimism and expectancies more generally), there are also important social and environmental factors. For instance, the classic work of Gracely et al \cite{Gracely1985} demonstrated that the effects of the awareness of the provider can have a large impact on the outcome. In this study, half of the dentists were informed that half of the patients would receive placebo. 

In fact, all of them received the real painkiller. However, compared to the group treated by dentists who had not been told that placebo was a possibility, the other patients reported significantly higher pain. Indeed, a systematic review of healthcare interventions \cite{DiBlasi2001} provided evidence that provider characteristics accounted for a large proportion of the observed placebo effects. More recently \cite{Kaptchuk2008}, a three armed randomised control trial of acupuncture demonstrated that healing rates were greatly increased when the provider spent more time with the patient discussing symptoms and treatment (45 minutes as opposed to 15). This would seem to suggest that one of the reasons so many people use alternative treatments is not the efficacy of the particular form of treatment, but rather the chance to discuss their symptoms in detail and for longer period of time, while the average GP time per patient is currently around 5 minutes in the UK and Ireland. 

Another important feature of the context in which healing takes place is the social context, that is the shared beliefs and rituals that make up a culture. For instance, Valium has a powerful resonance in our culture, immortalised in songs by the Rolling Stones are glorified through media et al. Therefore, even when participants have never experienced the drug itself, they bring pre-conceived notions of what it can do, and apply these to their perception of treatment, which alters its efficacy. However, research using the open-hidden paradigm (discussed previously) would seem to suggest that Valium lacks any efficacy when participants are not aware that they are taking it \cite{benedetti2003}. An additional example of this effect can be seen in the prescription of antibiotics for viral infections. Even though doctors know that they will have no effect, they are still administered to patients. It would be extremely interesting to give people placebo antibiotics and assess its impact on viral infections, relative to the efficacy of true antibiotics. My thesis would suggest that real antibiotics should be slightly more effective, given their obvious side-effects, but that this difference in standardized means would be less than 0.1. 

% A final (in my estimation) variable on placebo response would seem to be the effects of environmental factors. There is some evidence that suggests that a view of nature decreases healing times, and that patients in rooms with sunlight recover faster than those in rooms 
% without a direct view of the sun. It can be argued that this is as a result of environmental influences activating particular implicit cognitions relating to health. It is worth noting that the sun has been consistently associated with health in many cultures across the globe, and that people's mood tends to improve when the weather is nice. 

\subsection{Towards an Embodied Conception of Placebo}

Bearing the previous sections in mind, we can now move forward into proposing a new model for placebo, the embodied placebo model. This model, while not rejecting outright the findings of the expectancy theory, aims to enhance it with more recent research. Indeed, this theory is also compatible with that of conditioned placebo, and also with the motivational concordance approach of Hyland et al \cite{Hyland2007}.  
The essential features of the embodied model of placebo are as follows. Placebo effects are those healing effects which arise from the perception of treatment or caring on the part of the health care provider or from the context surrounding the treatment. They are mediated by four different kinds of factors.


\begin{itemize}
\item They are mediated by implicit cognitions operating extremely quickly
\item They are also mediated by awareness of the treatment and cognitive models of its efficacy
\item In addition, these implicit and explicit attitudes are enhanced or degrenated by somatic sensations
\item They are mediated by the communications exchanged between the participant and the provider
\item They are also mediated by the system of communications surrounding both the participant and the provider
\item Finally, they are mediated by somatic feedback from the surrounding environment.
\end{itemize}
 
In an effort to advance research and to conclusively demonstrate either my own percipience or ignorance, I shall argue that my theory makes a number of key predictions. 

\begin{itemize}
\item First, that placebo effects will be correlated with scores on implicit measures
\item Second, that there will be an interaction effect between explicit and implicit attitudes on placebo response
\item Third, that increased interioceptive awareness (or mindfulness) will increase the size of placebo responses
\item Fourth, that the implicit attitudes of the practitioner will exert a significant impact on the placebo response of the patient \footnote{due to researcher constraints, this hypothesis was not tested in this thesis}
\item Fifth, that adding sensory input to a placebo treatment will increase its effectiveness. 
\item Sixth, that the extent of changes in physiological measures in participants will be correlated (as a lagged variable) with the size of the next reported pain rating.
\end{itemize}

However, in order to properly test this model, it needs to be compared to other plausible models. The first of these is the model of Krisch, noted in his 1985 paper. This model claims that expectancies are the major (indeed only) mediating factor between consciousness and placebo responses. Therefore, this model suggests that there is a direct effect of exxpectancies on placebo response, that physiological outcomes should not be predictive of the placebo response and that any effect of optimism and mindfulness will be mediated through expectancies. This model claims that there will be no direct effects of any other explicit or implicit measure on placebo response, but that they will all be mediated by expectancies. 

Another, equally plausible model is that optimism is the driver of the placebo response, and that the effects of expectancies are mediated by levels of optimism. This model would suggest that both implicit and explicit optimism will have direct effects on the placebo response, and that expectancies (implicit and explicit) shall be mediated by levels of optimism. This model makes no predictions about the effects between physiological response variables and placebo response. 

These three models will be examined using a structural equation modelling approach, which should allow for an efficient and accurate test of my theory, even if the population is not particularly representative. 


\section{Qualitative Research Methodology}

Qualitative research typically relates to the analysis of interviews and other texts derived from people. It differs fundamentally from quantitative analysis in that it aims for a deep understanding of particular individuals, while quantitative analysis aims for a broad understanding of the sample as a whole. However, as the discussion above relating to inter-individual variability shows, sometimes it is better to focus on small groups of people in order to gain insight at this level before applying this knowledge to develop instruments which can be more easily applied to larger samples. The biggest problem with qualitative analysis is that it cannot scale to the level of large scale surveys, as it requires significant amounts of researcher time per participant. % while quantitative surveys have a cost of development in time, but the marginal cost of administering the survey to a new participant is essentially zero (assuming distribution over the internet).

Qualitative analysis was an essential part of this project, as it gave insight and data into the development of the IAT's used in the final part of the research project. The methods used for qualitative analysis here were twofold, firstly, a thematic analysis \cite{braun2006using} of the interviews conducted was carried out to develop themes both for the repertory grid and for the IAT, and secondly, an Interpretive Phenomenological Analysis \cite{smith2003interpretative}  of this same data was also done, in order to develop an understanding of how the participants conceptualised health, in order to feed into the development of a theory of placebo and to provide insight into the responses garnered from more structured methods of analysis (i.e. surveys and experiments).

The issue of reflexivity is crucial to qualitative research (and also appears in quantitative research, though rarely as openly) \cite{rosenthal1967covert, rosenthal1969interpersonal}.
Reflexivity refers to the impact of the researcher's prior conceptions and approaches have on the course of the interviews \cite{finlay2002outing}. This is extremely obvious in the choice of the major questions to be asked in the interviews, but it can occur in subtle ways during the interviews also (for example in the use of language by the interviewer) and in the quality of communication or rapport experienced by the researcher in the course of the interview. Reflexivity is also critical during the analysis, as the researcher must be aware of their own biases and ensure that this affects the analysis as little as possible, or at least report where the problems arose for them.

\subsection{Thematic Analysis}

The thematic analysis's primary purpose was to look for common patterns in the conceptualisation of health, sickness and treatment. As such, it was felt that the best approach would be to develop the codes from the transcipts themselves. This is called an inductive approach to coding of the data \cite{haberman1979analysis}.   Following transcription, each interview was coded line by line by the primary researcher, and codes were developed throughout this process.

After all the interviews had been transcribed, the codes were pruned and amalgamated to reduce redundancy, and this process was repeated. This second coding lead to a number of new codes and insights which had been missed the first time, and the text was again coded for a third time following the development of these new codes.

Then, the document was coded a fourth time, but on this run through the aim was to look at higher level patterns that emerged from the text. Following this coding procedure, a process of chunking of codes was carried out. This involved looking at how codes fit together and grouping them under a number of thematic headings. The original texts and recordings were referred back to at this point to ensure that the themes were representative of the original data, and finally the themes were written up to record the results of this exercise.

% \subsection{Interpretative Phenomenological Analysis}

% This method is quite a distinct approach within the field of qualitative analysis of text, and it focuses on the individual ways in which participants construct their experience and the world around them\cite{smith2003interpretative}.  This differs from the aims of a method like grounded theory \cite{glaser1977discovery} where the commonalities between participants are looked for and examined in the light of the developing theory. It also differs from the aims of the thematic analysis, where codes were chunked into higher level themes and patterns common to all participants were examined.

% In this, the aim was for an understanding of the individual participants experience, and no attempt was made during the coding process to link the positions of one participant to those of another. A similiar coding process as before was followed, except that the codes were developed individually for each participant, and much more use was made of the pauses and semi-verbal expressions (such as sighs and laughter). Following the analysis and coding of the data for each individual participant, the analysis was then repeated across participants while looking for some common patterns.

% In addition, some discursive approaches were taken to the texts. To some extent this was unavoidable, given that most of the interviews took place with healthcare professionals (conventional and alternative) and given the researcher's background in placebo, some of them may have felt that they needed to justify their positions, or indeed some may have felt threatened by this research. This however, was not a full blown discourse analysis, but rather a discursive perspective was taken on the interviews carried out.

\subsection{Repertory Grids}

The use of repertory grids in this research was as a bridge between the qualitative analysis carried out and the quantitative side of the research. Repertory grids were developed by George Kelly as an aid to therapy \cite{kelly2003psychology} although it has been used in many diverse situations in the ensuing years. Repertory grids were developed out of Kelly's theory of cognitive consistency, an active area of research which fell out of favour following the discovery of cognitive dissonance by Festinger in 1947 \cite{greenwald2002}.

The premise of the technique is simple. Firstly, participants are supplied with a list of important people in their life, such as their mother, an older sibling and a teacher whom they liked or disliked. They write down the names they have chosen for each person, and then they compare the people in groups of 3. For each group (or sort), they are asked to describe how two of them are similiar and also how one of them is different in a word or short phrase. These words or phrases can then be analysed both quantitively or qualitatively.

The primary method of quantitative analysis was through factor analysis, which has been extensively covered above. The approach taken in this project was as follows. In one of the surveys carried out on the UCC population (TCQ version 1) participants were asked to rank the most important people in their life who were related to healthcare. This data was then sorted and ranked, and a list of the most common people used was compiled into a health related repertory grid. This was then administered to a small sample (N=17) to test the instrument. The results of this testing are described in Chapter \ref{cha:preliminary-research}, in Section \ref{cha:preliminary-research-3}.


\section{Quantitative Research Methodology}
\label{sec:quant-rese-meth}
This section  will describe the methods employed for each part of the thesis and provide a rationale for why these methods were used in the thesis.
A mostly quantitative approach was taken for the following reasons. Firstly, the placebo  is a very noisy phenomenon \cite{Singer2005}, subject to many sources of error and bias (described in Chapter 2 under the heading of the Concept of the Placebo ). Secondly, in order to predict the placebo effect, there needs to be some kind of measure, and these are typically metered in numerical terms. 

This thesis consisted of three main parts. Firstly, following a thorough literature review, the major constructs associated with the placebo effect and implicit measures were identified. These constructs were then administered to large samples of the population from which the experimental participants were drawn. This procedure was carried out for two reasons. In the first case, this was so that the population means could be estimated more precisely and thus the experimental sample compared on these measures.  

The second reason was so that more sophisticated models could be developed for person responses (which typically require larger samples than are common to experimental studies) and could then be applied to the experimental sample. This approach marries two of the strengths of psychological research; firstly, the latent variable approach common in psychometric research; and secondly, the use of rigorous experimental design to determine casuality. This thesis aimed to use both of these strengths in combination to gain insight into the causes underlying the response to placebo in healthy volunteers.

The second major part of the thesis was the development of the implicit association tests (IAT's) and the explicit measure of expectancies (treatment credibility questionnaire) used in the experimental portion of the research. 

The third, and final, part of the thesis was the testing of these measures in an experimental setting using a placebo analgesia design examining the response to ischemic pain in healthy volunteers. 

\subsection{General Statistical Approach}

In this section, the general approach taken towards the analysis of numerical data taken will be described and background information will be given on the methods.
The analysis of survey data has already been described in Chapter \ref{cha:literature-review} . In this section, the remainder of the statistical tools used in the thesis shall be discussed.  Firstly, time series analysis and the motivation thereof shall be examined.   Following this, the approach taken towards the analysis of reaction time data shall be detailed, and the methods of measurement of all experimental data points will be described.


\subsubsection{Time Series Analysis}

Time Series Analysis is an extremely widely used statistical tecnhique.  The models in use today in the social sciences tend to be ARIMA models and their descendents, and these were developed in the 1970's by Box and Jenkins\cite{box1970time}.  \footnote{ARIMA stands for Auto Regressive Integrated Moving Average models}, The features of these models are described below \cite{mccleary1980applied}.

The general framework for the models is as follows. Firstly, the time series process is assumed to be stochastic and a random walk. Given the observations at time $t$, the observation at $t+1$ is assumed to be generated by a random shock from a normal distribution with mean 0 and standard deviation 1 (the standard normal). This procedure is repeated over time, and the time series thus progresses and changes due to the compounded influence of all of these random shocks.

The series at time t can be modelled as follows: $Y_t=\phi Y_t+\phi Y_{t+1}\ldots+a_t$.

Time series analysis was used in this research mainly for the physiological data, though the pain ratings were also modelled as a function of time. This kind of analysis is important as it allows for the dependencies between successive scores to be examined, which is a factor often neglected in placebo research. 

% The most important feature of a time series model is parsimony, as there are many ARIMA models which can fit a sequence equally well, and the best approach is to start from an extremely simple model and add extra parameters until an acceptable level of fit is reached.  In order for any kind of ARIMA model to be fit, $N>50$, which is often quite difficult in some applications. Given the nature of the time series data included in this research (physiological series measured once per second) this was not expected to cause any problems. It was however, a problem for some of the pain ratings, as these were only collected once per minute. Resolutions for this problem are discussed below. 

% \paragraph{Assumptions of Time Series Analysis}

% There are a number of conditions that must be met in order to carry out a time series analysis:
% \begin{itemize}
% \item Each observation is considered to be on an interval scale;
% \item The time between each observation must be constant;
% \item  The time series must be stationary in both level and variance;
% \item  The data generating process must be identified;
% \item  The residuals must be white noise (i.e. drawn from the standard normal).
% \end{itemize}

% \paragraph{General Definitions}

% In the context of a time series $Y_t$, \textit{trend} refers to movement in a specific direction; i.e. any systematic change in the level of a time series. If a time series in trendless, and the subsequent values are generated as standard normal random shocks, then the best estimate of the time series at any point in the future will be the mean. However, very few time series are trendless, there is often something which moves the series in a particular direction. There are two options for dealing with trend: it must either be removed, or it must be modelled. Older texts on time series suggest regression as a means to remove trend, but this is not recommended as it will be heavily biased by outliers \cite{mccleary1980applied}.

% % Mathematically, a time series can be regarded as a random walk, that is, the independent realisation of a long sequence of independent and identically distributed events (the random shocks) \cite{venables2002modern}.
%  A better way of dealing with trend in a time series is through differencing, where each observation is subtracted from the last observation. This allows us to consider the time series as particular individual changes from one observation to the next, having accounted for (by differencing) all of the changes up to that point. A differenced time series can be referred to as $z_t=Yt-Yt-1$.

% The general form of ARIMA models is as follows: ARIMA(p,d,q) where $d$ is the number of differences required to make the time series stationary. In most social science cases, the order of $d$ will typically be less than 2 \cite{mccleary1980applied}.

% In the general form of the ARIMA(p,d,q) models, $d$ has already been discussed. In this context, $p$ refers to the number of lagged observations that must be retained in order to model the current observation $Y_t$. This is referred to as the Auto Regressive portion of the model, where $p$ is the autoregressive parameter. If the ARIMA model is (p,0,0) then it is already stationary, but other processes can be differenced to reach this point. Some authors note that first order AR processes are most common in social science applications \cite{mccleary1980applied}. The autoregressive parameter $\phi$ falls between -1 and 1 (if it does not, this represents a serious failure of the model) and thus, each lagged term converges towards zero (as the parameter is raised to the power of n lags).
% % If $\phi$ is not constrained, then past shocks would become exponentially more important as time went on, which defies the common experience of processes moving over time.

% Following the differencing procedure, the time series should be stationary in level. However, while this is necessary for an analysis of time series, it is not sufficient. In order for the time series to be analysed, it must be stationary both in level and in variance. One common way to acheive variance stationarity is by using a natural log transform. One major advantage in this approach is that coefficients garnered from this time series can merely be exponentiated in order to intepret them on the scale of the original data \cite{gelman2007data}.  


% The final term is the general form of ARIMA models, $q$ represents the moving average. Essentially, it is a measure of how long a random shock stays in the system before its effects can be ignored. An ARIMA (0,0,2) model would imply that the current term is made up from the previous two terms, and no more are necessary. Again, the parameters need to be constrained to the range between -1 and +1 \cite{mccleary1980applied}.


% \paragraph{Identification of ARIMA models}

% The models discussed above represent the general form of ARIMA models. These need to be estimated from the data, and this section deals with  methods of doing this. The general approach here is to use what is called the Auto Correlation Function (ACF)\cite{mccleary1980applied}. Essentially, the ACF is a generalised correlation coefficient for dependent (i.e. time-series) data. The ACF is normally calculated for different lags, and the results plotted. These plots are then primary tools to identify the ARIMA model which best fits the data. For example, an ARIMA(0,0,0) process will have ACF's for all lags which are essentially zero. An ARIMA(0,1,0) will have equal ACF for all lags. The ARIMA(0,0,1) will have a non-zero ACF(1) and all others will be zero. More generally, an ARIMA(0,0,q) will have $ACF(1)\ldots, ACF(q)$ which are not significantly different from  zero.ARIMA(1,0,0) processes will have exponentially decaying ACF's. Examination of the ACF plots and their absolute value can also determine whether or not a time series needs to be differenced.

% In addition, the Partial Auto-Correlation Function is also used in the identification of ARIMA models. The PACF is defined as the correlation between $t$ and $k$ after all lags inside that interval have been partialled out. Essentially, the PACF is equivalent conceptually to the differenced time series \cite{mccleary1980applied}.

% An issue which often causes problems for time series analyses is that of seasonality. Seasonality is defined as recurring patterns that occur at a higher order level of the time series. For example, time series of retail sales tend to spike in December and January, and this is an example of seasonality. Generally, seasonal effects are modelled using a higher order ARIMA(p,d,q) structure so a typical model of retail sales data might look like this ARIMA(2,0,0)(0,0,2). However, seasonal patterns are much rarer in physiological data, and so they will not be further discussed here.

\paragraph{Practical Model Building Strategy}

The following are general steps towards building an ARIMA model.

\begin{itemize}
\item Inspect the plots both of the series and the autocorrelations
\item Examine autocorrelations, differencing if necessary
\item Estimate parameters, ensuring that they are significantly different from zero, and within the bounds of invertibility
\item Examine residuals, if they are not white noise, repeat steps 1-4 until they are.
\end{itemize}

\paragraph{Event History Analysis}
\label{sec:event-hist-analys}
In this research, a particular form of time series analysis was used, which is known as event history analysis \cite{mccleary1980applied}. This type of analysis partitions the time series into two or more parts, based on whether or not a particular event has occurred. In the case of this research, there were either two or three parts to the analysis. In the Deceptive and Open Placebo groups (see Chapter \ref{cha:primary-research}), the first time series occcurred until the painful stimulus was applied, the second was the time from this point until the placebo was applied, and the third was this time point until the end. In the No Treatment group, there were two time series, one for the period before the pain was applied, and one after.

A major advantage of this method is that it allows us to examine changes in the parameters of the time series as a function of experimental stage and condition. This allowed us to estimate more precisely what changes occurred over time as a result of experimental procedure. The basic procedure is as above, except that parameters are estimated on a subset of the data, and cross-correlation functions are used to examine the changes between them.


\subsubsection{Regression Models}
\label{sec:regress-models}

At the heart of typical psychological modelling practice lies the general linear model.This model underlies such familiar techniques as correlation, regression and analysis of variance (ANOVA)\cite{gelman2007data }. These techniques are based on the idea of fitting a straight line (or plane, in the case of multiple predictor variables) to the observed data and using this line to make inferences about the relationships between variables of interest.

The models are typically fitted by a least squares method. Least squares is a criterion which suggests that in order to fit the best line, the average vertical distance between the points should be minimised. Least squares is the heart of many social and physical science modelling techniques, and is formalised in the Gauss-Markov theorems which prove that if the assumptions of the model are met, then in the limit, the least squares approach is the most efficient unbiased estimator\cite{friedman2009elements}.

% The assumptions of the general linear model are as follows:
% \begin{enumerate}
% \item The residuals of the model (the distances between the predicted line and the observed values) should be approximated by the standard normal distribution
% \item The variables should have constant variance across their entire range (homoscedasity)
% \item The residuals should be independent of the response variable

% \item The residuals should be uncorrelated with one another
% \end{enumerate}

% The general linear model has been elaborated greatly over the last century, and has been applied to the approximation of relationships that do not meet all of the assumptions noted above\cite{gelman2007data}. The introduction of link functions (non linear functions designed to transform the response variable into a form suitable for the model)  led to the development of generalised linear models, which allow the same computational techniques to be used to fit and test models for data which does not fit the requirements of the standard linear model \cite{mccullagh1989generalized}.

% For example, logistic regression is a technique for prediction of binary variables using a link function. Poisson regression is used to model data which is bounded by zero and positive infinity. A number of quasi methods are available for both of these techniques which allow models to be fitted to  data which has an excess of zeros (so called zero inflated models) \cite{gelman2007data,venables2002modern}. In another direction, the requirement for the residuals to be uncorrelated has been relaxed to allow for the development of multilevel or mixed models, which allow particular groups residuals to be correlated with one another\cite{gelman2007data}. %The topic of mixed models will be returned to below in the context of time series analysis (where the observations, by definition, are not independent).

A number of major issues arise with the use of the general linear model with psychological data. Firstly, the requirement of normal errors can sometimes be difficult to satisfy. This follows from the manner in which the normal distribution appears to behave. When it was originally discovered (by Gauss) \cite{stigler1986history} it was used to model the combination of many small, independent variables. It tends to work well as an approximation when there are many independent variables affecting the results of an analysis.

However, when psychological tests (such as self report instruments) are developed, the aim is to remove as many of these small influences as possible so that the measure taps one construct with clarity. This will often lead to a non-normal distribution of errors. Non parametric approaches are an alternative to the General Linear Model, but these often lack the power of their parametric alternatives.The central limit theorem assures us that, in the limit, any distribution of means will converge to a normal, and in practice perhaps as little as 100 observations may suffice for a t-test \cite{venables2002modern}. 

A more serious problem (in terms of the impact on outcomes) is heteroscedasity, where the response variables (or the predictor) do not possess constant variance across their range\cite{gelman2007data}. This can seriously affect the models built as points with less variance will be much more influential than those with a lower precision \footnote{precision is the inverse of variance\cite{gelman2007data}} . The assumption of homosecdasity can be checked by formal statistical tests, but often graphical tests of this assumption are much more revealing, as they can show where the model fails as well as whether or not it fails.

In this research, linear, logistic and mixed models were utilised throughout all of the quantitative research in order to test hypotheses. In deference to common convention, frequentist hypothesis testing was applied to examine the evidence for particular propositions. 

% A final issue in the use of linear regression is the popularity of stepwise approaches to variable selection in regression models\cite{antonakis2010looking}. In these approaches, the computer is told which variables to start with, and an alogrithmic approach to the analysis is taken, whereby variables are added or removed from the model on the basis of their p value (or AIC). These models, which appear to remove all responsibility from the researcher, lead to biased estimates, far too narrow confidence intervals and incorrect F tests \cite{antonakis2010looking} \cite{gelman2007data}. Indeed, Antonakis \& Dietz found that using stepwise multiple regression they were able to fit a model explaining 80\% of the variance to 20 variables distrbuted normally, which is merely fitting a model to noise. However, this problem occurs even when variables are added or removed from a model manually, as the false positive rate increases linearly with the number of models fit. The $R^2$ tended to decrease as they increased their sample size in the MCMC study, but any appreciable model should not be constructable from pure noise \cite{antonakis2010looking}.

% There are solutions to this dilemma. Perhaps the most justifiable solution is to only add or remove variables from the model based on theory. This is an excellent solution which if followed consistently would lead to much more parsimonous and replicable models. However, any approach which starts with a model and then adds or subtracts variables from it, whether driven by theory or an algorithmic approach, still suffers from the multiple comparisions problems and needs to have the p values adjusted \cite{friedman2009elements}. 

% The major problem with this approach is that it removes the opportunity for serendipituous findings from research. While the use of theory driven regressions may cut down on the discovery of spurious relationships, it does so at the cost of removing all opportunity for unexpected findings.

% One simple solution, which can be implemented in all statistical packages is through the use of cross-validation. With this approach, predictor variables are selected on N-1 splits of the data, and the model is then tested on the remaining split. This avoids the multiple comparison problems which stepwise variable selection methods incur, and allows for accurate p-values and standard errors to be estimated from the data, while allowing for serendipituous discovery of accurate models. This approach also harnesses the ability of the computer to fit an extremely large number of models and retain those predictors which perform the best. In this research, this form of stepwise selection was used, selecting predictor variables using a psuedo-AIC which penalises models with more parameters \cite{venables2002modern}.  

% Another solution lies in regression methods developed in the last thirty years which are designed to compensate for the deficiencies of stepwise approaches. These methods are known as lasso, ridge and least angle regression, and the premise behind them is simple. They allow variables to enter and exit algorithmically, but they penalise the coefficients towards zero each time this occurs\cite{friedman2009elements}. This allows for the effects of overfitting to be greatly reduced while at the same time enabling a variety of models to be fit to limited data. Combined with cross-validation of all models, these approaches hold real promise for reducing spurious findings and increasing the usefulness of models for prediction in this research.

\subsubsection{Analysis of Reaction Time Data}

Reaction time data has been studied by (mostly cognitive) psychologists for many years. The Implicit Association Test has been used in almost 300 published papers and reports (and doubtless many more times where the results were not published). However, with a few exceptions, there has been almost no overflow from one area of study to the other.

Classic work in examining the distributions of reaction time data was carried out by Ratcliff\cite{ratcliff1979group}\cite{ratcliff1993methods}. In the 1979 paper he suggested that a quantile based approach should be used for individual reaction time scores. This involves ranking each of the latencies for each individual participant, and using certain percentages of these as quantiles, which can then be used to estimate group distributions. This approach will be utilised in this research, and four quantiles will be used, as given that some conditions (Blocks one, two and four) have only twelve observations, and quartiles divide each of the block sizes (12 and 36) equally. These quartiles were then used to estimate group, block and condition level distributions for the reaction time data. 

The typical approach to analysis of IAT data goes as follows  \cite{Greenwald1998}: firstly, the data is checked for outliers. Outliers, in this case are defined as responses less than 300ms and greater than 3000ms. Any such outliers are recoded to 300 or 3000ms respectively. Following this procedure, a mean is taken of the items in each condition. These means along with standard deviations are reported. The response latencies are then log transformed (to reduce positive skew) and the IAT score is calculated as follows. 

Given the participants mean latency in each condition, their IAT score is the mean for the incompatible condition (i.e. White + Unpleasant) less the mean from the compatible condition (i.e. White + pleasant) divided by the average of the two within group standard deviations. This measure is typically called $D$, and was developed after extensive analysis of an extremely large sample of IAT responses\cite{Greenwald2003a}.In addition to the change of scoring procedure in the 2003 revisions, the threshold for outliers was also substantially widened to 10000 ms. Given that the sample used for this re-analysis was over one hundred thousand, this widening of the threshold was presumably based on experience with a much broader population than was used in many early IAT experiments (typically college students). 

There are a number of problems with this approach. Firstly, the approach throws away much information, more than once. In the first case, information regarding extreme responses is censored, in an unsystematic and theoretically unjustified manner. The breakpoints of 300 and 3000 milliseconds appear to have been chosen to allow for the use of the mean as a group value rather than for any principled reasons. 

It is arguable as to whether or not this censoring represents a good strategy, but certainly it is something which should have been examined in a principled fashion, which does not appear to have happened. The second issue relates again to the calculation of a mean.

While means are useful summary statistics, they are most optimal in situations where the distribution is unimodal and symmetric\cite{venables2002modern}. Reaction latency data are neither, so the choice of mean seems to have been made from familiarity rather than principle. 

The choice of log transform (while slightly more justifiable) is decidedly inferior to a more data driven approach. Again, the issue here is not that such choices in analysis were made, but rather that they have been made once and repeated many times in the later literature. Even those who have criticised the IAT \cite{Klauer2005,Mierke2003,Blanton2006} on methodological grounds appear to have ignored this issue. Possible resolutions of this issue are discussed below in Section \ref{sec:experimental-data-analysis}

Given the typical right skew observed in reaction time distributions, the median would seem to be a much better measure of central location than would the mean. This right skew typically occurs as there is a hard bound on how quickly a participant can respond, but no such bound (unless enforced by the procedure) in the maximum time taken to respond. Therefore, in this research, medians will be used for all reaction time based measures. The differences between the mean and median based approaches will be examined and reported, to determine if it makes any difference. 

\subsection{Problems of Sample Inference}
\label{sec:probl-sample-infer}
\subsubsection{The Problem}

In every statistical approach, the core is the development of inferential tools to reduce our uncertainty about the events under study\cite{gelman2010philosophy}. Given that we typically lack infinite resources, sampling from populations in a randomised manner is used to approximate the quantities of interest\cite{venables2002modern}.

However, we are rarely interested in the specific sample we have recruited; we tend to want to infer properties of the population  from which they are drawn.  In non technical terms, given a sample and some analyses, we develop a model which we hope will predict the behaviour of future samples (and indeed the population).

This approach toward inference is often operationalised in the creation of a model, whether based on the results of a linear regression or factor analysis. Typically, we aim to maximise the amount of the response variable(s) in a sample of size $n$ explained given some number of parameters. It is trivial to see that as the number of parameters ($p$) increases, so does the fit - in the limit, this would involve the fitting of a model with a seperate parameter for each observation. Clearly, such a model will violate the principles of parsimony and clarity that we aim for in our science. However, even when $p$ is less than $n$, we still run the risk of overfitting a model to our data. Overfitting is said to have occurred when we model features of the data that are essentially random (i.e. noise) \cite{friedman2009elements}. 

% Because factor analysis is lenient towards mis-specified models and tends to model error as well as signal, many psychometric theories have faltered on the rock of replication\cite{fabrigar1999evaluating}. SEM is often used as a panacaea for such problems. However, a Structural Equation Model is only as good as the data and theory behind it, and if the factor analysis models noise, so too will an SEM on the same data set (to a lesser extent, of course). This may account for the poor replicability of psychological theories based on the results of factor analysis and SEM.

\subsubsection{A Solution}

The typical scientific approach to this problem is simple - replication. Replication, preferably by independent researchers, is supposed to ensure that models eventually tend towards the minimum of parameters for the maximum of explanatory power.

Indeed, many fit indices penalise complex models over simple models . This suffices for some research, but there are cases where such an approach does not prove useful. Replication can also be fraught with difficulties, as it takes time, effort makes the assumption that population quantities are stable over time. Nonetheless, it is the ideal solution. However, given that replication is not incentivised by the scientific community (and may be unethical in some situations), researchers in the field of machine learning have come up with a novel approach which appears to improve predictive accuracy and can also aid in the development of theoretical understanding \cite{friedman2009elements}.

\subsubsection{Cross-Validation}

The solution proposed by those machine learning researchers is at once simple and elegant, while also extremely practical. The technique is known as cross-validation, and its application routine in commercial and scientific data-mining settings. However, it does not appear to have found much favour within psychology as of yet (with some notable exceptions \cite{dawes1979robust}).

The basic premises of the techniques are:
\begin{itemize}
\item All models are wrong;
\item Models are best tested on independent samples;
\item Independent samples are sometimes hard to come by;
\item Therefore, datasets should be split into training and test sets, where the model is developed on the training set, and its accuracy assessed on the test set.
\end{itemize}

This approach seems to improve predictive accuracy by an order of magnitude, especially when applied to large data sets \cite{breiman2001statistical}.  
% These tools can greatly aid in this task, if used with caution and due care \cite{friedman2009elements}.-

The principle of training and test sets has since been generalised to $k$-fold cross validation where the dataset is split into $k$ random pieces, and all but one of these are used to estimate a model, while the other is used as a test. This procedure is repeated $k$ times, and the results are averaged to form the best model (for that sample of data, at least). Some authorities argue that this procedure should be repeated at least $k$ more times, to control for the effects of random sampling\cite{friedman2009elements}.

Another variation on the central approach is leave-one-out cross validation, where given a sample of $n$ observations, fit a model on $n-1$ and test on the other, a total of n times. This approach, while taking the technique to its logical extreme is not of particular usefulness to us at this point, as large inter-personal variability between individual participants typically observed in psychological data would tend to reduce its efficacy\cite{friedman2009elements}.

In essence, cross-validation is an extremely valuable technique which has been mostly ignored in psychology. It is the opinion of this researcher  that this technique is useful, and it will be applied consistently to this research.

\subsection{Treatment of Missing Data}


Missing values can often cause a problem with large datasets. The
traditional approach has been to delete either pairwise (dropping
all variables that have missing values for a particular analysis)
or listwise (dropping all variables that have any missing values) \cite{graham2009missing}.
Both of these methods are flawed. The pairwise method can cause issues
with interpretation in that different analyses will be based on different
samples and degrees of freedom. The list wise method is slightly better,
but it does make the assumption that the missing values are missing
completely at random (MCAR) which is often not tenable \cite{graham2009missing}.

The primary deficiency in the above methods is their granularity. Both methods discard information in a rather crude way. Another approach towards the treatment of missing data is mean imputation, where any missing value is recoded as the mean for the sample. Predictive mean matching is a similiar approach which substitutes the mean for the respective variable containing missing data instead.

The major issue with mean (or median) imputation methods is that they artificially reduce the variance of the data. As more of the sample tends towards missing, this can lead to artificially centred variables which when used in model building, provide inaccurate standard errors and lead to poor inferences.

A more principled approach to missing data was developed by Donald Rubin, a statistician concerned with the problem of missing data in large public datasets such as the census or longitudinal studies\cite{little1987statistical}. The basic idea is quite simple, and has been generalised far beyond its original goal of filling in data in large representative samples.

The method is multiple imputation, and the process is as follows. Firstly, the missing values in the dataset are predicted given the information available in the non-missing values. Some noise is added (typically Gaussian, though bootstrapping from the observed distribution is often a better approach) \cite{gelman2007data} in order to prevent the computer from predicting the same values again, and these new values are then used as the starting predictors for the next imputation. This process is done a number of times (3-15, depending on the proportion of missing data) \cite{graham2009missing,little1987statistical} and then the the analysis is run on each of these datasets seperately, and the estimates are combined at the end of this process. The major advantage of this method is by looking at the variance of the estimated parameters, we can approximate the uncertainty which surrounds our method of imputation. In addition, the repeated draws ensure that chance features of the data do not lead to erronuous conclusions, as could easily be the case if a single imputation method were used.

Throughout this research, multiple imputation was used where the amount of missing data was substantial. Typically, between five and fifteen simulated data sets were created, and models run seperately across all imputed datasets, with inferences combined at the end. The multiple imputations were then analysed and tested to ensure that the simulated data was an accurate reflection of the non-missing data, in line with best practice \cite{abayomi2008diagnostics}.

% \section{Approaches to Statistical Inference}

% The dispute between Bayesian statistics and frequentist methods has run for over a century now, and the pendulum of use and acceptance has reversed a number of times in that period. In this section, the main differences and similarities shall be highlighted and an argument will be made for an approach which combines the best of both approaches.

% The frequentist paradigm is the dominant approach currently in most experimental sciences \cite{gill2002bayesian,gelman2004bayesian} . The frequentist approach regards the probability of an event as the long run frequency of this event given repeated sampling from a particular population. The primary focus of frequentist statistics is the null hypothesis which represents the state of knowledge of the researcher at the beginning of a particular piece of research. The null hypothesis (hereafter $H_0$) is that any difference between the two samples in an experiment (typically treated and control) is due to chance variability in the measured quantities. The parameter values in the population are regarded as fixed but unknown, so from this perspective, it is meaningless to apply probability to them \cite{gill2002bayesian,gelman2004bayesian}.

% The essential idea behind the approach of the null hypothesis is that the score on the test assessing the hypothesis of no difference must be sufficiently unlikely to have occurred by chance, given an assumed distributional form for the errors of the response variable (typically  the normal distribution).

% This approach to statistical inference has much in common with the mathematical method of proof by contradiction. The measure used to assess this is called a p-value, and represents the probability that data as or more extreme than the test statistic would have occurred, given that the null hypothesis is true. A low p value (typically less than .05) suggests that the pattern of data between treatment and control is unlikely to have occurred by chance, and argues in favour of the rejection of the null hypothesis, in favour of an alternative hypothesis, which is usually specified by the researcher.This p-value of less than 0.05 was suggested by Fisher to be used when the researcher had little prior knowledge regarding the likely differences between groups, but it has become reified into much of the scientific community in the years since \cite{gigerenzer2004mindless}.

% A number of issues arise from this definition and description. Firstly, given the definition of probability given above, it can be seen that the p value represents the fact that if the same experiment were to be run in exactly the same fashion using the same size sample from the same population then (assuming the results were significant at p<0.05) in 19 of 20 replications, the experimental data would again be significant.

% One problem with the use of this binary threshold is that the p value estimates the likelihood of the null hypothesis given the data, which is rarely what the researcher is interested in. More typically, the researcher would like to know about both the existence and size of the relationship between the two (or more) quantities of interest\cite{cohen1988statistical}. The p value does not supply this information, and needs to be supplemented by what is called an effect size which measures the degree of association between two or more variables in standardized units. Perhaps the most easily understood of the effect size measures is the coefficient of determination, or $R^2$.
% This measures the proportion of variance explained by a relationship between two variables, and is often computed by squaring the raw correlation coefficient. More formally, it is the ratio of the sum of squares included in the model divided by the total sum of squares in the data. As such, it is not relevant for most nonlinear and mixed models, though equivalent measures can sometimes be computed\cite{gelman2007data}.

% A second problem which can often arise in applied research, is the possibility of Type I error when examining a large number of relationships in one data set. Given the threshold value of .05 for each comparison, and if $n$ independent comparisons are undertaken on the same data set, then the probability of a false positive increases linearly with the number of comparisons, and reaches 1 (implying certainty) when the number of comparisons reaches 20 or greater.

% For this reason, a correction is often applied to p values, the most popular being the Bonoferroni correction, which divides the required p value by the number of comparisons which are to be made. However, this approach also has its problems. The existence of such comparisons, and their likelihood of rendering the major hypothesis non-significant leads to a situation where either researchers fail to report the analyses which they actually performed in order to retain their significant p value, or only analyse the data to the extent that their hypotheses are either confirmed or dis-confirmed.
% Neither of these situations are scientifically optimal. The first situation encourages the propagation of false positives throughout the scientific literature \cite{ioannidis2005most}, while the second discourages researchers from making the best use of their data.

% A further issue which can arise when using classical methods is the problem of replication. Confidence intervals and statistics (such as p values) are based on the notion that if an experiment were repeated 100 times, the true point estimate would lie within the interval 95 (for a 95\% confidence interval) times out of this 100.

% There are two problems here, one quite obvious and the other quite subtle. The obvious problem is that given a particular confidence interval, we have no way of telling whether or not this interval contains the true value of the parameter which we are attempting to estimate\cite{gill2002bayesian}. The second problem is one of language, where most scientists think and talk about confidence intervals as though they represent a 95\% chance that the true value lies within the interval. This is simply not true, and can lead to issues of ambiguity when the true definition is unknown or forgotten, which appears to be quite often \cite{falk1995significance} (at least in a sample of psychology students and lecturers).

% Another large problem with the notion of replication on equivalent samples as being the arbiter of our probability statements is that it violates Birnbaum's likelihood principle, which is a foundation of modern statistical thought\cite{gill2002bayesian, gelman2004bayesian}. The likelihood principle states that all inferences should be based only on the observed data. The classical methods fail this test through the supposition of arbitrary numbers of experiments which are not carried out in order to justify the uses made of probability.

% A similar issue arises when looking at data-sets common in many disciplines which study people. This is that it may be (for many applications, anyway) impossible to precisely replicate the phenomena which we are using statistics on. In a study of time series data for economic interpretation, by a strict following of the assumptions, we should consider a number of replications where the events of the past turned out differently, and use these as our basis for comparison. It is difficult to see how such a history generating device could be constructed, and the use of classical statistics in these situations rests on no clear foundation \cite{gill2002bayesian}.

% The final problem with the classical approaches is the approach taken to each new experiment. In classical statistics, the results of an experiment are strictly dependent on the data which are observed. While there is nothing wrong with this, it neglects the important fact that a researcher will often have prior information on the phenomenon under study - otherwise, why is the study being carried out?\cite{gill2002bayesian, gelman2004bayesian} The classical approach ignores such information, even when doing so could increase its explanatory power and compensate for many of its deficiencies, as shall be seen in the next paragraph below.

% \subsection{A different approach: Bayesian Statistics}

% Bayesian statistics takes it name from that of an English preacher, Reverend Thomas Bayes. In an article published posthumously, he suggested that judging the fairness of a dice (modelled with a binomial distribution) and utilising uniform prior in the first instance, would prove a more useful alternative to classical procedures. However, Bayes himself did no further work in this area, and the foundations for what is known as Bayesian statistics today were laid by Laplace, apparently independently of Bayes \cite{stigler1986history}.

% The first major difference between Bayesian and classical approaches is their definition of probability. While classical statistics defines probability as the long run frequency of an event, the Bayesian approach regards probability a subjective degree of belief in a particular outcome. The second major characteristic of the Bayesian approach is the belief that prior information should be incorporated into the results of the analysis. This can mean that the researcher's beliefs about the likelihood should be incorporated into the data, or it can mean that the results of a series of experiments should update the probabilities of the hypotheses over time. In one sense, this approach is similar to the approach of meta-analysis, where a number of different studies are combined and the overall effect size is estimated. However, the approach taken to the accumulation of evidence is quite different.

% The method of updating probabilities is through the use of the Theorem of Inverse Probability, or Bayes Theorem. The theorem essentially states: the evidence for a hypothesis given the data, is proportional to the product of the probability of the data given the hypothesis (the likelihood) and the prior probability of the hypothesis.
% \begin{equation}
%   \label{eq:bayes1}
%   \Pr(H|D)=\frac{\Pr(D| H)\times\Pr(H)}{\Pr(H)}
% \end{equation}

% Equation \ref{eqbayes1} is  a means for updating the beliefs held about a scientific hypothesis based on new evidence. The subjective part of the theorem (the prior) has been criticised by many (including the famous statistician R.A. Fisher) as subjective, and capable of rendering scientific testing meaningless \cite{salsburg2002lady}. However, the argument from the Bayesian side is that most scientists do have prior beliefs about the data, and that it is surely better to have these made part of the evidence base openly, rather than living in the background of a published scientific paper\cite{gill2002bayesian}.

% Perhaps the most compelling reason that Bayesian statistics is not the standard method of analysis in the sciences is that the solution of complex statistical problems leads to the evaluation of high dimensional integrals \cite{gill2002bayesian}, which were extremely difficult to solve prior to the development of high speed, easily accessible computing power. As a result, scientists using the Bayesian approach were limited in the kinds of models which they could specify and more importantly, analyse.
 
% Beginning in the late 1980's (although the method was developed in statistical physics in the 1950's), the accessibility of Markov Chain Monte Carlo (MCMC) methods on computers changed all of this. Essentially, these methods allow scientists to sample from the posterior distribution an arbitrary number of times (often 10,000 or more) until the chains converge to the same values through a random walk process \cite{gelman2004bayesian}. This has freed scientists from the impossibility of evaluating complex multidimensional integrals by hand, and opened up Bayesian approaches to many more professional scientists.

% There are a number of advantages to the Bayesian approach, as well as a number of problems. The first major advantage is that it is philosophically and internally consistent, while the classical procedures are not, as they are an unwieldy synthesis of Fisher's, Newman's and Pearson's approaches towards data analysis \cite{gill2002bayesian, gigerenzer2004mindless}. The second major advantage is that the use of prior information can reduce the impact of unlikely results, given that the prior will correct for extreme values based on previous experience\cite{gelman2010philosophy}.

% The third major advantage, already noted above, is the inclusion of prior beliefs openly in the analysis, rather than in an implicit manner through the formulation of hypotheses and methods to test these hypotheses. Fourthly, this use of priors allows the scientist with a diametrically opposed viewpoint to estimate the impact that some new data should have on his probabilities. A fifth major advantage is that more than two models can be compared. In comparison to the null and alternative hypotheses of the classical approach, any number of models can be considered and their probabilities updated in line with the results of the experiment.

% In light of the above, it would seem that most scientists should be using Bayesian techniques more often than they do. However, there are also a number of problems with this approach, which are outlined below. The first (and for some, the largest) is the subjective nature of the prior distributions, which many decry as preventing the data from speaking from themselves \cite{gelman2010philosophy}. To some extent this is true, but this problem disappears as more data is collected and the prior ceases to have as much of an impact. A related problem concerns the justification of pror probabilities as subjective degrees of belief. It is impossible for a particular scientist to have all of his or her beliefs regarding possible outcomes of the study in the prior, yet this is a logical necessity if surprising results are ever to be found\cite{gelman2010philosophy}. This is easy to see as if the prior probability of an event is 0, then no amount of evidence can ever change this.

% Some Bayesians  recommend thinking of the prior as a regularisation device, and setting them as widely as possible within the confines of the discipline under study \cite{gelman2010philosophy}. Others argue that the priors should be estimated from the data itself (so called Empirical Bayes) \cite{carlin2009bayesian} while many regard this as a violation of the principle that the data should only be used once \cite{gill2002bayesian}. Another major issue in the use of Bayesian statistics is that they are not taught in most disciplines, and require more mathematical and computational sophistication than do frequentist methods, as well as the use of perhaps unfamiliar command line driven software tools.

% A final problem with the use of Bayesian methods (the one most favoured by Fisher) \cite{salsburg2002lady} is that scientists may have incompatible priors, and this may retard the development of understanding. However, this is a straw man argument, as scientists already do have widely divergent views about the interpretation of particular research (witness the controversy about what implicit measures actually measure and around the size and interpretation of the placebo effect) , and it would surely be better to allow for these divergent views and incorporate them into analysis, rather than let them fester impotently in the annals of journals.

% \subsection{Likelihood Approaches to Statistics}

% These approaches are perhaps the most core to the practice of statistics whether Bayesian or frequentist. Conventionally attributed to Fisher \cite{salsburg2002lady}, who developed the method of maximum likelihood, these approaches focus on which hypothesis is most likely given the data. These methods are already in widespread use in many factor analytic and psychometric studies, where models are compared based on the likelihood of the model given the data. Indeed, the AIC and BIC are prototypical examples of likelihood based measures, and can be calculated for almost any statistical model, which allows them to serve as convenient metrics in many situations.

% These methods are not concerned with p-values, focusing more on the relative likelihood of different models on the same data set. Such an approach would seem to be ideal for many scientists, as the strong philosophical assumptions of Bayesian methods are absent, yet the problems with the use of p-values as a strict decision criterion are avoided. However, these approaches have difficulty with establishing the absolute superiority of a particular model, while excelling in assessing the relative superiority given a particular data set. In addition there is an attractive information-theoretic approach to statistics \cite{mackay2003information}, but that will not be covered here as this approach is utilised mostly in the field of signal processing and machine learning.  In this research, a mostly likelihood based approach will be taken, but given that much of the research involved two or more samples who were administered the same instruments, some Bayesian approaches will be evaluated alongside the likelihood and frequentist methods to assess their explanatory power and accuracy in psychological research.


\section{Preliminary Research}

 Sampling from a population can be difficult, especially as the requirement of randomness needs to be satisfied. However, even if the surveys and measures are sent to a random selection of participants, who responds will almost certainly not be random, as people may only respond to surveys which are salient to them, and ignore the others. This is especially true in a University environment where many surveys are sent out to either random samples or the entire student population regularly. Some of the issues and concerns around sampling for each of the different pieces of research are discussed below.



\subsection{Treatment Credibility Questionnaire}

\subsubsection{Measures}

\paragraph{Development of Treatment Credibility Questionnaire}

The Treatment Credibility Questionnaire was a measure developed for this research project, to compensate for the perceived lack of detail in placebo response expectancy measurement. The questionnaire was based on the Credibility/Expectancy Questionnaire developed by Devilly and Borkovec which consisted of six questions, three which tapped credibility and three which tapped expectancies \cite{Devilly2000}. The scale was developed to assess expectancies around treatment and each question was scored on a 10 point scale (0-9). A number of changes were made to this instrument for use in this research. Firstly, the scale was changed to a 1-5 scale, to simplify the scoring. Secondly, the six questions in each condition were prefaced by a statement that read: You have been suffering pain for a number of days. You go to the doctor, and he/she suggests you try X (where X is one of the treatments listed above).

The questions were as follows:


\begin{enumerate}
	\item How logical does the therapy offered to you seem?
	\item How successful do you think this treatment will be in reducing your symptoms?
 	\item How confident would you be in recommending this treatment to a friend?
	\item How much improvement in your symptoms do you think will occur?
	\item How much do you really \textit{feel} that therapy will help you to reduce your symptoms?
	\item How much improvement in your symptoms do you really \textit{feel} will occur?
\end{enumerate}

\paragraph{Beliefs About Medicine Questionnaire}

In addition to this, the Beliefs about Medicine Questionnaire (BAM) was administered to all participants. This is an instrument developed to assess the beliefs of chronic pain patients regarding their medicines\cite{Horne1999}. This instrument was included in order to validate the TCQ by means of correlations with this similar measure. The BAM is an eight item measure which purports to have two factors. The eight items were those retained from a pool of 16 items which derived from interviews with chronic pain patients. All 16 items were administered to all participants, of which 8 are the items selected for the final questionnaire by the original authors, and the other 8 are items which were dropped due to poor loadings.

\subsubsection{Sampling for Treatment Credibility Questionnaire and Beliefs about Medicine Questionnaire}

The sampling for the Treatment Credibility Questionnaire was conducted in two rounds, as the instrument needed to be validated in one sample, and then confirmed and revised in another. The first sample was sent to a random subset of students at the University in February 2010, to which 299 students responded. This data was analysed over the next two months, and following this, a revised version of the questionnaire was sent to both staff and students at the University. Along with this (full details in the Appendix) the Beliefs About Medicine Questionnaire was also sent, to assess the construct validity of the TCQ.

\subsubsection{Analysis for Treatment Credibility Questionnaire}

Sample one was treated as one training set, and the results tested on a subset of Sample 2. 

All data analysis was preceded by fitting a multiple imputation model where necessary, and was split into 4 parts as described above under the section Analysis for Health, Optimism and Mindfulness.

Firstly, the data was checked for errors in entry or recording using summary functions and plots. Following this, the question responses were recoded according to the instructions for use. Following this, the summary scores were calculated. Next, summary statistics and characteristics of the data were reported. Next, the data was tested for normality using a Shapiro-Wilks test. Following this, a correlation matrix for the data was calculated and analysed.

Next, simple reliability analyses were carried out on the scales themselves. Following this, parallel analysis, the MAP criterion and the scree plot were used to estimate the number of factors which could be extracted from the data. After this, factor solutions were extracted using maximum likelihood, principal axis and minimum residuals methods, and these structures were examined to assess the invariance of the loadings using different methods of estimation. Primarily, direct oblimin methods of rotations were utilised, but promax rotations were also applied to ensure that the proposed structure was not overly sensitive to the methods of rotation used.

After the various factor structures were obtained, they were plotted and analysed for interpretability. Communalities and uniquenesses were assessed to ensure that there was no over or under factoring in the solutions. Communalities were then graphed against the number of factors extracted and the methods of extraction to provide a simple graphical guide to the usefulness of each solution.

Following this procedure of extraction and interpretation, Structural Equation Modelling was applied to each of the proposed factor solutions using the OpenMx package for R. The optimal factor solution was chosen using the AIC of each fitted model, along with the RMSEA of the proposed solutions.


Following the investigation of structure with the methods of classical test theory, the scales were analysed using Rasch models and item response theory. Firstly, mokken analyses were run, in order to check the assumptions of monotonicity, local independence and to assess how many sub-scales the analysis should be carried out on. Following this, a Rasch model was fitted to the data, and person and item parameters estimated from the data. Item and person fit statistics were also calculated, and as this model did not fit any of the three scales, a two parameter model was fit to the data. Again, item and person estimates were obtained and the relevant fit statistics calculated. The person estimates were then used as predictors in a logistic regression analysis, as a way of assessing the fit of the model.


After this, linear regressions were run to examine the differential effects of each of the correlated variables. Stepwise selection on the training set was carried out, along with lasso, ridge and least angle regression methods. The performance of each of these methods was then assessed on held-out data. In the case of Sample one, some of sample 2 was used as a heldout data set. For Sample two, the entire dataset was split into three splits, and the cross validation procedure carried out for each. The splits were kept quite large (approximately 300 non missing observations) to allow for psychometric models to be fit to each split seperately, and to be able to compare the performance of simple mean/sum scores against the factor scores and ability estimates derived from the psychometric modelling procedures.

% After this, linear regressions were run to examine the differential effects of each of the correlated variables. A two step procedure was used. In the first, a simple model was run using one predictor and one response variable. At the next step, a new variable was added to the model, and its impact on the fit and explanatory power assessed. This step was repeated with all variables thought to be of importance to the response variable, until all useful predictor variables had been added and either retained or removed from the model. In the second stage, all variables were entered into the model and its fit assessed. Then, the least significant variables were removed from the model one by one, until a minimal parsimonious model was obtained. At each step of these regressions, qq-plots and plots of the residuals were examined to ensure that the assumptions of homoscedasity and normality of errors were met. As the residuals for the TCQ did not fit the assumptions of linear regression, a generalised linear model was fit to the data, using a poisson error structure, and allowing for over-dispersion.

The Beliefs About Medicine Questionnaire was a special case in this analysis. As only eight items are included in the canonical version of the scale, the other eight were given to test the hypothesis that different items might load better in a general population sample, rather than a clinical sample. Given the small sample sizes in the original research, and the inappropriate use of principal components analysis, this was also used as an opportunity to examine the structure using better methods. Therefore, the eight items of the published scale were analysed first, and then all analyses reported in this section were carried out on the larger sixteen item set. Results of these analyses are in the appropriate section, and the implications are discussed both in the discussion in that section and in the General Discussion.


\subsection{Placebo Analgesia Experiment}

\subsection{Recruitment for the Experimental Procedure}

Following piloting, a random subset of students were emailed to ask if they would like to participate in the experiment. Given that the experiment took place in the Applied Psychology department, somewhat off campus and that the experiment involved suffering painful stimuli, an inducement of a smartphone was offered to one participant who completed the procedure on the basis of a draw carried out after the experimental procedure was carried out for all participants.  After this email was sent and participants recruited, another email was sent to Zoology, Ecology and Plant Science students, along with Psychology students, as all of these students had lectures in the Psychology building. In addition to this, an email was sent to all of the researchers acquaintances on popular social networking sites. Three more emails were sent to random samples from the all students mailing list, and the experiment ran from January 17th until April 14th inclusive.

\subsection{Measures used in the Experiment}
\label{sec:meas-used-exper}

The following measures were used in this experiment. Firstly, Age, Gender and course of study were collected for each participant. The MAAS and LOTR were also administered to each participant, as was a shortened version of the treatment credibility questionnaire (described in Chapter \ref{cha:preliminary-research}, Section \ref{cha:preliminary-research-2}). Following the completion of the explicit measures, participants completed a Treatment Credibility IAT and an Optimism IAT. The stimuli used in each of the IAT's were as follows:
\textup{Treatment Credibility IAT:}
Conventional: creams, pills, surgery, injections
Alternative: homeopathy, acupuncture, reiki, flower essence
Real: Real, accurate, true
Fake: Fake, inaccurate, false
Optimism IAT
Optimism: Optimism, happy, improving, succeeding
Pessimism: Pessimism, unhappy, disimproving, failing
Self: Me, mine, myself
Other: You, yours, yourselves. 

Further details of the development and piloting of each IAT are in Chapter \ref{cha:preliminary-research-5}. 
In addition, participants gave verbal reports of their pain levels to the experimenter at one minute intervals, and these were recorded (along with condition and exact time of application of bandage and when the squeezing stopped) on a sheet of paper, along with the participants identification number. 


\subsection{Experimental Procedure}

All participants were met at the entrance to the building by the primary researcher. They were given the informed consent documentation, and after they signed it, they completed three questionnaires (the MAAS, the LOT-R and the TCQ). Following this, they completed both an Optimism IAT and the Treatment Credibility IAT, where order of administration was counterbalanced across participants.

Following this, the participants sat down next to the Biopac physiological monitoring data, and baseline data was recorded for five minutes.  Then, a blood pressure gauge was wrapped around the upper part of the non-dominant hand of the participant, and they were asked to squeeze a hand exerciser twenty times for two seconds each time. One minute after this, and every minute thereafter, participants were asked to rate their pain on a VAS from 1 to 10.

If the participant was in the treatment or placebo group, then when they rated their pain as 7 or higher, the placebo cream was applied. The experiment continued until the participant either decided to withdraw, their pain rating reached 10 or 45 minutes elapsed  from when the bandage was applied. ECG and EDA recordings were taken one thousand times per second second using the Biopac equipment and VAS ratings were recorded on paper by the experimenter. The placebo cream consisted of moisturiser in a pharmacuetical container, and was unlabelled.

Participants in the treatment group were told that \textit{the cream was a potent painkiller, recently approved and proven to reduce pain, which would take effect almost immediately}. Participants in the placebo group were told that \textit{they were receiving a placebo and that placebos have been clinically proven to reduce pain, and that it would take effect almost immediately}.

\subsubsection{Analysis of Experimental Data}

The first step in the analysis of experimental data was to examine the comparability of each of the different groups. This procedure was carried out with t-tests (for numerical data) and ANOVA's (for categorical data). There were no differences in pre-treatment levels for any of the variables, and so analytical techniques did not need to include any variables to ensure comparability. This finding demonstrated that the randomisation was successful. 

\paragraph{Analysis of IAT data}

Firstly,  an ex-Gaussian distribution was fit to the data. This distribution is a mixture distribution of a normal and an exponential, and can be used to account for the extremely long tails present in such data. If this distribution proved a reasonable fit to the data, then estimation of confidence intervals and predictions could be made more easily from the data.
In addition, as described previously, quantiles were determined for each participant in each Block, and these individual level quantiles were used to derive the group level distributions for each block and condition. 
Another approach was taken to the data in tandem with this. This approach involves ordinal test theory \cite{schulman1975test}. All of the reaction time data occurs on a common scale. The differences between the responses to a word ,$w$ in condition 1 and condition 2 are on the same scale, and a distance matrix can be constructed from this data. This distance matrix can either be ranked (which makes fewer assumptions and is less affected by outliers) or the Euclidean distance can be used in analysis. The advantage of the distance approach is that it discards less information, though at the cost of making more assumptions. A clustering approach was then used, to assess whether or not the stimuli fell into the same categories. This clustering approach was repeated 10 times with different seeds and the results averaged in order to reduce random variability inherent to the k-means approach. $k$ was also varied from 1 to 10, and the results averaged also.

The ordinal/euclidean matrix was then factor analysed and examined using item response theory to assess its psychometric structure, and assess fit or lack thereof.

Another approach which was applied to this data was to fit Samejina's continuous response model, which is a generalisation of the Graded Response Model % (described above in Section \ref{sec:polyt-item-resp})
to the reaction time data. This procedure aimed to produce ability estimates which were then applied to the prediction of placebo response data. 

\paragraph{Analysis of Pain Rating Data}
\label{sec:analysis-pain-rating}

The pain ratings were modelled as time series, and individual and gender, condition and group level ARIMA models were fitted to the data. In addition, their cross-correlations were examined with the physiological time series collected as part of the placebo analgesia experiment. The model fitting procedure was as follows:
\begin{itemize}
\item Examine the time series to assess stationarity

\item Examine the ACF plots to assess lag needed

\item Plot the lagged and differenced series


\item Examine the ACF and residuals of the newly lagged and differenced series

\item If residuals were white noise and the series was stationary, accept the model. If not, repeat the process until the residuals are white noise and the series was  stationary
\end{itemize}

\paragraph{Analysis of Placebo Response Data}
\label{sec:analys-plac-resp}

The first step was to classify participants as either placebo responders or non-responders. This was done simply by examining if their pain levels decreased following administration of placebo. If this happened, they were classified as placebo responders, and if not, they were classified as non-responders. Another approach taken was to examine the number of minutes spent with a pain rating of less than seven following administration of the placebo, and to model this as a Poisson variable using a generalised linear model. 

Because of the autocorrelation inherent in the pain ratings, general linear models were not entirely appropriate for this data. Therefore, the autocorrelation structure was determined for the pain ratings, and a generalised linear mixed model was fitted to the data using the IAT, explicit and physiological data as predictors. These models were carried out using stepwise, lasso, ridge and least angle regression methods, and validated using ten-fold cross validation. In addition, these models were compared against models using both factor scores and IRT ability estimates to determine the usefulness of this model based approach. 

\paragraph{Analysis of Physiological Data}
\label{sec:analys-phys-data}

The physiological data was collected (as described above) from  two different sources. The first was skin conductance, and the second was electro-cardiogram data. Both of these sources provided recordings every 1000th of one second for the entire procedure (and normally a few seconds after the blood pressure gauge). The ECG data was analysed to extract heart rate, heart variability  and QT and RR intervals. In addition, both of these time series were examined using event history analysis (see Section \ref{sec:event-hist-analys}), which modelled the time series in three intervals (before administration of gauge, before administration of placebo, after administration till end) for the Deceptive and Open Placebo Groups and two series (before administration of gauge and afterwards) for the No Treatment group.

Finally, the relationships between physiological responses and the psychological (both implicit and explicit) variables collected, were examined. More details on the hypotheses regarding this can be found in the appropriate Chapter (\ref{cha:primary-research-2}).


% \section{Methodology for Literature Review}

% The literature review was the first task undertaken as part of this research project, and was carried out according to the following instructions. First, a list of databases was drawn up for each area that needed to be searched. Secondly, a list of terms which would be searched was generated by the primary researcher in conjunction with his supervisors. Thirdly, the search terms were applied to each database in turn. The first author performed an title check on all of the results, followed by an abstract check on each of the selected papers. The papers which were germane to the subject of the thesis were then sorted into thematic areas, and ranked in order of priority. Email alerts were also set up for the search terms used, in order to ensure that all relevant material was included.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "ThesisContents030511"
%%% End:

