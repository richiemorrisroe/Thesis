\part{Measurement in Psychology}
\label{part:meas-psych}

\section{Introduction}
\label{sec:introduction}

One of the features which both the placebo effect and implicit measures have is common is that they are both fields of study where measurement is controversial. In the case of the placebo, there are multiple definitions and situations where it is attempted to be measured and some controversy as to its existence, and in the case of implicit measures (more specifically, the implicit association test) there is a measure, but no clear model for how the measure achieves its predictive validity. 

The major aim of this thesis is to examine the placebo effect using a variety of different kinds of predictors, and to use these multiple kinds of measures to test a number of models for how the phenomenon is mediated. In order to have another form of measurement, it was decided to use self report measures which have been associated both with the placebo effect (optimism, expectancies) and the IAT (mindfulness). 

The reason for the use of these measures was threefold:
\begin{enumerate}
\item Firstly, it was believed to be important to have both an explicit and implicit measure both of treatment credibility and optimism as this would allow for a comparison of the relatively efficacy of both of these forms of measurement in the prediction of placebo
\item Secondly, the use of self report measures (where there exists a large amount of  research on methods for establishing the validity of a measure) would provide a consistency check on the development of the implicit measures
\item Finally, the use of self report measures allowed for large samples to be collected on some of the predictors of interest in order to develop better models for the measures which could then be applied to the experimental sample. 
\end{enumerate}

The remainder of this section shall be structured as follows:
\begin{enumerate}
\item Firstly, measurement methods in psychology shall be briefly reviewed, along with the general measurement methods proposed for use in this thesis 
\item Secondly, the measurement of the placebo shall be reviewed, with a focus on statistical problems with current methods and on expectancy measures.
\item Next, the rationale for attempting to measure the placebo with implicit measures will be described. 
\item Finally, an approach to modelling the placebo response and the relative contributions of different measures shall be described. 
\end{enumerate}


\section{Review of Measurement Methods in Psychology}
\label{sec:revi-meas-meth}

\subsection{Introduction}
\label{sec:introduction}

Stevens defined measurement as the assignment of numbers to people or objects according to a rule (Stevens, cited in~\cite{michell2000normal}) Despite this simple definition, measurement in psychology can often be controversial.  In the study of people there are many extraneous factors which could effect the outcome of a particular experiment, and controlling all of these explicitly is a tremendous challenge. This has lead to the use of psychometric modelling and statistical techniques to attempt to counter some of these problems. Psychology is also lucky in that it is possible to carry out experimental research which can tease out and control for many factors which may not have been considered by the researchers (through the use of randomised assignment). 

Even so, inference of psychological constructs from measured variables is an error-prone process~\cite{Blanton2006d}, in which it is difficult to ensure that what is being measured is actually related to the outcomes of interest. This has lead to a large focus on latent variable modelling (c.f. Section~\ref{sec:latent-variables}). Latent variables are extensively defined below, but in essence, the measures collected by self report or implicit methods are considered to be a combination of the effect of the latent variable and error. This approach, when done well allows for a better understanding of the factors which effect psychological phenomena. 

\subsection{Are psychological attributes numerical?}
\label{sec:are-psych-attr}



However, this approach of assignment of numbers to traits of people according to rules is an approach not without its critics. For example, Michel~\cite{michell2000normal} has argued that psychometrics is pathological in that it is has never explicitly tested the hypothesis that psychological characteristics can be modelled by numerical methods. In fact Michel claims that these qualities are explicitly non-numerical. However, even if these characteristics are not themselves numbers, there is little reason why a model involving numbers cannot be used with great fruition. For example, the heights of men can be modelled by a normal distribution~\cite{gelman2007data}, but this does not mean that the normal distribution generated the heights of these individuals. 

Borsboom~\cite{borsboom2004psychometrics} replied to Michel, and noted that many of the problems Michel had pointed out were not problems with psychometrics in general, but rather with classical test theory, and pointed out that the proposal of Michel (using the additive conjoint measurement theory of Tukey and Luce) has actually been implemented in the methods of item response theory. 

\subsection{Latent Variables}
\label{sec:latent-variables}

Latent variables are a primary focus within psychometrics and psychology more generally~\cite{bollen2002latent,borsboom2006attack}. 
Latent variables have a number of both formal and informal definitions in the field~\cite{bollen2002latent} and the one most useful for this research is the local independence definition. This defines latent variables as \textit{the cause of the correlations between observed variables}, and asserts that, conditional on the latent variable, the correlations between observed variables not significantly different from zero (i.e. the observed variables are locally independent). 

This definition has the advantage of being equally applicable to both factor analytic and item response theory approaches, whereas other definitions (such as the expected value definition, where the latent variable is referred to as the true score of classical test theory~\cite{bollen2002latent}) do not apply as easily to the methodologies employed in this thesis. 

The essence of the latent variable concept is that psychological tools such as self report measures or implicit measures are impure measures~\cite{edwards2000nature}. They are not veridical measures of whatever construct is under investigation, they also tap into elements such as context, social desirability, response patterns and a myriad of other biases and heuristics~\cite{borsboom2006attack}. The latent variable approach is extremely common because of this, and forms the core of factor analysis, item response theory and structural equation modelling techniques~\cite{bollen2002latent}. 

Some psychometricians would argue that factor analysis is a data reduction technique rather than a latent variable technique~\cite{borsboom2006attack}. This critique has its merits, but is more applicable to principal components analysis (PCA) where all of the variance in a matrix is divided into components. Factor Analysis only examines the common variance between items, and as such is a latent variable approach.

In classical test theory (factor analysis, reliability analysis), a latent variable is often referred to as a ``true score'', that is, the score that would be obtained for a participant given an infinite number of replications of the study~\cite{bollen2002latent,edwards2000nature}. Indeed, the error terms in a multiple regression model can also be regarded as latent variables~\cite{bollen2002latent} in that they are variables which are conditional on a model which has been applied to a set of data. 

 The latent variable modelling approach necessitates that a number of impure measures of a construct are collected (for example, items on a self report measure or stimuli from an IAT) \cite{edwards2000nature}. Impure in this sense means that there is no one to one mapping between observable outcomes of interest and the responses to a particular measure. This means that there is at least some residual variance left unexplained between the criterion (our measure, for example, of extraversion) and the outcome (extraverted behaviour).  By examining what these items have in common (by either their correlations or a non-linear function of the response patterns) a better estimation of values on the  construct can be derived \cite{borsboom2006attack}. This derived measure is then used as a predictor for the outcome variable. 

There are two main perspectives on latent variables. The first is the reflective model of latent variables, where the measures are believed to reflect the underlying construct. The second model is the formative model which suggests that latent variables are formed of the measures observed \cite{bollen2002latent,edwards2000nature}.

The first model reflects a positivistic concept of latent variables (i.e. that they exist within people and psychological measures elicit them) while the second represents a more constructivist approach (latent variables are constructed from our measures, and do not necessarily correspond to anything that exists within individuals) \cite{borsboom2005measuring}.

This research assumes the formative model of latent variables, as this is more useful for practical modelling of psychological constructs. 

\subsubsection{Constructs}
\label{sec:constructs}

The major focus of measurement in psychology is aimed at uncovering constructs, which are typically regarded as the underlying dimension which is reflected in observable outcomes of interest. These constructs are, by definition, unobservable, and are usually modelled with a latent variable framework. It is important to remember that the indicator of a construct is not itself a contruct \cite{borsboom2006attack}, but rather that the construct is the idealised ``cause'' of the construct. 

All psychological theories consist of two parts which are the relationships between constructs (unobservable) and relations between constructs and measures (observed, and from which inferences regarding the relationships between constructs can be drawn) \cite{edwards2000nature}. The problem here is that multiple relationships between constructs can be consistent with the observed data, and this is why some argue that only when relationships between constructs, measures and observable outcomes can be developed are metrics useful. 

In thinking about constructs, there are two main schools of thought. Then first, called the formative model specifies that constructs are the cause of measures, while the second, reflective model argues that constructs are a useful idealisation of measures \cite{edwards2000nature}. In this thesis, the second approach is taken towards constructs.  To quote Edwards \textit{et al}, constructs 
\begin{quotation}
  ... are elements of scientific discourse that serve as verbal surrogates for phenomena of interest
\end{quotation}

In essence, constructs provide us with a way to conceptualise the relationships between observed behaviour and measures of particular forms. To the extent that they achieve this aim, they are useful, and no further. 

Even within the reflective and formative models, there are multiple ways in which constructs and measures can be related. In the direct models (reflective and formative) the contructs are related to the measures directly, while in the indirect models, there is a presumed mediating variable which intervenes between the construct and the measure. To use an example discussed earlier in this chapter, Kirsch's model of how response expectancies are related to the placebo response is a direct model, and in this model, the effects of all other variables would be regarded as indirect in that they are mediated by expectancies. 

\subsubsection{Arbitrary Metrics}
\label{sec:arbitrary-metrics}


Blanton \textit{et al} argue that what is required within psychology are metrics which they define as \textit{a number that measures take on when describing individuals on the construct of interest. } \cite{Blanton2006d}. They further argue that the IAT (more specifically the Race IAT) is an arbitrary metric in that there is no well understood way of linking it to the population, nor of understanding how a one unit change in the metric affects the observable outcome of interest. This is an extremely good point, often forgotten in the development of psychological models and metrics \cite{borsboom2006attack}. Without re-examining the specific claims of Blanton and Jaccard (as was done in the previous part of the literature review), the general point is worthy of examination. 

In essence, the argument is that if we cannot relate a construct to an observable outcome, then it is arbitrary and should not be used as a metric. This, while true, is a point not specific to implicit measures but rather a more general point regarding most of measurement within psychology. Even those  Big-5 traits have been linked to outcomes but not conclusively (correlations tend to average around $r=0.3$ which is certainly not conclusive), and we cannot predict what one extra unit of extraversion on the NEO-FFI is likely to translate into in terms of outcomes, and yet these measures are still given credence by the community of researchers. This does appear to be a perhaps unmaintainable double standard.  



\subsection{Explicit Measures}
\label{sec:explicit-measures}
The use of self report measures of personality and attitude has been standard practice in psychology for over one hundred years \cite{spearman1904general}. In this time, well developed methodologies have been developed for the design and analysis of these measures.

The primary concerns for these measures were their validity and reliability. Validity is typically taken to mean the extent to which a measure actually does measure what it purports to, while reliability is the extent to which the same measure applied to the same individuals will give consistent results \cite{raykov2010introduction}.  The validity of a measure can be assessed by correlational analyses with other measures which are theoretically related to the measure under study (convergent validity), but the ultimate test of the validity of the measures comes from its association with an indpendently measured outcome of interest. 

The reliability of the self report measures is assessed by the use of reliability indices such as Cronbach's $\alpha$ \cite{cronbach1951coefficient}. Typically,  factor analysis, structural equation modelling and item response theory methods are applied to the data to investigate latent structure underlying the observed responses. Each of these will be dealt with in turn.  



\subsubsection{Factor Analysis}
\label{sec:factor-analysis}
Factor analysis has a long history in psychology, and is now over one hundred years old. It is the most commonly used latent variable modelling technique in psychology, and more pages of \textit{Psychometrika} have been devoted to it than to any other technique \cite{henson2006use}.  Despite this, there are still a number of issues and controversies which surround the technique \cite{sass2010comparative}.  Essentially, factor analysis is an attempt to approximate a correlation matrix with a smaller matrix of parameters.  These hypothesised latent variables tend to be called factors or components.

 % One of the first controversies surrounding factor analysis is the dispute between Factor Analysis proper and Principal Components Analysis\cite{henson2006use}. The major difference between Factor Analysis and Principal Components Analysis is that in Factor Analysis, only the variance common across observed response to items (or communalities) is analysed, while in PCA, all of the variance (including variance only found in one item, or unique variance) is analysed. PCA tends to work better for data reduction, and indeed this is the reason why it was developed \cite{borsboom2006attack}.

% Throughout this research only factor analytic methods were used for psychometric purposes, as Factor Analysis provides a true latent variable modelling approach, while Principal Components Analysis  (PCA) does not. 

The most critical issue surrounding factor analysis concerns determination of the number of factors to extract\cite{zwick1986comparison}.  This is an important issue, as theory and practice are likely to be held back if an incorrect choice is made.  The issue is not that there are no criteria on which to base a principled decision, but rather that the different criteria often do not agree, and it is thus ultimately left to the informed opinion of the researcher which factor solution is to be preferred.  All of the decision criteria will be reviewed in turn, and their advantages and disadvantages will be discussed \cite{henson2006use}. 

\begin{quotation}
  "Solving the number of factors problem is
     easy, I do it everyday before breakfast.  But knowing the right
     solution is harder" (Kaiser, 1954).
\end{quotation}

The choice of criterion for retention of factors is extremely important for this thesis. This is because if an incorrect number of factors are extracted, then the predictions for the experimental portion of the research will be biased, and thus will not prove as useful as the method could otherwise be. 

The first, and most popular, criterion is surprisingly the least useful \cite{zwick1986comparison}. This rule is called eigenvalues greater than one criterion and recommends keeping all factors whose eigenvalues are greater than one. The rationale behind this approach is that eigenvalues less than one explain less of the variance in the matrix than one item, and as such should not be retained.  More recent research appears to put the minimal criterion for retention of eigenvalues at approximately 0.7 \cite{henson2006use}. 

The second criterion often used is the scree plot technique, which was popularised by Raymond Cattell . This criterion recommends that the eigenvalues of all factors should be plotted against their number, and only factors before the drop off in eigenvalues should be used. As the process of factor analysis ensures that the first factor will have the largest eigenvalue, followed by the second and so forth, this criterion looks for the point where the eigenvalues are very close to one another.  This criterion has a number of advantages.  It is available in all statistical packages, it can be used without any special training and it tends to give results which are somewhat, if not totally accurate \cite{zwick1986comparison}.  Its major disadvantage is that it relies upon the interpretation of the researcher, but given the strong emphasis on interpretation throughout factor analytic literature this should not be regarded as too much of a handicap.

The next criterion which can be used is that of parallel analysis. Parallel analysis is a Monte Carlo (simulation)  technique which simulates a data matrix of equal size and shape to the matrix under study, and calculates the eigenvalues of these simulated matrix against those of the real matrix \cite{horn1965rationale}. All factors are retained up to the point where the simulated eigenvalues are greater than the true eigenvalues.  Parallel analysis is one of the better techniques for assessment of the number of factors to extract , and it can often give very accurate results \cite{zwick1986comparison}.

Its major disadvantage is that it tends not to be available in many statistical packages, and that it can often over factor the data-set. Additionally, many tools simulate the new data from a normal distribution, the requirements of which are not often met in practice in psychological instruments \cite{micceri1989unicorn}.  It does produce some of the most accurate results in simulation studies so it is a useful tool in practice \cite{zwick1986comparison}. 

Another useful criterion is that of the Minimum Average Partial Criterion (MAP) which extracts factors from the data-set until only random variance is extracted \cite{revelle1979very}. Again, this is an accurate criterion  \cite{zwick1986comparison} which is little used as it is not available in popular statistical programs. The only problem that  has been found with this criterion is that it tends to under-extract factors. %However, with the use of this criterion as a lower bound, and parallel analysis as an upper bound, then the decision of how many factors to retain can be made much easier. [I found this, but the literature does not appear to have as many examples of it] This leads on to the major point and issue with much factor analytic research today, whereby one decision rule is used to the exclusion of all others. Many researchers have recommended the use of multiple decision criteria, but this does not appear to be am approach utilised by many in the literature \cite{henson2006use}\cite{sass2010comparative}. However, this is the approach which has been taken in this research. This work will use parallel analysis, the MAP criterion and examination of scree plots to ensure that all relevant factor solutions are examined.

However, the ultimate test of a factor solution (without using other methodologies, such as Structural Equation Modelling) \cite{joreskog1978structural} is its theoretical clarity and interpretability, and this will be the first test used for all proposed factor solutions.

Another area of dispute amongst researchers in the factor analytic field is which method of rotation to use\cite{sass2010comparative}. As the eigenvalues are only defined up to an arbitrary constant, these rotations do not have any substantive impact on the factor matrix, except that they can make it easier to interpret (which is normally very useful). 

Rotations are commonly applied to factor solutions in order to reduce items loading on multiple factors, and to aid in the discovery of simple structure \cite{henson2006use}. Rotations can be divided into two classes, orthogonal and oblique \cite{sass2010comparative}. Orthogonal rotations return uncorrelated factors, while oblique rotations allow the factors to be correlated. Given that most psychological measures are correlated with one another, one would expect oblique rotations to be more common. However, the default appears to be orthogonal rotations, as they are apparently easier to interpret \cite{henson2006use}. Oblique rotations were applied throughout this research, as if the factors are truly uncorrelated, then the oblique rotation will show that, while the converse is not true for orthogonal rotations.

In conclusion, factor analysis is an extremely useful technique which has been widely applied in psychology. It is available in most software packages, is typically easy to interpret and can be carried out with a small number of items (more than 200 is often sufficient). The major problems with factor analysis are the necessity of determining how many factors to extract, which is a difficult and often subjective decision, and additionally if scores are required then many methods exist, again without any clear rationale for choosing one over the other.  





\subsubsection{Item Response Theory}
\label{sec:item-response-theory}
Item response theory (IRT) is often called model-based measurement\cite{fischer1995rasch}, (also referred to as Rasch modelling), is a newer approach to analysing self report data, developed both by the Danish mathematician Rasch in work for the Danish army, and also seperately by Lord and Novick in the US, while working for the Educational Testing Service (ETS) in the 1950's and 60's \cite{van1997handbook}.
The fundamental premise of IRT is that the properties and scores on a psychometric test can be modelled as functions of both the items on the test and the ability of the people taking the test.

The IRT approach suggests that conditional on both the ability of the person and the difficulty of the test, the responses of each participant can be predicted probabilistically. As the latent ability of the participant rises, they tend to choose alternative responses which are more reflective of this latent ability. One example of this might be an item for extraversion ``I am always the life and soul of the party``, those respondents who had a higher latent score on the extraversion construct would tend to choose the agree or strongly agree options (on a typical five point scale). 

For instance, if one was modelling extraversion using a set of ten items, the participants who scored highest in extraversion would be most likely to respond strongly agree to the items.  IRT also assumes a property called local independence, which states that conditional on the ability measured by the test, the scores of each participant are independent of one another.

There are a number of different approaches taken to IRT \cite{van1997handbook,fischer1995rasch}. The  Rasch models are the simplest, and have a number of extremely appealing mathematical properties. These models assume that only one trait is measured by the items, that all items are equally predictive of the trait, and that there is no guessing \cite{van1997handbook}.

Because of these assumptions, it is possible to seperate out person abilities and item difficulties perfectly. However, another approach (normally referred to as a two parameter model, or IRT proper) claims that items are differentially predictive of the ability being measured, in a manner analogous to different strength of loadings of items on a construct in factor analysis. Another model the three parameter model \cite{lord1968statistical}, allows for correct responses through a process of guessing, but this model is not normally applied to polytomous (many valued, like a Likert scale) items \cite{van1997handbook,Mair2010}.

The Rasch model is the simplest of these three general forms of models, and will be discussed first, followed by a discussion of two parameter models, after which I discuss three parameter models. Finally, this section ends with a discussion about non-parametric item response theory and multidimensional IRT.

In general, IRT models are represented by the logistic function, and are estimated iteratively through procedures of numerical optimisation (maximum likelihood \cite{fischer1995rasch}). The function used to describe the data is a logistic one, where ability is estimated from the probability of answering the question correctly \footnote{a probit model was used for many years as the logistic function was harder to estimate, and the difference between these two functions after scaling are minimum}.

The difficulty of an item is conventionally defined as the ability of participants who answer the question with 50\% accuracy. The parameter $\alpha$ is defined as the difficulty of the item. In two parameter models, another parameter $\beta$ is defined and is used for the discrimination of the item (the slope of the curve). In the more complex 3 parameter model, $\theta$ is used to measure guessing (the probability of a correct answer given low ability)\cite{van1997handbook}.

IRT was developed in the context of ability tests, and this leads to much of the vocabulary fitting uneasily within personality psychology. For instance, in the context of a credibility questionnaire about various treatments, the questions on homeopathy are categorised as most difficult (see Chapter \ref{cha:tcq-thesis}). This does not mean that they are harder to answer, just that the probability of a respondent endorsing them is lower than the probability of a participant endorsing a similiar item on the efficacy of painkilling pills.

Below, all of the models will be described in terms of their original use in ability testing, and the section concludes with a discussion of polytomous IRT, which is the methodology exclusively relied upon in this research as this research focused on personality  rather than ability testing.

It is important to note that the names of the models are slightly deceiving, while they are called 1, 2 \& 3 parameter models, they actually involve the estimation of 1,2, or 3 parameters for each item. This normally means that a test of ${1,2,3\ldots, n}$ items will require the estimation of either ${n, 2n, 3n}$ parameters. The parameters are estimated by an iterative maximum likelihood approach, as so issues of optimisation and ensuring that a global maximum has been found are often important in practical applications\cite{gill2002bayesian}.


\subsubsection{IRT Models}
\label{sec:irt-models}
Rasch models are the simplest of the different kinds of IRT models. The model makes a number of assumptions, here restated in less mathematical language than is typical\cite{fischer1995rasch}.

\begin{enumerate}
\item The latent variable is normally distributed
\item Each of the items provides equal information with regard to the latent trait
\item Given a set of responses to a question, $k$ and a latent ability, for every k there is a $\theta$ which is identical in rank (monotonicity)
\item Given the latent variable, all of the $n$ items are independent (local indpendence)
\end{enumerate}

The first and third assumptions are common to most IRT models, while the second assumption is the defining characteristic of the Rasch model, and the feature that allows for some of its appealing mathematical properties.

In essence, all of the information about a participant garnered from a Rasch model can be represented by the ability score (it is a sufficient statistic for the distribution) \cite{fischer1995rasch}. This is a characteristic which does not generalise to the more complex models discussed below.

Two parameter (IRT) models are defined by two parameters, the difficulty of the item and the discrimination of the item. The difficulty represents the probability of correctly answering the question, while the discrimination represents the change in difficulty for a given ability (the slope of the curve). In the Rasch model, these slopes are set to 1, so that all items provide equal information about the abilities of participants throughout the sample. This, while allowing for exact estimation of the difficulty of the test and strict sample invariance, is an assumption which is often not met for psychological testing instruments \cite{embretson2000item}.

Two parameter models maintain all of the assumptions of the Rasch model, except for 2, which is generalised \cite{van1997handbook}. The two parameter model recognises that different items provide different amounts of information regarding different participants. This alters the overall model by allowing the slopes of the item to differ, which disallows the property of conjoint additivity, which is the feature of Rasch models that ensures that participant ability and item difficulty can be seperated perfectly.

Non parametric IRT was developed by Mokken \cite{mokken1997nonparametric}, and is often called Mokken scale analysis\cite{van2007mokken}. It maintains the assumptions of all of the previous models, except for the parametric form of the latent variable. Mokken scaling is often performed as a prelude to parametric IRT, as it provides useful checks of all the assumptions noted above. In addition, Mokken scaling can be used in order to assess whether a scale should be broken into two or more subscales, which can then be modelled using parametric IRT. This is the manner in which mokken scale analysis was used in this research. 

% \subsubsection{Two Parameter Models}
\label{sec:irt-models}
% Two parameter models were developed by Lord \& Novick while they worked at Educational Testing Services (ETS)\cite{van1997handbook}. . However, in return this model allows for greater complexity and fidelity to the responses made by the participant.

% \subsubsection{Three Parameter Models}
\label{sec:irt-models}
% The three parameter model was developed by Birnbaum and is identical to the two parameter model, except for one crucial difference\cite{van1997handbook}. Both the one and two parameter models assume that participants only get the correct answer if they know it, that is, there is no guessing. This is a difficult assumption to meet in practice, as in an educational testing setting (assuming that there is no negative marking) participants are likely to guess if they do not know the answer.

%  Even if there is a negative mark penalty, as long as it is not equal to the mark gained from a correct answer, any rational participant will guess if they have narrowed the options down to two of four possible responses. Therefore, Birnbaum's model incorporates this into the test, and assumes that there will be a non-zero level of guessing throughout the test. This means that it cannot be assumed that a participant with extremely low ability will answer a difficult question incorrectly, as so the lower bound on the probability of answering is estimated from the data, and will be greater than 0 (where it is in both the one and two parameter models). Apart from that, the model is exactly the same as the two parameter model.

% \subsubsection{Non Parametric Item Response Theory}
\label{sec:irt-models}


% \subsubsection{Multi Dimensional Item Response Theory}
\label{sec:irt-models}
% So far, all of the IRT models we have considered have been univariate IRT models, where there is assumed to be a single latent variable $\theta$ responsible for the different patterns of responses. However, it is easy to see that often, this assumption will not be met.

% In terms of personality testing, there is a long history of stable inter-individual differences in the manner in which people respond to personality test items. Some participants will tend to utilise the extreme scores of the scale, while others will stick to the middle. These kinds of behaviours occur stably across divergent instruments, and need to be accounted for.

% As yet, unfortunately, there is no useful mathemtatical model \cite{borsboom2009end} which can be used to model these kinds of individual differences, which is essential if the measurement of human ability is to progress. However, what can be done is fit a multidimensional model which accounts for some of these factors in a coherent  fashion\cite{doran2007estimating}.

% Indeed, multi-level IRT models can allow us to examine the intercorrelations between latent variables, and can lead to much more stable estimates of ability and difficulty, though at the cost of a loss of parsimony and extensive computing requirements (though far less computation than is needed for even the simplest Markov Chain Monte Carlo model). Therefore, as many of the scales used in this research seem to measure multiple latent variables, ML IRT approaches will be employed after an acceptable  unidimensional model has been built for each of the sub scales.

\subsubsection{Polytomous Item Response Theory}
\label{sec:polyt-item-resp}
In contrast the dichotomous IRT models presented above, almost all of the IRT models used in psychology are IRT models for polytomous (many responses) data. This follows as with a typical Likert scale, there are five possible responses, arranged  in a montonically increasing fashion.

% Indeed, failures of monotonicity are common when fitting simpler models, and this needs to be checked before estimation of parameters. However, such failures tend to be obvious after the estimation of parameters, and the model can then be improved in an iterative process.

Common polytomous models used in psychology are the Graded Response Model \cite{van1997handbook} (two parameter) and the Partial Credit Model (one parameter)\cite{fischer1995rasch}. The generalised partial credit model is also used in research, and represents a generalisation of the PCM to more flexible two and three parameter IRT models. 

All of these models assume that the data is ordinal (as per a Likert scale). There is another model, the Rating Scale Model \cite{bock1997nominal}, which is used to fit nominal data, or when the assumptions of ordinal data have been breached. The model fitting procedure typically followed a path from simplest to most complex, as described below.

\subsubsection{Scoring Self Report Measures}
\label{sec:scoring-self-report}


Scoring of self report measures is an area which has received surprisingly little research within psychology. Almost all scales seem to use either a sum or mean as their method for calculating a score \cite{borsboom2006attack}. This use of sums and means makes a number of assumptions, some of which may not be justified in this research:

\begin{enumerate}
\item All scores contribute equally to the construct;
\item All questions tap the same construct;
\item All questions are scored in the same direction.
\end{enumerate}

Of these assumptions, many of them will be violated for many measures. The first, the notion that all questions contribute equally to the construct will be false in all cases where the factor loadings are significantly different from one another (as assessed by correlations between them at conventional levels of significance). The second assumption will be violated where a scale measures more than one factor. 

It would seem obvious that scales which measures multiple factors need to have a separate score for each one, but this does not appear to occur in many cases. As an example, the factor structure of the Life Orientation Test -Revised is a matter of controversy, with some studies have found a one factor structure while others have found a two factor structure. However, the official scoring guidelines produce only a single mean score for all items.  The third assumption is the only one which appears to be consistently followed in practice.

There are some reasons why means and sums are preferred over the more complex methods described below throughout psychology. There are two main alternatives to the use of means, medians or sum-scores (with many variations within each of these two broad types), but both of these methods require large samples, which (especially in clinical work) may not be readily available. 
The two methods are:


\begin{enumerate}
\item Factor scores;
\item Participant abilities and item difficulties, as estimated from an IRT model.
\end{enumerate}

These two methods have much in common, and also some important differences. The first commonality is that they both require large amounts of participant data (perhaps 300 or more) \cite{van1997handbook, henson2006use}. The second commonality is that they are both model-based, in that a model is fitted to the observed responses, and then this model is used to turn these responses into scores for each of the participants. 

Given the paucity of experimental research using over 300 participants in most areas of psychology, it is not surprising that means and summations have remained the standard within the field. Another commonality between the two methods is that they both weight particular items differently, based either on their correlations with the factors (for factor analysis) or the probability of a particular response (for IRT). For Rasch models, the assumption of equal discrimination for each item implies that sum scores are a perfect representation of the model, but Rasch models are difficult to fit to psychological instruments, especially those developed using classical test theory \cite{borsboom2006attack}.   

There are also some differences between the two methods, which are described below. Firstly, factor analysis is a technique which falls squarely under the rubric of linear models \cite{venables2002modern}, so there is an assumed linearity between the responses and the factor scores. In contrast to this, IRT utilises a non linear approach to scoring the items (generally a logistic function), although these can also be classified under the rubric of linear models, albeit generalised \cite{venables2002modern}.

One issue with the use of factor scores is that they come from an unidentified model, in the sense that there are an infinite number of factor scores consistent with the data \cite{grice2001computingit}, and the validity of these cannot be assessed through factor analysis alone. The resolution of this issue is discussed in Chapter \ref{cha:methodology} in Section \ref{sec:probl-sample-infer}. There are two major types of factor scores, coarse and fine, and both of them will be assessed for their predictive ability on cross-validation test data \cite{grice2001computingit}.

Another difference concerns the assumptions about the parameters estimated from the sample. In factor analysis, they are assummed to be sample dependent, in that the same instrument given to a different population could produce entirely different factors.  For this research the models were developed and applied to the same sample, so theoretically, this should not be an issue.  In item response theory, the item difficulties are assumed to be independent of the sample which is used to estimate them. This can be rigorously proven for the Rasch model, and is often extended to the more complex 2 and 3 parameter models also, though this is still controversial \cite{van1997handbook}.

The approach taken in this research  was the following. Firstly, all measures were administered to a random sample of the population from which experimental participants were to be drawn (students at University College Cork). These large datasets ($n$ between 800 and 1500) were then used to build and test models based on both factor analysis and item response theory.

N fold Cross-validation \cite{friedman2009elements} (see also Section \ref{problems-sample-infer}) approaches were employed to decide amongst the different methods of scoring. N-fold cross-validation involves splitting the data into N sets, fitting the models on N-1 of these sets and testing the model on the remaining split. This process is repeated for all splits. These models were then  applied to the response patterns of the experimental participants, and estimates of their abilities and scores were created, along with a determination of the best model(s), which was then applied to the experimental data. 

 This allows for the psychometric tools developed for analysis of large samples to be applied to experimental data where there would not have been enough respondents to fit a model on if this was the only data available. This is important, as many measures (including the ones described in Chapter \ref{cha:health-for-thesis} and \ref{cha:tcq-thesis}) have structure which implies that the use of means, medians or sum-scores would be invalid, and the collection of large amounts of data allowed for models to be applied to the scales which can then be used to predict the response to placebo more accurately in Chapter \ref{cha:primary-research}. 


The most important feature of a time series model is parsimony, as there are many ARIMA models which can fit a sequence equally well, and the best approach is to start from an extremely simple model and add extra parameters until an acceptable level of fit is reached.  In order for any kind of ARIMA model to be fit, $N>50$, which is often quite difficult in some applications. Given the nature of the time series data included in this research (physiological series measured once per second) this was not expected to cause any problems. It was however, a problem for some of the pain ratings, as these were only collected once per minute. Resolutions for this problem are discussed below. 

\paragraph{Assumptions of Time Series Analysis}

There are a number of conditions that must be met in order to carry out a time series analysis:
\begin{itemize}
\item Each observation is considered to be on an interval scale;
\item The time between each observation must be constant;
\item  The time series must be stationary in both level and variance;
\item  The data generating process must be identified;
\item  The residuals must be white noise (i.e. drawn from the standard normal).
\end{itemize}

\paragraph{General Definitions}

In the context of a time series $Y_t$, \textit{trend} refers to movement in a specific direction; i.e. any systematic change in the level of a time series. If a time series in trendless, and the subsequent values are generated as standard normal random shocks, then the best estimate of the time series at any point in the future will be the mean. However, very few time series are trendless, there is often something which moves the series in a particular direction. There are two options for dealing with trend: it must either be removed, or it must be modelled. Older texts on time series suggest regression as a means to remove trend, but this is not recommended as it will be heavily biased by outliers \cite{mccleary1980applied}.

% Mathematically, a time series can be regarded as a random walk, that is, the independent realisation of a long sequence of independent and identically distributed events (the random shocks) \cite{venables2002modern}.
 A better way of dealing with trend in a time series is through differencing, where each observation is subtracted from the last observation. This allows us to consider the time series as particular individual changes from one observation to the next, having accounted for (by differencing) all of the changes up to that point. A differenced time series can be referred to as $z_t=Yt-Yt-1$.

The general form of ARIMA models is as follows: ARIMA(p,d,q) where $d$ is the number of differences required to make the time series stationary. In most social science cases, the order of $d$ will typically be less than 2 \cite{mccleary1980applied}.

In the general form of the ARIMA(p,d,q) models, $d$ has already been discussed. In this context, $p$ refers to the number of lagged observations that must be retained in order to model the current observation $Y_t$. This is referred to as the Auto Regressive portion of the model, where $p$ is the autoregressive parameter. If the ARIMA model is (p,0,0) then it is already stationary, but other processes can be differenced to reach this point. Some authors note that first order AR processes are most common in social science applications \cite{mccleary1980applied}. The autoregressive parameter $\phi$ falls between -1 and 1 (if it does not, this represents a serious failure of the model) and thus, each lagged term converges towards zero (as the parameter is raised to the power of n lags).
% If $\phi$ is not constrained, then past shocks would become exponentially more important as time went on, which defies the common experience of processes moving over time.

Following the differencing procedure, the time series should be stationary in level. However, while this is necessary for an analysis of time series, it is not sufficient. In order for the time series to be analysed, it must be stationary both in level and in variance. One common way to acheive variance stationarity is by using a natural log transform. One major advantage in this approach is that coefficients garnered from this time series can merely be exponentiated in order to intepret them on the scale of the original data \cite{gelman2007data}.  


The final term is the general form of ARIMA models, $q$ represents the moving average. Essentially, it is a measure of how long a random shock stays in the system before its effects can be ignored. An ARIMA (0,0,2) model would imply that the current term is made up from the previous two terms, and no more are necessary. Again, the parameters need to be constrained to the range between -1 and +1 \cite{mccleary1980applied}.


\paragraph{Identification of ARIMA models}

The models discussed above represent the general form of ARIMA models. These need to be estimated from the data, and this section deals with  methods of doing this. The general approach here is to use what is called the Auto Correlation Function (ACF)\cite{mccleary1980applied}. Essentially, the ACF is a generalised correlation coefficient for dependent (i.e. time-series) data. The ACF is normally calculated for different lags, and the results plotted. These plots are then primary tools to identify the ARIMA model which best fits the data. For example, an ARIMA(0,0,0) process will have ACF's for all lags which are essentially zero. An ARIMA(0,1,0) will have equal ACF for all lags. The ARIMA(0,0,1) will have a non-zero ACF(1) and all others will be zero. More generally, an ARIMA(0,0,q) will have $ACF(1)\ldots, ACF(q)$ which are not significantly different from  zero.ARIMA(1,0,0) processes will have exponentially decaying ACF's. Examination of the ACF plots and their absolute value can also determine whether or not a time series needs to be differenced.

In addition, the Partial Auto-Correlation Function is also used in the identification of ARIMA models. The PACF is defined as the correlation between $t$ and $k$ after all lags inside that interval have been partialled out. Essentially, the PACF is equivalent conceptually to the differenced time series \cite{mccleary1980applied}.

An issue which often causes problems for time series analyses is that of seasonality. Seasonality is defined as recurring patterns that occur at a higher order level of the time series. For example, time series of retail sales tend to spike in December and January, and this is an example of seasonality. Generally, seasonal effects are modelled using a higher order ARIMA(p,d,q) structure so a typical model of retail sales data might look like this ARIMA(2,0,0)(0,0,2). However, seasonal patterns are much rarer in physiological data, and so they will not be further discussed here.


A final issue in the use of linear regression is the popularity of stepwise approaches to variable selection in regression models\cite{antonakis2010looking}. In these approaches, the computer is told which variables to start with, and an alogrithmic approach to the analysis is taken, whereby variables are added or removed from the model on the basis of their p value (or AIC). These models, which appear to remove all responsibility from the researcher, lead to biased estimates, far too narrow confidence intervals and incorrect F tests \cite{antonakis2010looking} \cite{gelman2007data}. Indeed, Antonakis \& Dietz found that using stepwise multiple regression they were able to fit a model explaining 80\% of the variance to 20 variables distrbuted normally, which is merely fitting a model to noise. However, this problem occurs even when variables are added or removed from a model manually, as the false positive rate increases linearly with the number of models fit. The $R^2$ tended to decrease as they increased their sample size in the MCMC study, but any appreciable model should not be constructable from pure noise \cite{antonakis2010looking}.

There are solutions to this dilemma. Perhaps the most justifiable solution is to only add or remove variables from the model based on theory. This is an excellent solution which if followed consistently would lead to much more parsimonous and replicable models. However, any approach which starts with a model and then adds or subtracts variables from it, whether driven by theory or an algorithmic approach, still suffers from the multiple comparisions problems and needs to have the p values adjusted \cite{friedman2009elements}. 

The major problem with this approach is that it removes the opportunity for serendipituous findings from research. While the use of theory driven regressions may cut down on the discovery of spurious relationships, it does so at the cost of removing all opportunity for unexpected findings.

One simple solution, which can be implemented in all statistical packages is through the use of cross-validation. With this approach, predictor variables are selected on N-1 splits of the data, and the model is then tested on the remaining split. This avoids the multiple comparison problems which stepwise variable selection methods incur, and allows for accurate p-values and standard errors to be estimated from the data, while allowing for serendipituous discovery of accurate models. This approach also harnesses the ability of the computer to fit an extremely large number of models and retain those predictors which perform the best. In this research, this form of stepwise selection was used, selecting predictor variables using a psuedo-AIC which penalises models with more parameters~\cite{venables2002modern}.  





Another solution lies in regression methods developed in the last thirty years which are designed to compensate for the deficiencies of stepwise approaches. These methods are known as lasso, ridge and least angle regression, and the premise behind them is simple. They allow variables to enter and exit algorithmically, but they penalise the coefficients towards zero each time this occurs\cite{friedman2009elements}. This allows for the effects of overfitting to be greatly reduced while at the same time enabling a variety of models to be fit to limited data. Combined with cross-validation of all models, these approaches hold real promise for reducing spurious findings and increasing the usefulness of models for prediction in this research.


\section{Approaches to Statistical Inference}

The dispute between Bayesian statistics and frequentist methods has run for over a century now, and the pendulum of use and acceptance has reversed a number of times in that period. In this section, the main differences and similarities shall be highlighted and an argument will be made for an approach which combines the best of both approaches.

The frequentist paradigm is the dominant approach currently in most experimental sciences \cite{gill2002bayesian,gelman2004bayesian} . The frequentist approach regards the probability of an event as the long run frequency of this event given repeated sampling from a particular population. The primary focus of frequentist statistics is the null hypothesis which represents the state of knowledge of the researcher at the beginning of a particular piece of research. The null hypothesis (hereafter $H_0$) is that any difference between the two samples in an experiment (typically treated and control) is due to chance variability in the measured quantities. The parameter values in the population are regarded as fixed but unknown, so from this perspective, it is meaningless to apply probability to them \cite{gill2002bayesian,gelman2004bayesian}.

The essential idea behind the approach of the null hypothesis is that the score on the test assessing the hypothesis of no difference must be sufficiently unlikely to have occurred by chance, given an assumed distributional form for the errors of the response variable (typically  the normal distribution).

This approach to statistical inference has much in common with the mathematical method of proof by contradiction. The measure used to assess this is called a p-value, and represents the probability that data as or more extreme than the test statistic would have occurred, given that the null hypothesis is true. A low p value (typically less than .05) suggests that the pattern of data between treatment and control is unlikely to have occurred by chance, and argues in favour of the rejection of the null hypothesis, in favour of an alternative hypothesis, which is usually specified by the researcher.This p-value of less than 0.05 was suggested by Fisher to be used when the researcher had little prior knowledge regarding the likely differences between groups, but it has become reified into much of the scientific community in the years since \cite{gigerenzer2004mindless}.

A number of issues arise from this definition and description. Firstly, given the definition of probability given above, it can be seen that the p value represents the fact that if the same experiment were to be run in exactly the same fashion using the same size sample from the same population then (assuming the results were significant at p<0.05) in 19 of 20 replications, the experimental data would again be significant.

One problem with the use of this binary threshold is that the p value estimates the likelihood of the null hypothesis given the data, which is rarely what the researcher is interested in. More typically, the researcher would like to know about both the existence and size of the relationship between the two (or more) quantities of interest\cite{cohen1988statistical}. The p value does not supply this information, and needs to be supplemented by what is called an effect size which measures the degree of association between two or more variables in standardized units. Perhaps the most easily understood of the effect size measures is the coefficient of determination, or $R^2$.
This measures the proportion of variance explained by a relationship between two variables, and is often computed by squaring the raw correlation coefficient. More formally, it is the ratio of the sum of squares included in the model divided by the total sum of squares in the data. As such, it is not relevant for most nonlinear and mixed models, though equivalent measures can sometimes be computed\cite{gelman2007data}.

A second problem which can often arise in applied research, is the possibility of Type I error when examining a large number of relationships in one data set. Given the threshold value of .05 for each comparison, and if $n$ independent comparisons are undertaken on the same data set, then the probability of a false positive increases linearly with the number of comparisons, and reaches 1 (implying certainty) when the number of comparisons reaches 20 or greater.

For this reason, a correction is often applied to p values, the most popular being the Bonoferroni correction, which divides the required p value by the number of comparisons which are to be made. However, this approach also has its problems. The existence of such comparisons, and their likelihood of rendering the major hypothesis non-significant leads to a situation where either researchers fail to report the analyses which they actually performed in order to retain their significant p value, or only analyse the data to the extent that their hypotheses are either confirmed or dis-confirmed.
Neither of these situations are scientifically optimal. The first situation encourages the propagation of false positives throughout the scientific literature \cite{ioannidis2005most}, while the second discourages researchers from making the best use of their data.

A further issue which can arise when using classical methods is the problem of replication. Confidence intervals and statistics (such as p values) are based on the notion that if an experiment were repeated 100 times, the true point estimate would lie within the interval 95 (for a 95\% confidence interval) times out of this 100.

There are two problems here, one quite obvious and the other quite subtle. The obvious problem is that given a particular confidence interval, we have no way of telling whether or not this interval contains the true value of the parameter which we are attempting to estimate\cite{gill2002bayesian}. The second problem is one of language, where most scientists think and talk about confidence intervals as though they represent a 95\% chance that the true value lies within the interval. This is simply not true, and can lead to issues of ambiguity when the true definition is unknown or forgotten, which appears to be quite often \cite{falk1995significance} (at least in a sample of psychology students and lecturers).

Another large problem with the notion of replication on equivalent samples as being the arbiter of our probability statements is that it violates Birnbaum's likelihood principle, which is a foundation of modern statistical thought\cite{gill2002bayesian, gelman2004bayesian}. The likelihood principle states that all inferences should be based only on the observed data. The classical methods fail this test through the supposition of arbitrary numbers of experiments which are not carried out in order to justify the uses made of probability.

A similar issue arises when looking at data-sets common in many disciplines which study people. This is that it may be (for many applications, anyway) impossible to precisely replicate the phenomena which we are using statistics on. In a study of time series data for economic interpretation, by a strict following of the assumptions, we should consider a number of replications where the events of the past turned out differently, and use these as our basis for comparison. It is difficult to see how such a history generating device could be constructed, and the use of classical statistics in these situations rests on no clear foundation \cite{gill2002bayesian}.

The final problem with the classical approaches is the approach taken to each new experiment. In classical statistics, the results of an experiment are strictly dependent on the data which are observed. While there is nothing wrong with this, it neglects the important fact that a researcher will often have prior information on the phenomenon under study - otherwise, why is the study being carried out?\cite{gill2002bayesian, gelman2004bayesian} The classical approach ignores such information, even when doing so could increase its explanatory power and compensate for many of its deficiencies, as shall be seen in the next paragraph below.

\subsection{A different approach: Bayesian Statistics}

Bayesian statistics takes it name from that of an English preacher, Reverend Thomas Bayes. In an article published posthumously, he suggested that judging the fairness of a dice (modelled with a binomial distribution) and utilising uniform prior in the first instance, would prove a more useful alternative to classical procedures. However, Bayes himself did no further work in this area, and the foundations for what is known as Bayesian statistics today were laid by Laplace, apparently independently of Bayes \cite{stigler1986history}.

The first major difference between Bayesian and classical approaches is their definition of probability. While classical statistics defines probability as the long run frequency of an event, the Bayesian approach regards probability a subjective degree of belief in a particular outcome. The second major characteristic of the Bayesian approach is the belief that prior information should be incorporated into the results of the analysis. This can mean that the researcher's beliefs about the likelihood should be incorporated into the data, or it can mean that the results of a series of experiments should update the probabilities of the hypotheses over time. In one sense, this approach is similar to the approach of meta-analysis, where a number of different studies are combined and the overall effect size is estimated. However, the approach taken to the accumulation of evidence is quite different.

The method of updating probabilities is through the use of the Theorem of Inverse Probability, or Bayes Theorem. The theorem essentially states: the evidence for a hypothesis given the data, is proportional to the product of the probability of the data given the hypothesis (the likelihood) and the prior probability of the hypothesis.
\begin{equation}
  \label{eq:bayes1}
  \Pr(H|D)=\frac{\Pr(D| H)\times\Pr(H)}{\Pr(H)}
\end{equation}

Equation \ref{eqbayes1} is  a means for updating the beliefs held about a scientific hypothesis based on new evidence. The subjective part of the theorem (the prior) has been criticised by many (including the famous statistician R.A. Fisher) as subjective, and capable of rendering scientific testing meaningless \cite{salsburg2002lady}. However, the argument from the Bayesian side is that most scientists do have prior beliefs about the data, and that it is surely better to have these made part of the evidence base openly, rather than living in the background of a published scientific paper\cite{gill2002bayesian}.

Perhaps the most compelling reason that Bayesian statistics is not the standard method of analysis in the sciences is that the solution of complex statistical problems leads to the evaluation of high dimensional integrals \cite{gill2002bayesian}, which were extremely difficult to solve prior to the development of high speed, easily accessible computing power. As a result, scientists using the Bayesian approach were limited in the kinds of models which they could specify and more importantly, analyse.
 
Beginning in the late 1980's (although the method was developed in statistical physics in the 1950's), the accessibility of Markov Chain Monte Carlo (MCMC) methods on computers changed all of this. Essentially, these methods allow scientists to sample from the posterior distribution an arbitrary number of times (often 10,000 or more) until the chains converge to the same values through a random walk process \cite{gelman2004bayesian}. This has freed scientists from the impossibility of evaluating complex multidimensional integrals by hand, and opened up Bayesian approaches to many more professional scientists.

There are a number of advantages to the Bayesian approach, as well as a number of problems. The first major advantage is that it is philosophically and internally consistent, while the classical procedures are not, as they are an unwieldy synthesis of Fisher's, Newman's and Pearson's approaches towards data analysis \cite{gill2002bayesian, gigerenzer2004mindless}. The second major advantage is that the use of prior information can reduce the impact of unlikely results, given that the prior will correct for extreme values based on previous experience\cite{gelman2010philosophy}.

The third major advantage, already noted above, is the inclusion of prior beliefs openly in the analysis, rather than in an implicit manner through the formulation of hypotheses and methods to test these hypotheses. Fourthly, this use of priors allows the scientist with a diametrically opposed viewpoint to estimate the impact that some new data should have on his probabilities. A fifth major advantage is that more than two models can be compared. In comparison to the null and alternative hypotheses of the classical approach, any number of models can be considered and their probabilities updated in line with the results of the experiment.

In light of the above, it would seem that most scientists should be using Bayesian techniques more often than they do. However, there are also a number of problems with this approach, which are outlined below. The first (and for some, the largest) is the subjective nature of the prior distributions, which many decry as preventing the data from speaking from themselves \cite{gelman2010philosophy}. To some extent this is true, but this problem disappears as more data is collected and the prior ceases to have as much of an impact. A related problem concerns the justification of pror probabilities as subjective degrees of belief. It is impossible for a particular scientist to have all of his or her beliefs regarding possible outcomes of the study in the prior, yet this is a logical necessity if surprising results are ever to be found\cite{gelman2010philosophy}. This is easy to see as if the prior probability of an event is 0, then no amount of evidence can ever change this.

Some Bayesians  recommend thinking of the prior as a regularisation device, and setting them as widely as possible within the confines of the discipline under study \cite{gelman2010philosophy}. Others argue that the priors should be estimated from the data itself (so called Empirical Bayes) \cite{carlin2009bayesian} while many regard this as a violation of the principle that the data should only be used once \cite{gill2002bayesian}. Another major issue in the use of Bayesian statistics is that they are not taught in most disciplines, and require more mathematical and computational sophistication than do frequentist methods, as well as the use of perhaps unfamiliar command line driven software tools.

A final problem with the use of Bayesian methods (the one most favoured by Fisher) \cite{salsburg2002lady} is that scientists may have incompatible priors, and this may retard the development of understanding. However, this is a straw man argument, as scientists already do have widely divergent views about the interpretation of particular research (witness the controversy about what implicit measures actually measure and around the size and interpretation of the placebo effect) , and it would surely be better to allow for these divergent views and incorporate them into analysis, rather than let them fester impotently in the annals of journals.

\subsection{Likelihood Approaches to Statistics}

These approaches are perhaps the most core to the practice of statistics whether Bayesian or frequentist. Conventionally attributed to Fisher \cite{salsburg2002lady}, who developed the method of maximum likelihood, these approaches focus on which hypothesis is most likely given the data. These methods are already in widespread use in many factor analytic and psychometric studies, where models are compared based on the likelihood of the model given the data. Indeed, the AIC and BIC are prototypical examples of likelihood based measures, and can be calculated for almost any statistical model, which allows them to serve as convenient metrics in many situations.

These methods are not concerned with p-values, focusing more on the relative likelihood of different models on the same data set. Such an approach would seem to be ideal for many scientists, as the strong philosophical assumptions of Bayesian methods are absent, yet the problems with the use of p-values as a strict decision criterion are avoided. However, these approaches have difficulty with establishing the absolute superiority of a particular model, while excelling in assessing the relative superiority given a particular data set. In addition there is an attractive information-theoretic approach to statistics \cite{mackay2003information}, but that will not be covered here as this approach is utilised mostly in the field of signal processing and machine learning.  In this research, a mostly likelihood based approach will be taken, but given that much of the research involved two or more samples who were administered the same instruments, some Bayesian approaches will be evaluated alongside the likelihood and frequentist methods to assess their explanatory power and accuracy in psychological research.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "PlaceboMeasurementByMultipleMethods"
%%% End: 

