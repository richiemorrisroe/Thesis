

<<loadpackages, echo=FALSE, results=hide>>=
require(QuantPsyc)
require(GPArotation)
require(ggplot2)
require(psych)
require(xtable)
require(OpenMx)
require(car)
require(arm)
require(caret)
require(nFactors)
require(bcv)
require(glmnet)
require(mokken)
require(eRm)
require(ltm)
require(lavaan)
require(semPlot)
require(reshape2)
require(plyr)
@

<<sourcefunc, echo=FALSE, results=hide>>=
source("func.R")
@




<<importdata, echo=FALSE, results=hide>>=
hom<-read.csv("HOM data for RFINAL.csv")
hom <- hom[,1:65]
write.csv(hom, "homfinal.csv")
hom1 <- hom[with(hom, CollectMeth=="Paper"),]
hom2 <- hom[with(hom, CollectMeth=="Online"),]
## setOptions("xtable.include.rownames"=FALSE)
## setOption("xtable.format.args", list=(big.mark=","))
@ 



\section{Introduction}
\label{sec:introduction}

This chapter reports an investigation of the psychometric properties of three measures, the RAND Medical Outcomes Survey (RAND-MOS), a measure of self reported health, the Mindful Attention Awareness Scale (MAAS), a measure of mindfulness and the Life Orientation Test, Revised (LOT-R), a measure of optimism. 


The major experimental work of this thesis involved the placebo effect, and as the research considered the utility of a new measure to predict this response, it was important to administer some instruments which have been shown to be associated with it in previous studies.

Additionally, as the new method proposed to predict the placebo response in healthy volunteers involved the use of implicit measures (two IAT's) (c.f.  Chapter~\ref{cha:literature-review}) it was also necessary to collect data on the construct of mindfulness as operationalised using the Mindful Attention Awareness Scale (MAAS), as this construct has been shown to mediate the relationship between explicit and implicit measures~\cite{Levesque2007}. 

Over the course of the study of the placebo, there have been few individual-level psychological predictors of the effect which have been reproducible. Expectancy is the most commonly measured construct within the field, and some meta-analytic evidence suggests that it is only significantly associated with the response around 60\% of the time~\cite{DiBlasi2001}.   

Some authors claim that the search is fruitless \cite{Shapiro1997} while others argue that the placebo effect is shaped more by the situation, and therefore there is no such thing as a placebo responder \cite{Kaptchuk2008a}. 

However, in recent years optimism~\cite{Geers2005,morton2009reproducibility} has been shown to be associated with the placebo response. More specifically, the Life Orientation Test, Revised (LOT-R) has been shown in a number of studies to be associated with the response to placebo. 
%% Interestingly enough, the Morton study referenced above showed an effect for optimism only in a repeated measures design, but the Geers \textit{et al} study showed a significant relationship in a between-subjects design. The relationship between placebo effect and optimism is discussed in more detail in Section \ref{sec:optimism} below. 

Mindfulness is a construct that has both been associated with health~\cite{Carmody2008} and moderation  of the relationship between explicit and implicit measures~\cite{Levesque2007}. As such, it is a construct which had been associated with the IAT in previous work (c.f. Chapter ~\ref{cha:literature-review}) and could plausibly be associated with health. Some more in-depth discussions of this potential relationship were outlined in Chapter~\ref{cha:methodology}, in Section~\ref{sec:embod-cogn-plac}. 

Health (in the form of the RAND-MOS) was included in this piece of research both to replicate the optimism-health link and examine the relationship between  optimism and mindfulness. 

Additionally, the relationship between these three variables (health, optimism and mindfulness) can provide some insight into health cognitions, which are theoretically linked to placebo-related cognitions. This follows as if the placebo effect is the result of expectancies, then there should be some shared variance between the response to placebo and other health-related cognitions. 

The primary aim of this study was to investigate the relationships between optimism and mindfulness in the population under study, and to provide a basis for the building of psychometric models which could be used to predict the relationship of these variables to the placebo response in the experimental study. 

The study formed an opportunity to collect background data for the population of interest, to assess if the participants in the experiment were systematically different from those who had responded to a survey invitation. This is critical if the results from the experimental portion of the research are to generalise to any further samples, given that inferences cannot be made about the experimental sample if it is not understood how they stand in relation to other samples from the overall population which was used throughout the research. This process should allow for more accurate predictions of participant responses, assuming that the models generalise to the new sample. 



\section{Optimism and Placebo}
\label{sec:optimism}


Dispositional optimism is often defined as \textit{generalised outcome expectancies about the future}~\cite{Carver2010}. When using this definition, it seems relatively likely that there may be a relationship between optimism and placebo response, and yet this has only been investigated in recent years. 

Dispositional optimism appears to exert some influence on placebo effects, in some situations~\cite{Geers2005,morton2009reproducibility}. The effect seems to be that those higher in optimism respond better to positive suggestions, while those higher in pessimism respond better to negative suggestions. 

Another study found that general (but not specific) expectancies had a significant impact on the response to placebo in a meta-analysis of randomised controlled trials of chronic back pain~\cite{myers2008patient}. Generalised expectancies are how optimism is defined, given that all expectancies around future states of health are essentially outcome expectancies. 

Other studies appear to  show that this optimism effect is not general, but rather depends on the context in which the experiment takes place~\cite{Hyland2006}. In the Hyland \textit{et al} experimental study, spirituality rather than optimism was a predictor of the response.  

Crucially, this only occurred when the treatment was classified as spiritual. When a gratitude based treatment was used, gratitude acted as a predictor. These results suggest that any trait which predicts placebo response will likely only be effective in certain situational settings~\cite{Kaptchuk2008a}. 

This would seem to suggest that although there appear to be some dispositional predictors of the placebo response, they are moderated by the context in which the study or treatment takes place. 

Alternatively, one could argue that expectancies drive these effects by mediating the impact of other contextually relevant variables. However, to take this position would require that the theory of Kirsch, that expectancies exert direct physiological effects, would need to be abandoned~\cite{Kirsch1985}, and indeed this proposition was tested in the experimental portion of the research (c.f. Chapter~\ref{cha:primary-research})

A recent study~\cite{morton2009reproducibility} in a placebo analgesia paradigm argues for a stronger interpretation of the role of optimism. This experimental study used a repeated measures design, and utilised a preconditioning method in the first session which is known to increase the size of the placebo response~\cite{Voudouris1985}. 

While in the first session there was no effect of optimism on the results, in the second study dispositional optimism was significantly correlated with placebo analgesia, explaining 55\% of the variance. This would suggest that while optimism may not produce a placebo response in itself, once a response has been produced it can be effective in maintaining it over time. 

Hyland suggests that the optimism effects on placebo response are mediated through expectancies, and that when these are not a factor, neither is optimism. This sounds plausible, but the relationship could easily go the other way in that optimism could drive the observed effects of expectancies. This is not a question which can be answered without further empirical investigations, some of which were conducted as part of this research (see Chapter \ref{cha:primary-research}, especially Section \ref{sec:test-theor-models}). 


The relationship between optimism and health is well known. 
Optimism was recently reviewed in relation to its 
effect on physical health and well-being~\cite{Carver2010}, 
optimism has been used as a predictor for many years in the area of Psychoneuroimmunology (PNI) and appears to be  associated with better health outcomes than
is pessimism~\cite{Baker2007,Conway2008}. Optimism has also
been associated with lower mortality risk in a large longitudinal
cross-sectional study of individuals at risk for cardiovascular disease
(the Women's Health Initiative)~\cite{Tindle2009}. 

A meta-analysis
has also confirmed this link between optimism and better coping styles,
as well as a strong negative relationship between optimism and negative
affect~\cite{andersson1996benefits,nes2006dispositional}. Higher levels
of optimism have also been associated with quicker recovery from surgery,
insulin therapy and chemotherapy~\cite{Allison2000}. A prospective
study looking at outcomes from a group of head and neck cancer patients
found that optimists consistently reported better health outcomes
than pessimists~\cite{Allison2000}.%%  A meta-analysis reported on correlations
%% between optimism and post-traumatic growth, which found a mean effect
%% size of 0.2, but the studies were of quite poor quality, so this finding
%% must be regarded as tentative~\cite{Bostock2009}.

The question of the mechanism by which optimism manifests differences
in health is still unclear. Some researchers argue for a direct effect
of optimism on immune function,  while others
argue that optimism exerts its protective effects through the effects
of persistent striving after health goals~\cite{Segerstrom2003}.

%% Additionally, there appears to be a curvi-linear relationship between optimism and health in some studies, which has been  attributed to participants holding both high optimistic and high pessimistic outcome expectancies simultaneously. 

\subsection{Factorial Structure of the LOT-R}
\label{sec:fact-struct-lot}

The factor structure of the LOT-R (and indeed, the dimensionality of the optimism construct more generally), has been a subject of some debate. The originators argue for a one-factor structure~\cite{Carver2010,Scheier1994}, while other researchers have found that the scale is better modeled as having both optimism and pessimism components. The Geers work referenced above took a latter approach, dichotomising the scores on the scale to create groups of optimists and pessimists\footnote{this approach is not without its problems}. Therefore, both one and two factor models were applied to the scale to determine if a one or two factor structure fit better in the population of interest. 


\section{Mindfulness}

The construct of Mindfulness or 'attentional control' has been defined as: 
\begin{quotation}
'a mental ability which facilitates a direct and 
immediate perception of the present moment with non-judgemental awareness'  
\end{quotation}
\cite{kohls2009facets} (p. 2). Derived from the Buddhist 
contemplative traditions, mindfulness represents attention to the thought process, rather than to thought content~\cite{brown2007addressing}. 
Jon Kabat-Zinn popularised the practice of mindfulness in the West~\cite{Kabat-Zinn2003}, developing an eight-week 
long Mindfulness Based Stress Reduction (MBSR) programme which is now often used in clinical settings for patients with chronic illnesses. 

The usefulness of mindfulness in this research was twofold
\begin{enumerate}
\item It has been shown to be a predictor of health, and treatment programs developed around mindfulness practice have been associated with improved physiological and psychological outcomes

\item Additionally, the construct has also been implicated as a moderator of the relationship between explicit and implicit measures~\cite{Levesque2007}
\end{enumerate}

The combination of these two features was why mindfulness (specifically the Mindful Attention Awareness Scale) was chosen as the measure to test the mediation relationship between explicit and implicit expectancies. 

\subsection{Health benefits of Mindfulness}
\label{sec:health-benef-mindf}



In a study of cancer patients mindfulness training was found to reduce stress and to improve quality of life~\cite{Carlson2007}, 
while in a study of patients who were HIV positive, MBSR training was associated with improved natural killer cell activity 
\cite{Robinson2003}. Meta-analyses have suggested that MBSR programmes have an effect size of $d=0.5$ for psychological variables 
such as quality of life and mental health, and $d=0.2$ for physical outcome variables such as cortisol levels and immune function~\cite{Grossman2004}.
Measures of the construct of mindfulness date from the early years of this century. Mindfulness has been operationalised 
into a number of different scales including the Mindful Attention Awareness Scale (MAAS)~\cite{brown2003benefits} , 
the Kentucky Inventory of Mindfulness Skills~\cite{Ruth2004} and the Five Facet Mindfulness Questionnaire (FFMQ), 
which was developed by the factor analysis of items from a number of different scales~\cite{Ruth2006}. The most popular instrument is
 the Mindful Attention Awareness Scale (MAAS)~\cite{brown2003benefits}.

 Interestingly, while higher MAAS scores have been found to be associated with meditation experience in some 
studies~\cite{brown2003benefits}, other research using student samples found no significant correlation between 
MAAS scores and experience with meditative practices (assessed by self report)~\cite{MacKillop2007}. This finding which
 was also replicated by Thompson and Waltz~\cite{thompson2007everyday}  who suggest that this may be due to the fact that 
mindfulness during meditation may be a state like construct, while mindfulness in everyday life may be a trait like construct. 
While various rigorous reviews of clinical trials have found MBSR programmes to be effective for
 reducing stress~\cite{Chiesa2009,Praissman2008} , it is still unclear whether individuals 
with higher levels of mindfulness are also psychologically healthier.
 MBSR programs have also been associated with higher levels of optimism~\cite{Carson2004}, but mindfulness and optimism have not tended to be researched together. 
 Only one study where mindfulness levels were correlated positively 
with self reported health~\cite{hansen2009measuring} was identified as part of this research. Hansen, using a cross-sectional design, 
found that MAAS scores correlated with a five point one item measure of health,   
a very crude measure of health status. In a recent meta-analysis of 29 studies, 
Giluk explored the relationship between mindfulness, Big Five personality and affect~\cite{giluk2009mindfulness}.
These authors found found that mindfulness was strongly correlated with Neuroticism and negative affect, though its not obvious whether being mindful lowers neuroticism or whether neuroticism interferes with mindfulness. 


\section{Aims of the Research}

As discussed above, optimism is well known as a predictor of health, both self reported and objectively assessed. Mindfulness, while a much newer construct, has also been associated with health in a number of studies.  In addition, optimism has been associated with the placebo response in some research, while mindfulness has been proposed as a mediator of the relationship between explicit and implicit measures.

The primary aim of this study was to develop and test psychometric models that could be used to predict scores according to IRT and factor analysis criteria in the experimental part of the research. In order to do so, both factor analytic and item response models were fit to the optimism and mindfulness scales, as well as the health scales for Sample One. 

A secondary aim of this part of thesis was both to replicate the optimism-health link reported in previous studies and to examine the relationships between health, mindfulness and optimism.



The major hypotheses of this part of the thesis were as follows:
\begin{itemize}
  \item The RAND MOS would have 8 first order factors

\item The MAAS would have one factor

\item The LOT-R would have one factor.
  
\item Optimism and mindfulness would be positively associated with health.

\item Optimism and mindfulness would be positively associated with one another. 
\end{itemize}

%% The major types of analysis carried out are described below. 

The response rate for Sample One  (paper) was approximately
90\% of those asked (N=392), while the response rate for Sample Two  (Online)
was 10\% (N=1501). %% Note that pseudo-random sampling was used for Sample One, while simple random sampling was used for Sample Two. 

\section{Methods}
\label{sec:methods}

The methods used for this part of the thesis were primarily psychometric. Cross-validation approaches (described in Chapter \ref{cha:methodology}) were applied to both samples to increase generalisability of the models to the experimental portion of the research. This section describes the measures used for this part of the study, followed by a description of the sample, and concludes with a description of the methods of analysis used in this study. 

\subsection{Participants}
\label{sec:participants}

<<demo, echo=FALSE, results=tex>>=
hom.gend.meth <- plyr::count(hom, vars=c("Gender", "CollectMeth"))
hom.gend.meth2 <- na.omit(hom.gend.meth)
hom.gend.meth.tab <- recast(hom.gend.meth2, Gender~CollectMeth)
print(xtable(hom.gend.meth.tab, label="tab:homdemo", caption="Gender Breakdown of Participants by Collection Method"), include.rownames=FALSE)
@ 

As can be seen from Table~\ref{tab:homdemo}, both samples were predominantly female, in line with the general gender balance of the University from which the samples were taken. 

The majority of the first sample (N=390) were undergraduate students (N=304), and the mean age in the sample was 23.18 (SD=7.9). The majority of the second sample (N=1107) were undergraduate students (N=937) while the mean age was 22.3 (SD=6.8). 

<<scorescalestransform, echo=FALSE, results=hide>>=
grep.rand <- grep("^RANDQ", x=names(hom))
randitems <- hom[,grep.rand]
rand <- "RANDQ"
randset1 <-paste(rand, c(1, 2, 20, 22, 34, 36), sep="")
randset1 <- hom[,randset1]
randrecode1 <- RecodeMany(randset1, vars=c("RANDQ1", "RANDQ2","RANDQ20", "RANDQ34","RANDQ36"), Recodings=("1=100;2=75;3=50;4=25;5=0"))
randset2 <- paste(rand, c(3:12), sep="")
randset2 <- hom[,randset2]
randrecode2 <- RecodeMany(randset2, vars=c("RANDQ3", "RANDQ4","RANDQ5","RANDQ6","RANDQ7","RANDQ8","RANDQ9", "RANDQ10","RANDQ11","RANDQ12"),Recodings="1=0;2=50;3=100")
randset3 <- paste(rand, c(13:19), sep="")
randset3 <- hom[,randset3]
randrecode3 <- RecodeMany(randset3, vars=c("RANDQ13", "RANDQ14", "RANDQ15", "RANDQ16","RANDQ17","RANDQ18", "RANDQ19"), Recodings="1=0;2=100")
randset4 <- paste(rand, c(21,23,26,27,30), sep="")
randset4 <- hom[,randset4]
randrecode4 <- RecodeMany(randset4, vars=c("RANDQ21", "RANDQ23", "RANDQ26","RANDQ27","RANDQ30"), Recodings="1=100;2=80;3=60;4=40;5=20;6=0")
randset5 <- paste(rand, c(24,25,28,29,31), sep="")
randset5 <- hom[,randset5]
randrecode5 <- RecodeMany(randset5, vars=c("RANDQ24", "RANDQ25", "RANDQ28", "RANDQ29", "RANDQ31"), Recodings="1=0;2=20;3=40;4=60;5=80;6=100")
randset6 <- paste(rand, c(32,33,35), sep="")
randset6 <- hom[,randset6]
randrecode6 <- RecodeMany(randset6,vars=c("RANDQ32","RANDQ33","RANDQ35"), Recodings="1=0;2=25;3=50;4=75;5=100")
randrecoding <- ls(pattern="randrecode")
randrecoding.df <- as.data.frame(lapply(randrecoding, function (x) get(x)))
randsortpaste <- paste(rand, c(1:36), sep="")
randitems.unscored <- hom[,grep.rand]
randitems.scored <- randrecoding.df[,randsortpaste]
hom[,grep.rand] <- randitems.scored
hom <- createSumScores(hom)
hom.scores <- createSumScores(na.omit(hom))
hom1 <- hom[hom$CollectMeth=="Paper",]
hom2 <- hom[hom$CollectMeth=="Online",]
@



\subsection{Measures}
\label{sec:measures}
There were three measures used for this part of the research.
\begin{enumerate}
\item \textit{RAND-MOS}: The RAND Medical Outcomes Survey produced the most widely used instrument of HRQoL (Health Related Quality of Life) worldwide~\cite{hays1993rand}. The instrument was later revised and the number of response categories standardised across scales and renamed the SF-36. The older version was used for this research, as they are extremely similar and the newer version is under copyright and expensive to use, even for non-commercial research. The RAND-MOS has 36 questions, and is divided into 8 sub-scales, General Health (GH),
Physical Functioning (PF), Role Limitations (RL), Emotional Role Limitations
(RLE), Pain (PN), Energy (EN), Emotional Well Being (EMWB) and Social
Functioning (SF). All sub-scales have shown acceptable reliability
(>.7) in other studies in the literature~\cite{Lam2007,Ferreira2000}.
The instrument has 8 first order factors and two higher order factors~\cite{Hann2008}. The scale involves dichotomous, trichotomous and
five and six point scales for various items, so all questions were
transformed to a 100 point scale before analysis, where higher scores
represent better functioning.
\item The \textit{Mindful Attention Awareness Scale (MAAS)}
\cite{brown2003benefits} is a 15 item scale which is scored on
a six point scale from ``almost always'' to ``almost never''. The scale uses questions which measure mindlessness. The summary score is produced
from the mean of all individual scores. The scale has shown adequate
psychometric validity in many samples, with alpha ranging from 0.7
to 0.9~\cite{brown2003benefits,Ruth2006}.
\item \textit{Life Orientation Test, Revised:} The Life Orientation Test Revised (LOT-R) was developed and revised
by Scheier and Carver~\cite{Scheier1994}, and consists of 10 items.
Three of the items load on pessimism, three on optimism and four are
distractor items. The LOT-R has shown excellent psychometric validity,
and is very commonly used as a measure of optimism/pessimism. The
scale is scored on a 5 point scale,
and the mean of all items after items 3,7 and 9 are reverse coded (and the distractor items removed) is taken to produce the overall score.
\end{enumerate}



\subsection{Sampling for this research}
\label{sec:sampl-this-rese}
The participants in the paper collection method part completed the forms by hand between August and October 2009. The participants were sampled pseudo-randomly from all of the public areas (coffee shops, restaurants etc) of the campus.

Following this pen and paper approach to sampling, the survey was sent to a random selection of students via email on December 12th 2009, and data was collected and analysed from this point until the 24th of December 2009. Differences between the samples and the possible effects of these on the results obtained are discussed below. %% In addition, due to the unexpected results of the analysis, a third sample was collected in Summer 2011. 

\section{Analysis}
Analysis was carried out separately on the two samples, to allow for development of factor analysis and IRT models on the first sample and validation on the second. In addition, it could not be assumed that two samples collected in different ways would be comparable. 

%% Firstly, the proportion of missing data was identified, and multiple imputation was employed to combat this problem in cases where there were substantial amounts of missing data. This was only necessary in the case of the sample collected by online methods. 

All missing data was assumed to be Missing Completely at Random (MCAR)~\cite{little1987statistical}, and thus a complete-cases analysis was carried out on all data. 

%% The data was checked for errors in entry or recording using summary functions and plots. Following this, the question responses were re-coded according to the instructions.

%% Following this, the summary scores were calculated. Next, summary statistics and characteristics of the data were reported. 
%% Following this, a correlation matrix for the data was calculated and %% analysed.

The majority of the analytical procedures were as described in Chapter \ref{cha:methodology}. In brief, Factor analytic and item response theory (not Rasch) models were fit to the data, and tested on either new samples (Sample One) or on other splits of the data (Sample Two). All regression results were obtained through the use of ten-fold cross-validation on the training set with a stepwise selection method, and the reported coefficients are from the test set, and thus are unbiased by the search process. Stepwise selection methods were used as they are reasonably standard within the field, and unlike other selection methods (lasso and ridge regression), these report a p-value, in line with APA standards.   In general, all correlations reported are rank-correlations, to control for the non-normality of the input variables. 



%% Next, simple reliability analyses (Cronbach's $\alpha$) were carried out on the scales themselves. Following this, parallel analysis, the MAP criterion and the scree plot were used to estimate the number of factors which could be extracted from the data. After this, factor solutions were extracted using principal axis methods, with maximum likelihood estimation used if these failed to converge. Primarily, direct oblimin methods of rotations were utilised, but promax rotations were also applied to ensure that the proposed structure was not overly sensitive to the methods of rotation.

%% After the various factor structures were obtained, they were plotted and analysed for interpretability. %% Communalities and uniquenesses were assessed to ensure that there was no over or under factoring in the solutions. Communalities were then graphed against the number of factors extracted and the methods of extraction to provide a simple graphical guide to the usefulness of each solution.

%% Following this procedure of extraction and interpretation, Structural Equation Modelling was applied to each of the proposed factor solutions. The optimal factor solution was chosen using the AIC of each fitted model, along with the RMSEA of the proposed solutions.

%% The successful SEM models from the first sample were then tested on a subset of the second sample, to determine their performance on new data. 

%% Following the development and testing of SEM models, each of these was tested on the validation set and factor scores were created for each of the measures.

%% Additionally, SEM models were applied to determine the best fitting model for the relationships between the three primary constructs of interest (general health, optimism and mindfulness). 

%% Following the investigation of structure with the methods of classical test theory, the scales were analysed using Rasch models and item response theory. Firstly, Mokken analyses  were run, in order to check the assumptions of monotonicity, local independence and to assess how many sub-scales the analysis should be carried out on. 

%% Following this, three consecutively more complex IRT models were fit to the data, and person and item parameters estimated for each of these. 


%% After this, linear regressions were run to examine the differential effects of each of the correlated variables. Stepwise selection on the training set was carried out, along with lasso and ridge regression  methods. Within each split, each regression model used ten-fold cross-validation to choose the optimal penalty criterion. 

%% The performance of all methods was then assessed on held-out data. In the case of Sample One, some of Sample Two was used as a heldout data set. For Sample Two, the entire dataset was split into three splits, and the cross validation procedure carried out for each. The splits were kept quite large (approximately 300 non missing observations) to allow for psychometric models to be fit to each split seperately. %% and to be able to compare the performance of simple mean/sum scores against the factor scores and ability estimates derived from the psychometric modelling procedures. 

%% The approach taken to the psychometric analysis of the second sample of data was as follows.  Firstly, factor models were built on the two remaining samples from this dataset (the first having been used to validate the results from Sample 1).  Next, a CFA was run on each of the other samples, such that if the model was developed on the B sample, it was tested on both the A and the C sample.  This provides a better measure of accuracy and replicability for each of the proposed factor structures.  Finally, the most successful model was back-tested on the data from Sample One. The model(s) chosen by this procedure was then used to predict these scores for the experimental portion of the research (and compared against the typical scoring methods). 

%% The testing was carried out using a few different methods.  Firstly, CFA models were fitted to the new data, allowing for comparision of their effectiveness on unseen data. Factor scores were then calculated from these models. Additionally, the ability estimates from the best fitting IRT models were also calculated. These models were used to predict on the experimental data (c.f. Chapter \ref{cha:primary-research}).

%% A different procedure was  followed for the IRT models, using  three fold cross validation used to build an IRT model on each of the ten segments, and then comparing the estimates ability scores from the model built on the 33\% of the data, and from the estimates built on the held-out data. The difference between these two measures will provide an estimate of error for each of the model\'s predictive capability. This will then provide a metric for the selection of the best model, which can then be applied to the experimental data. 

%% \subsection{Crossvalidation Approaches}
%% \label{sec:crossv-appr}

%% \subsubsection{Crossvalidation for factor selection}
%% \label{sec:crossv-fact-select}

%%  As discussed in the Methodology, there are two main approaches here, either item-subject CV (known as Gabriels method) or the typical ten fold validation common in  machine learning (Wolds method). Both of these approaches were examined for the RAND items, and the results are shown in Section \ref{sec:rand-mos-1}. In the gabriel approach, a leave two of item and subject out was used, while ten fold cross-validation was used for the Wold method. One problem with this approach is that there were a huge number of non-responses to questions 13-16 on the RAND MOS, for unknown reasons.  This brings down the potential sample on this instrument to 281, which is not enough for the full analysis.  However, it is enough to test the factor solutions from the first sample, and as the RAND was not used in the experimental part of the research, this was considered sufficient. 

%% \subsubsection{CrossValidation for Psychometric Models}
%% \label{sec:crossv-psych-models}

%% Cross-validation is a tool for developing and testing models where data is limited or it is desirable that all data should be used efficiently, while guarding against over-fitting, which is when transitory features of a particular data-set are modeled, which reduces generalisability. 

%% Typical cross-validation holds back some data in order to test the models developed on the rest.  This hold-out sample is typically of the order of 10\%.  However, for factor analyses, around 300 observations are typically needed for accurate estimation of parameters.  
%% In this research, models were developed on the first sample, and then were fitted to a subset of the second sample.  This procedure was then  repeated with the data not used for testing in the second sample and the first sample.  This reduced the potential influence of over-fitting, while developing psychometric models which could be applied to the experimental sample. 













<<missingdata, echo=FALSE, results=hide, cache=TRUE>>=
paper.missing <- sapply(hom1, function (x) sum(is.na(x)))
online.missing <- sapply(hom2, function (x) sum(is.na(x)))
@
%% \begin{figure}
<<label=papermissingplot, echo=FALSE, results=hide,  eval=FALSE>>=
ggplot(as.data.frame(paper.missing),aes(x=paper.missing, y=..density..))+geom_density()
@
%%  ~\caption{Density Plot of Missing values in paper sample. x Axis is the proportion of responses with that  number of missing observations in each variable }
%%   \label{fig:papermissingplot}
%% \end{figure}

%% \begin{figure}
<<onlinemissingplot, echo=FALSE, results=hide, eval=FALSE>>=
  print(ggplot(as.data.frame(online.missing), aes(x=online.missing))+geom_density())
@
%%  ~\caption{Histogram of Missing Values, Online sample}
%%   \label{fig:onlinemissingplot}
%% \end{figure}




%% As can be seen from Figure \ref{fig:papermissingplot}, there are quite low levels of missing data for the sample collected by paper.


%% However, the situation is very different with the second sample, as can be seen from Figure \ref{fig:onlinemissingplot}, where there are a number of items which have between six and eight hundred missing values.  This was investigated further, as this amount of missing values in a small set of the data may cause problems in the course of the analysis. 

<<missingvaluestable, echo=FALSE, results=hide>>=
missing.many <- online.missing[online.missing>300]
missing.many.df <- as.data.frame(missing.many)
names(missing.many.df) <- "Number of Missing Observations"
missing.many.xtab <- xtable(missing.many.df, label="tab:missingmanytable", caption="Number of Missing Observations for RAND MOS items with greater than 10 percent missingness")
print(missing.many.xtab)
@

%% It can be seen from Table \ref{tab:missingmanytable} that the problems with missing data are concentrated in four consecutive questions, RAND questions 13 through 16. These questions all load on the Role Limitations subscale, explaining why this subscale shows up with lots of missing data.  The consecutive nature of the data suggests that the reason for this may be that participants believed that it was only necessary to answer one of the questions, though that does not explain why so many people did not answer the first question.

%% Next, we will impute the missing data using a multiple imputation procedure (as discussed in the Methodology).

<<impute, eval=FALSE, echo=FALSE, results=hide, cache=TRUE>>=
require(mice)
hom.imp <- mice(hom[,2:length(hom)], m=10) #remove first column as it causes problems with the imputation
hom.comp1 <- complete(hom.imp, 1)
hom.comp2 <- complete(hom.imp, 2)
hom.comp3 <- complete(hom.imp, 3)
hom.comp4 <- complete(hom.imp, 4)
hom.comp5 <- complete(hom.imp, 5)
hom.comp6 <- complete(hom.imp, 6)
hom.comp7 <- complete(hom.imp, 7)
hom.comp8 <- complete(hom.imp, 8)
hom.comp9 <- complete(hom.imp, 9)
hom.comp10 <- complete(hom.imp, 10)
homlist <- list(hom.comp1, hom.comp2, hom.comp3, hom.comp4, hom.comp5, hom.comp6, hom.comp7, hom.comp8, hom.comp9, hom.comp10)
for (i in 1:length(homlist)) {
  homi <- homlist[[i]]
  write.csv(homlist[[i]], file=paste("homcomp", i, ".csv", sep=""))
}
  
@ 

<<readcomphom, echo=FALSE, results=hide, cache=TRUE>>=
homfiles <- list.files(pattern="homcomp")
homlist <- lapply(homfiles, read.csv)
@ 







<<sumstats, echo=FALSE,results=hide>>=
sumorder <- c("generalhealth", "physfun", "rolelim", "rolelimem", "energyfat", "emwellbeing", "socialfunctioning", "pain", "mindfulness", "optimism")
hom.tot <- hom1[,66:75]
hom.tot <- hom.tot[,sumorder]
tot.sum2 <- apademotables(hom.tot)
longnames <- c("General Health","Physical Functioning (RAND)", "Role Limitations (RAND)", "Emotional Role Limitations", "Energy Fatigue", "Emotional Well Being", "Social Functioning", "Pain",  "Mindfulness (MAAS)", "Optimism (LOT-R)")
tot.sum2[,1] <- longnames

## print(tot.xtab, include.rownames=FALSE) #include packing rotating if fails here
@

\section{Results}
\label{sec:results}



\subsection{Descriptive Statistics}
\label{sec:descr-stat}



In advance of the analysis, frequencies, means and ranges were calculated
for the major variables of interest (General Health, Mindfulness, Optimism and Emotional Well Being) for the first sample.  The results of this analysis can be seen in Table~\ref{tab:sumstatscales}. 





%% In Table \ref{tab:democollect} the breakdown of the demographics
%% of the sample by Collection Method is shown.

<<demostats, echo=FALSE, results=hide>>=
hom.demo <- hom1[,2:8]
hom.demo.xtab <- xtable(summary(hom.demo), label="tab:democollect", caption="Demographic Statistics for Sample One (paper sample)")
print(hom.demo.xtab, scalebox=0.6, include.rownames=FALSE)
@


The mean level of mindfulness in the sample was higher than the mid-point of the scale, while optimism levels were at the mid-point point of the scale. The average level of self-reported health  tended to be well above the mid-point for all of the subscales, which makes sense given the non-clinical sample involved in this research. 

<<scaleitems, echo=FALSE, results=hide, cache=TRUE>>=
## rand.grep <- grep("^RAND", x=names(hom1))
## randitems <- hom1[,rand.grep]
maas.grep <- grep("^MAASQ", x=names(hom1))
maasitems <- hom1[,maas.grep]
lotr.grep <- grep("^LOTRQ", x=names(hom1))
lotritems <- hom1[,lotr.grep]
@

<<scalesalpha, echo=FALSE, results=tex>>=

physicalfunctioning <- with(hom1,  hom1[,grep("RANDQ[3456789]$|RANDQ[1][012]$", x=names(hom1))])

rolelimitations <- with(hom1, hom1[,grep("RANDQ[1][3456]$", x=names(hom1))])
emotionalrolelimitations <- with(hom1, hom1[,grep("RANDQ[1][789]$",x=names(hom1))])
energyfatigue <- hom1[,with(hom1, grep("RANDQ[2][379]$|RANDQ31$", x=names(hom1)))]
emotionalwellbeing <- hom1[,with(hom1, grep("RANDQ[2][4568]$|RANDQ30$", x=names(hom1)))]
socialfunctioning <- hom1[,with(hom1, grep("RANDQ20|RANDQ32", x=names(hom1)))]
pain <- hom1[,with(hom1, grep("RANDQ[2][12]$", x=names(hom1)))]
generalhealth <- hom1[,with(hom1, grep("RANDQ1$|RANDQ[3][3456]$", x=names(hom1)))]
gh.alpha <- alpha(generalhealth)$total$std.alpha
pf.alpha <- alpha(physicalfunctioning)$total$std.alpha
rl.alpha <- alpha(rolelimitations)$total$std.alpha
erl.alpha <- alpha(emotionalrolelimitations)$total$std.alpha
emwb.alpha <- alpha(emotionalwellbeing)$total$std.alpha
sf.alpha <- alpha(socialfunctioning)$total$std.alpha
pain.alpha <- alpha(pain)$total$std.alpha
ef.alpha <- alpha(energyfatigue)$total$std.alpha
scalenames <- c("genhealth", "physfun", "rolelim", "rolelimem", "emwellbeing", "socialfunctioning", "pain", "energyfatigue")

maas.alpha <- alpha(maasitems)$total$std.alpha
lotr.alpha <- alpha(lotritems)$total$std.alpha
alphas <- c(gh.alpha, pf.alpha, rl.alpha, erl.alpha, ef.alpha, emwb.alpha, sf.alpha, pain.alpha,  maas.alpha, lotr.alpha)
## alpha.df <- data.frame(Scale=scalenames, Alpha=alphas)
tot.sum2[,"Alpha"] <- alphas
tot.xtab <- xtable(tot.sum2, label="tab:sumstatscales", caption="Summary Statistics for Health Scales, Mindfulness and Optimism, Sample One")
print(tot.xtab, scalebox=0.9, include.rownames=FALSE)
@ 





<<hom2scales, echo=FALSE, results=hide, cache=TRUE>>=
maas <- "MAASQ"
lotr <- "LOTRQ"
randitems.paste <- paste(rand, c(1:36), sep="")
randitems2 <- hom2[,randitems.paste]
maasitems.paste <- paste(maas, c(1:15), sep="")
maasitems2 <- hom2[,maasitems.paste]
lotritems.paste <- paste(lotr, c(1,3,4,7,9,10), sep="")
lotritems2 <- hom2[,lotritems.paste]
@

<<calcalpha, echo=FALSE, results=hide>>=
maas.alpha <- alpha(maasitems)
lotr.alpha <- alpha(lotritems)
@ 




<<hom2demo, echo=FALSE, results=tex>>=
physicalfunctioning <- with(hom2,  hom2[,grep("RANDQ[3456789]$|RANDQ[1][012]$", x=names(hom2))])

rolelimitations <- with(hom2, hom2[,grep("RANDQ[1][3456]$", x=names(hom2))])
emotionalrolelimitations <- with(hom2, hom2[,grep("RANDQ[1][789]$",x=names(hom2))])
energyfatigue <- hom2[,with(hom2, grep("RANDQ[2][379]$|RANDQ31$", x=names(hom2)))]
emotionalwellbeing <- hom2[,with(hom2, grep("RANDQ[2][4568]$|RANDQ30$", x=names(hom2)))]
socialfunctioning <- hom2[,with(hom2, grep("RANDQ20|RANDQ32", x=names(hom2)))]
pain <- hom2[,with(hom2, grep("RANDQ[2][12]$", x=names(hom2)))]
generalhealth <- hom2[,with(hom2, grep("RANDQ1$|RANDQ[3][3456]$", x=names(hom2)))]
gh2.alpha <- alpha(generalhealth)$total$std.alpha
pf2.alpha <- alpha(physicalfunctioning)$total$std.alpha
rl2.alpha <- alpha(rolelimitations)$total$std.alpha
erl2.alpha <- alpha(emotionalrolelimitations)$total$std.alpha
emwb2.alpha <- alpha(emotionalwellbeing)$total$std.alpha
sf2.alpha <- alpha(socialfunctioning)$total$std.alpha
pain2.alpha <- alpha(pain)$total$std.alpha
ef2.alpha <- alpha(energyfatigue)$total$std.alpha
maas2.alpha <- alpha(maasitems2)$total$std.alpha
lotr2.alpha <- alpha(lotritems2)$total$std.alpha
alphas2 <- c(gh2.alpha, pf2.alpha, rl2.alpha, erl2.alpha, emwb2.alpha, sf2.alpha, pain2.alpha, ef2.alpha, maas2.alpha, lotr2.alpha)
scalenames <- c("genhealth", "physfun", "rolelim", "rolelimem", "emwellbeing", "socialfunctioning", "pain", "energyfatigue")
hom2.tot <- hom2[,c("generalhealth", "physfun",  "rolelim", "rolelimem",  "emwellbeing", "socialfunctioning", "pain", "energyfat", "mindfulness", "optimism")]
hom2.tot <- 
hom2.sum <- apademotables(hom2.tot)
names(hom2.sum)[1] <- "Scale"
hom2.sum[,"Alpha"] <- alphas2
longnames <- c("General Health","Physical Functioning (RAND)", "Role Limitations (RAND)", "Emotional Role Limitations", "Emotional Well Being", "Social Functioning", "Pain","Energy Fatigue" , "Mindfulness (MAAS)", "Optimism (LOT-R)")
hom2.sum$Scale <- longnames
print(xtable(hom2.sum, label="tab:hom2tots", caption="Summary Statistics for Health Scales, Mindfulness and Optimism, Sample Two "), scalebox=0.9, include.rownames=FALSE)
@ 

The summary statistics for the Sample Two are shown in Table~\ref{tab:hom2tots}. It can be seen that general health levels were much lower than in Sample One. This may perhaps be the result of more precise estimation due to a larger sample size, or potentially the impact of the online administration. Note that zeros in some of the totals were due to some respondents only completing a subset of questions. 


\subsection{Psychometric Analyses}

%% Alpha was calculated for the MAAS ($\alpha=0.88$) and for the LOT-R ($\alpha=0.78$). Alpha for all RAND subscales was above 0.7, with the exception of Social Functioning. 

%% \subsection{Number of Factors to retain}

The primary aim of this chapter was to develop psychometric models for the mindfulness and optimism measures that could be used to supplement the data collected as part of the experimental research. Each scale, across both samples was taken as one unit, and factor analytic, item response methods, and structural equation models were used to develop and test combinations of items which seemed to provide useful explanatory power. Additionally, the use of multiple methods here allows for the actual impact of the different models to be assessed against one another, and those built on different sub-samples, to ensure that these models are reproducible. 

This section covers the RAND-MOS data for Sample One (this was not repeated for Sample Two, as this instrument was not used in the experimental portion of the research). Next, the MAAS is investigated over both samples using all of these methods, followed by an examination of the LOT-R. The most successful models from this process were then tested against Sample One data in a back~testing process.


<<retainfactors, echo=FALSE, results=hide, cache=TRUE, eval=FALSE>>=
sink("tmp.txt")
rand.nscree <- nScree(x=cor(na.omit(randitems.scored), use="pairwise.complete.obs"), model="factors")
sink(NULL)
@

<<retainfactorsprint, echo=FALSE, results=hide, eval=FALSE>>=
print(xtable(summary(rand.nscree), label="tab:randretain", caption="Comparison of Criteria to retain factors"))
@ 



%% Another approach which can be applied to select the number of factors is a cross-validation method.

<<randfactorcv, echo=FALSE, results=hide, cache=TRUE, eval=FALSE>>=
rand.fact.gabriel <- cv.svd.gabriel(na.omit(randitems.scored), krow=2, kcol=2, maxrank=18)
rand.fact.wold <- cv.svd.wold(na.omit(randitems.scored), k=10, maxrank=12)
@ 

<<randcvwold, echo=FALSE, results=hide, eval=FALSE>>=
print(xtable(Svdcv(rand.fact.wold, label="tab:randfactwold", caption="Estimate of number of factors to retain based on 10-fold Cross Validation")))
@ 

%% It can be seen from Table \ref{tab:randfactwold} that the Wold method of cross validation suggests that four factors should be retained for further analysis. This seems in line with the results of the other criteria.



<<randcvgabriel, echo=FALSE, results=hide, eval=FALSE>>=
print(Svdcv(rand.fact.gabriel, label="tab:randfactgabriel", caption="Estimate of number of factors to retain based on leave out 2 item/subject Cross-validation"))
@

%% It can be seen from Table \ref{tab:randfactgabriel} that the Gabriel method of cross-validation suggests that there are thirteen factors which underlie this scale. This seems relatively implausible, but it will be tested. 



%% Next, the various metrics for determining the number of factors were applied to the Mindfulness scale. 

<<retainmaas, echo=FALSE, results=hide, cache=TRUE, eval=FALSE>>=
sink("tmp.txt")
maas.nscree <- nScree(x=cor(na.omit(maasitems), use="pairwise.complete.obs"), model="factors")
sink(NULL)
@ 


<<printretainmaas, echo=FALSE, results=hide, eval=FALSE>>=
print(xtable(summary(maas.nscree), label="tab:maasretain", caption="Comparison of Criteria to retain factors, MAAS"))
@ 

%% Looking at the eigenvalues, it is clear that one factor explains the majority of the variance. However, both the Kaiser criterion and parallel analysis suggest that three factors should be retained, while the VSS and the Minimum Average Partial criterion procedures suggest a one factor solution.

<<maasfactorcv, echo=FALSE, results=hide, eval=FALSE>>=
maasitems <- as.data.frame(lapply(maasitems, as.numeric))
maas.fact.gabriel <- cv.svd.gabriel(na.omit(maasitems), krow=2, kcol=2, maxrank=7)
maas.fact.wold <- cv.svd.wold(na.omit(maasitems), k=10, maxrank=7)
@


<<maascvwold, echo=FALSE, results=hide, eval=FALSE>>=
print(Svdcv(maas.fact.wold, label="tab:maasfactwold", caption="Estimate of number of factors to retain based on 10-fold Cross Validation, MAAS Sample One"))
@ 

%% The Wold method of cross-validation, (shown in Table \ref{tab:maasfactwold}), agrees with the other criteria in suggesting that one factor should be retained. 


<<maascvgabriel, echo=FALSE, results=hide, eval=FALSE>>=
print(Svdcv(maas.fact.gabriel, label="tab:maasfactgabriel", caption="Estimate of number of factors to retain based on leave out 2 item/subject Cross-validation, MAAS Sample One"))
@ 

%% It can be seen from Table \ref{tab:maasfactgabriel} that the Wold method of cross-validation suggests that there are seven factors which underlie this scale. This seems relatively implausible, but it will be tested. 


%% One and three  factor solutions were examined and their performance assessed on unseen data. 


<<retainlotr, echo=FALSE, results=hide, cache=TRUE, eval=FALSE>>=
lotr.nscree <- nScree(x=cor(na.omit(lotritems), use="pairwise.complete.obs"), model="factors")
@ 


<<lotrprint, echo=FALSE, results=hide, eval=FALSE>>=
print(xtable(summary(lotr.nscree), label="tab:lotrretain", caption="Comparison of Criteria to retain factors, LOTR, Sample One"))

@ 

%% In the case of the LOT-R, all of the methods for determining the correct number of factors to retain suggest that a one factor solution fits the data matrix best. 

<<lotrfactorcv, echo=FALSE, results=hide, cache=TRUE>>=
lotritems <- as.data.frame(lapply(lotritems, as.numeric))
lotr.fact.gabriel <- cv.svd.gabriel(na.omit(lotritems), krow=3, kcol=3, maxrank=3)
lotr.fact.wold <- cv.svd.wold(na.omit(lotritems), k=10, maxrank=3)
@


<<lotrcvwold, echo=FALSE, results=hide,eval=FALSE>>=
print(Svdcv(lotr.fact.wold, label="tab:lotrfactwold", caption="Estimate of number of factors to retain based on 10-fold Cross Validation"))
@ 

%% The Wold method of cross-validation, (shown in Table \ref{tab:lotrfactwold}), agrees with the other criteria in suggesting that one factor should be retained. 


<<lotrcvgabriel, echo=FALSE, results=hide, eval=FALSE>>=
print(Svdcv(lotr.fact.gabriel, label="tab:lotrfactgabriel", caption="Estimate of number of factors to retain based on leave out 2 item/subject Cross-validation"))
@ 

%% It can be seen from Table \ref{tab:lotrfactgabriel} that the Gabriel method of cross-validation suggests that there are three factors which underlie this scale. This seems relatively implausible, but it will be tested. 

%% Despite all of the selection criteria pointing towards a one factor solution, one and two factor solutions will be assessed for the LOT-R, and their performance evaluated on unseen data. 

<<mapall, echo=FALSE, results=hide, cache=TRUE, eval=FALSE>>=
MAP.rand<-VSS(na.omit(randitems.scored), n=12, rotate="oblimin", fm="ml", plot=FALSE )
MAP.maas<-VSS(na.omit(maasitems),rotate="oblimin", fm="ml", plot=FALSE )
MAP.lotr<-VSS(na.omit(lotritems),rotate="promax", fm="gls", plot=FALSE )
@

%As can be seen from Figure ~\ref{fig:rand1}, the 8 factor structure was replicated.  However, the MAP criterion suggests a four factor solution, so both of these proposed solutions were examined and tested.




\subsection{RAND MOS}
\label{sec:rand-mos-1}

The parallel analysis criterion and the Kaiser criterion suggested eight factors. The acceleration factor and the optimal calibration index measures suggested two factors. These two measures may be picking up on the higher order factor structure of the items, as the RAND is typically modelled as having two higher order factors (physical and mental health).

Two and eight  factor structures were examined for the RAND MOS and their performance assessed on unseen data (from sample 2) to determine which of these provides the best fit. 



<<rand2fact, echo=FALSE, results=hide>>=
rand.fact.2<-fa(na.omit(randitems.scored), 2,fm="ml", rotate="promax")
print(FactorXtab(rand.fact.2, names=c("PhysEmHealth", "PhysicalFunc"),label="tab:rand2fact", caption="Factor Loadings, RAND MOS Two Factor Solution, Sample One"))
@
The results of the two factor solution (Appendix~\ref{cha:append-1:-supp}, Table~\ref{tab:rand2fact}, Page \pageref{tab:rand2fact}) were as follows:
The first factor appears to contain all of the scales except for Physical Functioning, which loads on Factor 2. This factor can be best termed as General and Emotional Health.

The second factor maps exactly to the Physical Functioning Scale, and so retains that name. The non-normed fit index was equal to \Sexpr{round(rand.fact.2[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(rand.fact.2[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(rand.fact.2[["RMSEA"]][2],3 )} to \Sexpr{round(rand.fact.2[["RMSEA"]][3],3 )}.

This factor solution does not appear to be useful, as it has extremely low fit indices, and the breakdown of the factors is rather strange. If the factors had broken down in terms of Physical and Mental Health, then this would have made more sense. The factor loadings were invariant under a number of rotations (varimax, oblimin and promax), so it appears to be a real (if less than interpretable) factor structure. 

<<rand2corr, echo=FALSE, results=hide, eval=FALSE>>=
print(xtable(rand.fact.2$r.scores, label="tab:hom1rand4corr", caption="Factor Correlations, RAND MOS Two Factor Solution, Sample One"))
@

The factor correlations were quite low for this solution, at 0.27. This suggests that an orthogonal rotation might be more appropriate, but attempting this did not change any of the loadings. It seems that this factor structure is not theoretically or conceptually useful, which is an important feature of any proposed psychometric model. 



%% \paragraph{Rand MOS 4 Factor Solution}

<<rand4fact, echo=FALSE, results=hide>>=

rand.fact.4<-factor.pa(na.omit(randitems.scored), 4, rotate="oblimin")
print(FactorXtab(rand.fact.4, names=c("PhysFunc", "SocEmFunc", "PhysLim", "GenHealth"), label="tab:rand4fact", caption="Four Factor Solution, RAND MOS, Sample One (Oblimin Rotation)"))
@

%% The loadings on the four factor solution (not shown) broke down as follows. 
%% PA2: "RANDQ3" , "RANDQ4" , "RANDQ5" , "RANDQ6" , "RANDQ7" , "RANDQ8" , "RANDQ9" , "RANDQ10" ,"RANDQ11" ,"RANDQ12". This factor maps exactly to the Physical Functioning scale, and so retains that name. 

%% PA1: "RANDQ17" ,"RANDQ18" ,"RANDQ19" ,"RANDQ20" ,"RANDQ23" ,"RANDQ24" ,"RANDQ25" ,"RANDQ26" ,"RANDQ27" ,"RANDQ28" ,"RANDQ29" ,"RANDQ30" ,"RANDQ31" ,"RANDQ32". This factor maps to the emotional role limitations, social functioning and emotional well being scales, and so can probably best be termed as Social and Emotional Functioning. 

%% PA3: "RANDQ13" ,"RANDQ14" ,"RANDQ15" ,"RANDQ16" ,"RANDQ21" ,"RANDQ22". This factor maps to the role limitations and pain sub-scales, and so can probably best be termed as physical limitations. 

%% PA4: "RANDQ1" , "RANDQ3" , "RANDQ21" ,"RANDQ27" ,"RANDQ33" ,"RANDQ34" ,"RANDQ35" ,"RANDQ36". This scale maps to the General Health scale, with one item from the Physical Functioning and one item from Energy/Fatigue (27). However, both these items have higher loadings on other factors, and so this factor can probably best be termed as General Health. 



%% The non-normed fit index was equal to \Sexpr{round(rand.fact.4[["TLI"]], 3)}
%% and the RMSEA was equal to \Sexpr{round(rand.fact.4[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(rand.fact.4[["RMSEA"]][2],3 )} to \Sexpr{round(rand.fact.4[["RMSEA"]][3],3 )}.

%% The fit indices are somewhat better for this solution than for the two factor solution, though the NNFI is still quite low. The RMSEA is somewhat too high for comfort, also. 

<<rand4corr, echo=FALSE, results=hide, eval=FALSE>>=
print(xtable(rand.fact.4[["r.scores"]], label="tab:hom1rand4corr", caption="Factor Correlations, RAND MOS Four Factor Solution, Sample One"))
@

%% The factor correlations are quite low in this solution also, though they are all high enough to retain the oblique rotations.





<<rand8fact, echo=FALSE, results=tex>>=
rand.fact.8<-fa(na.omit(randitems.scored), 8, rotate="oblimin", fm="pa")
questions.rand <- paste(rand, 1:36, sep="")

rand8.xtab <- FactorXtab(rand.fact.8, names=c( "PhysFunc", "SocEmWB", "GenHealth", "EmRoleLim", "RoleLim", "Fatigue","Pain", "Energy"), label="tab:tcq1rand8fact", caption="Factor Loadings Eight Factor Solution, RAND MOS, Sample One")
rownames(rand8.xtab) <- questions.rand
print(rand8.xtab, scalebox=0.7, digits=2, sanitize.text.function= function (x) x, table.placement="tp")
@

\newpage

Table~\ref{tab:tcq1rand8fact} on Page~\pageref{tab:tcq1rand8fact} shows the loadings and has the named factors for the eight factor solution. 

PhysFun:  the first factor extracted maps exactly to the Physical Functioning scale, and retains that name. 

SocEmWB: This scale maps to the Social Functioning, and the Emotional Well Being Scale. There are some items taken from the Energy/Fatigue scale, and as these are the positively worded items, this scale can probably best be termed as Social and Emotional Well Being. 

GenHealth: These items map exactly to the General Health scale. Item 35 also loads on this scale, and as its loading was 0.29 while the cutoff was 0.30, it can be best characterised as part of that scale. Therefore, this factor can be best termed as General Health.

EmRoleLim: This scale maps to the Emotional Role Limitations and one item (20) from the Social Functioning scale (which asks about social events that have been missed due to health problems) and so this can probably best be termed as Emotional Role Limitations.

RoleLim:  This scale maps exactly to the Role Limitations sub-scale, and so retains that name. 

Fatigue: These items are the negative items from the Energy/Fatigue scale, and so this factor can probably best be termed as Fatigue. 

Pain: These items map exactly to the Pain scale, and so retain that name. 

Energy: The loadings on this factor are all below .4, which argues against its unproblematic interpretation. In addition, Q2 loads on this factor, when it is not typically associated with any factor. The other two items are the positively worded items from the  Energy/Fatigue scales, and so this factor can best be termed Energy. 

The non-normed fit index was equal to \Sexpr{round(rand.fact.8[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(rand.fact.8[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(rand.fact.8[["RMSEA"]][2],3 )} to \Sexpr{round(rand.fact.8[["RMSEA"]][3],3 )}.

This factor structure definitely makes sense, and the fit indices are acceptable, although the RMSEA is a little higher than would be wanted. %% In addition, the items map quite well to the subscales, which further reinforces our confidence in this solution.

This factor structure seems appropriate for the data, and matches the number of factors proposed for this instrument. However, the interesting part is how it is different from the published sub-scales. To start, Social Functioning and Emotional Well Being have been merged. This makes sense, as the Social Functioning scale is quite small, and there may not have been enough data available to precisely segment them. However, it appears that Energy and Fatigue are perceived differently by this sub-population. The discrepancies here may have arisen from the age of the sample - while the instrument was developed on a sample from the general population, the sample here was predominantly young, and such differences from the scale could represent this fact. Additionally, the sample was that of university students, which may also have affected the responses. 






<<rand8corr, echo=FALSE, results=tex>>=
rand8.cor <- rand.fact.8[["r.scores"]]
rand8.cor[lower.tri(rand8.cor, diag=FALSE)] <- NA
rand8corr.xtab <- xtable(rand8.cor, label="tab:hom1rand8corr", caption="Factor Correlations, Eight Factor Solution RAND MOS, Sample One")
rand8names <- c( "PhysFunc", "SocEmWB", "GenHealth", "EmRoleLim", "RoleLim", "Fatigue","Pain", "Energy")
names(rand8corr.xtab) <- rand8names
rownames(rand8corr.xtab) <- rand8names
print(rand8corr.xtab, scalebox=0.7)
@

The factor correlations are shown in Table~\ref{tab:hom1rand8corr} and were moderate ($r=0.1-0.5$), and in line with expectations. 

<<randhigherord, echo=FALSE, results=hide, eval=FALSE>>=
scales.hom <- hom[,66:73]
scales2 <- scales[,2:9]
rand.higher.fa
@ 


%% \paragraph{CFA for RAND MOS}


<<rand2sem, echo=FALSE, results=hide, cache=TRUE>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "General and Emotional Health")
physfun <- paste(rand, c(3:12), sep="")
genemhealth <- paste(rand,c(1,14,17:36), sep="")
Rand2model <- mxModel(name="RAND2Samp1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="General and Emotional Health", to=genemhealth),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=364)
                      )
rand2fit <- mxRun(Rand2model)
rand2summ <- summary(rand2fit)
@

<<rand4sem, echo=FALSE, results=hide, cache=TRUE>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "Social and Emotional Functioning", "General Health", "Physical Limitations")
physfun <- paste(rand, c(3:12), sep="")
genhealth <- paste(rand, c(1,33:36), sep="")
physlim <- paste(rand, c(13:16,21:22), sep="")
emsocfunc <- paste(rand, c(17:20, 23:32), sep="")
Rand4model <- mxModel(name="RAND4Samp1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="Social and Emotional Functioning", to=emsocfunc),
                      mxPath(from="General Health", to=genhealth),
                      mxPath(from="Physical Limitations", to=physlim),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=364)
                      )
rand4fit <- mxRun(Rand4model)
rand4summ <- summary(rand4fit)
@

<<rand8sem, echo=FALSE, results=hide, cache=TRUE>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "Social and Emotional Wellbeing", "General Health", "Emotional Role Limitations", "Role Limitations", "Fatigue", "Pain", "Energy")
physfun <- paste(rand, c(3:12), sep="")
socemwellbeing <- paste(rand, c(20,23:28, 30, 32), sep="")
genhealth <- paste(rand, c(1,33:36), sep="")
emrolelim <- paste(rand, c(17:20), sep="")
rolelim <- paste(rand, c(13:16), sep="")
energy <- paste(rand, c(29, 31), sep="")
pain <- paste(rand, c(21:22), sep="")
fatigue <- paste(rand, c(2, 23, 27), sep="")
Rand8model <- mxModel(name="RAND8Samp1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="Social and Emotional Wellbeing", to=socemwellbeing),
                      mxPath(from="General Health", to=genhealth),
                      mxPath(from="Emotional Role Limitations", to=emrolelim),
                      mxPath(from="Role Limitations", to=rolelim),
                      mxPath(from="Energy", to=energy),
                      mxPath(from="Pain", to=pain),
                      mxPath(from="Fatigue", to=fatigue),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=364)
                      )
rand8fit <- mxRun(Rand8model)
rand8summ <- summary(rand8fit)
@



<<randsemcompare, echo=FALSE, results=tex>>=
randsemcomp <- mxCompare(base=rand2fit, comparison=c(rand8fit), all=TRUE)
print(xtable(randsemcomp,label="tab:randsemcompare", caption="SEM Comparison for RAND MOS Factor Solutions, Sample One"), scalebox=0.8, table.placement="ht", include.rownames=FALSE)
@

As can be seen from Table~\ref{tab:randsemcompare}, the 8 factor solution appears to fit better (lower AIC), so on the basis of this analysis, this is the solution which should be retained.



%% \subsubsection{Confirmatary Factor Analysis, Second Sample}
%% \label{sec:rand-mos-cfa}



<<rand2semhom2, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "General and Emotional Health")
physfun <- paste(rand, c(3:12), sep="")
genemhealth <- paste(rand,c(1,14,17:36), sep="")
Rand2model2 <- mxModel(name="RAND2Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="General and Emotional Health", to=genemhealth),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                       mxData(observed=cov(na.omit(randitems2)), type="cov", numObs=281)
                      )
rand2fit2 <- mxRun(Rand2model2)
rand2summ2 <- summary(rand2fit2)
@

<<rand4semhom2, echo=FALSE, results=hide, cache=TRUE>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "Social and Emotional Functioning", "General Health", "Physical Limitations")
physfun <- paste(rand, c(3:12), sep="")
genhealth <- paste(rand, c(1,33:36), sep="")
physlim <- paste(rand, c(13:16,21:22), sep="")
emsocfunc <- paste(rand, c(17:20, 23:32), sep="")
Rand4model2 <- mxModel(name="RAND4Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="Social and Emotional Functioning", to=emsocfunc),
                      mxPath(from="General Health", to=genhealth),
                      mxPath(from="Physical Limitations", to=physlim),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=281)
                      )
rand4fit2 <- mxRun(Rand4model2)
rand4summ2 <- summary(rand4fit2)
@ 


<<rand8semhom2, echo=FALSE, results=hide, cache=TRUE>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "Social and Emotional Wellbeing", "General Health", "Emotional Role Limitations", "Role Limitations", "Fatigue", "Pain", "Energy")
physfun <- paste(rand, c(3:12), sep="")
socemwellbeing <- paste(rand, c(20,23:28, 30, 32), sep="")
genhealth <- paste(rand, c(1,33:36), sep="")
emrolelim <- paste(rand, c(17:20), sep="")
rolelim <- paste(rand, c(13:16), sep="")
energy <- paste(rand, c(29, 31), sep="")
pain <- paste(rand, c(21:22), sep="")
fatigue <- paste(rand, c(2, 23, 27), sep="")
Rand8model2 <- mxModel(name="RAND8Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="Social and Emotional Wellbeing", to=socemwellbeing),
                      mxPath(from="General Health", to=genhealth),
                      mxPath(from="Emotional Role Limitations", to=emrolelim),
                      mxPath(from="Role Limitations", to=rolelim),
                      mxPath(from="Energy", to=energy),
                      mxPath(from="Pain", to=pain),
                      mxPath(from="Fatigue", to=fatigue),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=281))
                      
rand8fit2 <- mxRun(Rand8model2)
rand8summ2 <- summary(rand8fit2)
@



<<randsemcomparehom2, echo=FALSE, results=tex>>=
randsemcomp.hom2 <- mxCompare(base=rand2fit2, comparison=c(rand8fit2))
print(xtable(randsemcomp.hom2,label="tab:randsemcomparehom2", caption="Model Comparison for RAND MOS data using Models from Sample 1 on Sample 2" ), scalebox=0.8, include.rownames=FALSE)
@

As Table \ref{tab:randsemcomparehom2} shows, the eight factor model provided the best fit to this unseen data (Split A of the second Sample), which is  similar to the data on which the model was created. This supports the hypothesis proposed in the introduction, and is in line with previous research. 



\section{Mindfulness Attention Awareness Scale}

The next step in the analyses was to examine the MAAS using factor analytic and IRT methods. 

<<maasitems, echo=FALSE, results=tex>>=
maasquestions <- read.csv("maasitems.csv")
names(maasquestions)[2] <- "Content"
print(xtable(maasquestions, label="tab:maasitems", caption="MAAS Item Content"), include.rownames=FALSE, scalebox=0.68)
@ 

Table~\ref{tab:maasitems} shows the item content for the MAAS. 

\subsection{Factor Analyses, Sample One}
\label{sec:fact-analys-sample}



For the MAAS, parallel analysis, the MAP, VSS and Kaisers rule methods suggested a one factor solution, while the acceleration factor and optimal coordinates index methods suggested a three factor solution. Therefore, one and three factor solutions were extracted and the results interpreted, as shown below.

%% \subsection{MAAS One Factor Solution}
%% \label{sec:maas-one-factor}

<<maas1fact, echo=FALSE, results=tex>>=
maas.fact.1<-factor.pa(na.omit(maasitems), 1, rotate="oblimin")
maas.questions <- paste(maas, 1:15, sep="")
maas1.xtab <- FactorXtab(maas.fact.1, names=c("Mindfulness"),label="tab:hom1maas1fact", caption="Factor Loadings, One Factor Solution, MAAS, Sample One")
rownames(maas1.xtab) <- maas.questions
print(maas1.xtab, sanitize.text.function=function (x) x)
@

The results of the one factor solution for the MAAS are shown in Table~\ref{tab:hom1maas1fact}. 

The non-normed fit index was equal to \Sexpr{round(maas.fact.1[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(maas.fact.1[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(maas.fact.1[["RMSEA"]][2],3 )} to \Sexpr{round(maas.fact.1[["RMSEA"]][3],3)}.




The next factor solution to be examined was the three factor solution (loadings shown in Chapter~\ref{cha:append-1:-supp}, Table~\ref{tab:tcq1maas3fact},  Page~\pageref{tab:tcq1maas3fact})


<<maas7fact, echo=FALSE, results=hide>>=
maas.fact.3<-factor.pa(na.omit(maasitems), 3, rotate="oblimin")
print(FactorXtab(maas.fact.3, names=c("LackAware", "PhysUnaware", "LackPresAttention"), label="tab:tcq1maas3fact", caption="Factor Loadings, Three Factor Solution, MAAS, Sample One"), sanitize.text.function= function (x) x)
@


PA1: ``Q8''  ``Q10'' ``Q11'' ``Q12'' ``Q13'' ``Q14'' ``Q15''
All of these questions relate to lack of awareness, and so this factor can best be termed this. 

PA3: ``Q4'' ``Q5'' ``Q6'' ``Q7'' ``Q8'' ``Q9''
These mostly relate to sensations of physical unawareness, and so this factor can best be termed physical unawareness. 

PA2: ``Q1''  ``Q2''  ``Q3''  ``Q14''. This factor can perhaps best be termed as lack of present attention.

While the amount of variance explained increased with the number of factors, this model does not seem to be particularly useful, in that it does not shed new light on the construct. 



The non-normed fit index was equal to \Sexpr{round(maas.fact.3[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(maas.fact.3[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(maas.fact.3[["RMSEA"]][2],3 )} to \Sexpr{round(maas.fact.3[["RMSEA"]][3],3)}.




%% \subsection{CFA for MAAS}
%% \label{sec:cfa-maas}






<<maas1fit, echo=FALSE, results=hide, cache=TRUE>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Unawareness")
unawareness <- paste(maas, c(1:15), sep="")
Maas1model <- mxModel(name="MAAS1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Unawareness", to=unawareness),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems)), type="cov", numObs=364)
                      )
maas1fit <- mxRun(Maas1model)
maas1summ <- summary(maas1fit)
@

<<maas3sem, echo=FALSE, results=hide, cache=TRUE>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Lack Present Awareness", "Lack of Awareness", "Physical Unawareness")
lackaware <- paste(maas, c(8, 10:15), sep="")
lackpresaware <- paste0(maas, c(4:9))
physunawareness <- paste0(maas, c(1:3, 14))

Maas3model <- mxModel(name="MAAS3",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Lack Present Awareness"
                             ,to=lackpresaware),
                      mxPath(from="Lack of Awareness", to=lackaware),
                      mxPath(from="Physical Unawareness", to=physunawareness),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems)), type="cov", numObs=364)
                      )
maas3fit <- mxRun(Maas3model)
maas3summ <- summary(maas3fit)
@

<<maassemcompare, echo=FALSE, results=tex>>=
maascomp <- mxCompare(base=maas1fit, comparison= maas3fit)
maascomp.xtab <- xtable(maascomp,label="tab:maassemcomp", caption="Comparison of One and Three Factor Models, Sample One")
print(maascomp.xtab, scalebox=0.9, include.rownames=FALSE)
                      
@

Factor solutions for one and three factors were extracted, and the results were subjected to CFA.

From Table~\ref{tab:maassemcomp} it can be seen that the best model is the one factor model, which is in line with previous research.
The factor structure is not reported here as all factors loaded on the first factor.  This factor explained 35\% of the variance
in the sample, which is low.


<<splitsample, echo=FALSE, results=hide, cache=TRUE>>=
set.seed(17)
maassamp <- sample(1:1109, 1109)
ms1 <- maassamp[1:370]
ms2 <- maassamp[371:740]
ms3 <- maassamp[741:length(maassamp)]
maasitems2a <- maasitems2[ms1,]
maasitems2b <- maasitems2[ms2,]
maasitems2c <- maasitems2[ms3,]
maasitems.nota <- maasitems2[c(ms2, ms3),]
maasitems.notb <- maasitems2[c(ms1, ms3),]
maasitems.notc <- maasitems2[c(ms1, ms2),]
lotrsamp <- sample(1:1109, 1109)
lr1 <- lotrsamp[1:370]
lr2 <- lotrsamp[371:740]
lr3 <- lotrsamp[741:length(lotrsamp)]
lotritems2a <- lotritems2[lr1,]
lotritems2b <- lotritems2[lr2,]
lotritems2c <- lotritems2[lr3,]
lotritems.nota <- lotritems2[c(lr2, lr3),]
lotritems.notb <- lotritems2[c(lr1, lr3),]
lotritems.notc <- lotritems2[c(lr1, lr2),]
randsamp <- sample(1:1109, 1109)
r1 <- randsamp[1:370]
r2 <- randsamp[371:740]
r3 <- randsamp[741:length(randsamp)]
randitems2a <- randitems[r1,]
randitems2b <- randitems[r2,]
randitems2c <- randitems[r3,]
randitems.nota <- randitems[c(r2, r3),]
randitems.notb <- randitems[c(r1, r3),]
randitems.notc <- randitems[c(r1, r2),]
scales <- hom2[,c(4,66:75)]
scales.samp <- sample(1:1109, 1109)
s1.samp <- scales.samp[1:370]
s2.samp <- scales.samp[371:740]
s3.samp <- scales.samp[741:length(scales.samp)]
scales2a <- scales[s1.samp,]
scales2b <- scales[s2.samp,]
scales2c <- scales[s3.samp,]
@

<<maas7sem2, echo=FALSE, results=hide, cache=TRUE>>=
maas3model2 <- mxModel(Maas3model, name="MAAS3Fit2", mxData(observed=cov(na.omit(maasitems2a)), type="cov", numObs=313))
maas3fit2 <- mxRun(maas3model2)
maas3fitsum <- summary(maas3fit2)
@


<<maas1sem2, echo=FALSE, results=hide, cache=TRUE>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Unawareness")
unawareness <- paste(maas, c(1:15), sep="")
Maas1model2 <- mxModel(name="MAAS1Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Unawareness", to=unawareness),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2a)), type="cov", numObs=313)
                      )
maas1fit2 <- mxRun(Maas1model2)
maas1summ2 <- summary(maas1fit2)
@




<<maassemcompare2, echo=FALSE, results=tex>>=
maascomp2 <- mxCompare(base=maas1fit2, comparison=maas3fit2)
maascomp.xtab2 <- xtable(maascomp2,label="tab:maassemcompare2", caption="Comparison of Sample One MAAS Factor Models on a subset of Sample Two Data (Split A)")
print(maascomp.xtab2, scalebox=0.9, include.rownames=FALSE)
@

Table \ref{tab:maassemcompare2} demonstrates that the MAAS 1 factor model provided the best fit to the subsample of data ($N=313$) used to test the model.

\subsection{Factor Analyses, Sample Two}
\label{sec:fact-maas}

<<maas2bparallel, echo=FALSE, results=hide, cache=TRUE, eval=FALSE>>=
maas2b.parallel <- fa.parallel(na.omit(maasitems2b))
vss.maas.2b <- VSS(na.omit(maasitems2b))
@

The parallel analysis procedure for Split B suggested that this sample of the responses to the MAAS has five factors, while the MAP criterion suggests that it has only one.  Following our previous approach, each of these factor solutions will be examined and interpreted before a CFA is applied on the remainder of the dataset.

For split C, the same procedure was carried out, and the various methods both suggested five and one factors, respectively. 

<<maas2cparallel, echo=FALSE, results=hide, cache=TRUE, eval=FALSE>>=
maas2c.parallel <- fa.parallel(na.omit(maasitems2c))
vss.maas.2c <- VSS(na.omit(maasitems2c))
@



<<maas2b1, echo=FALSE, results=hide>>=
maas2b.fact1 <- fa(maasitems2b, 1, rotate="oblimin", fm="pa")
print(FactorXtab(maas2b.fact1, names=c("Mindfulness"),label="tab:maas2bfact1", caption="MAAS One Factor Solution, Sample 2B"), sanitize.text.function= function (x) x)
@

%% The solution shown above in Table \ref{tab:maas2bfact1} shows adequate loadings of all the questions on a first factor which can be named mindfulness.  This solution explained 41\% of the variance, which is quite low for a factor solution.

<<maas2c1, echo=FALSE, results=hide>>=
maas2c.fact1 <- fa(maasitems2c, 1, rotate="oblimin", fm="pa")
print(FactorXtab(maas2c.fact1, names=c("Mindfulness"), label="tab:maas2cfact1", caption="MAAS One Factor Solution, Sample 2C"), sanitize.text.function= function (x) x)
@

<<maasfact1average, echo=FALSE, results=tex>>=
maas2c.fact1.n <- FactorNames(maas2c.fact1, names=c("Mindfulness"))
maas2b.fact1.n <- FactorNames(maas2b.fact1, names=c("Mindfulness"))
maas.fact1.av <- FactorAverage(sols=list(maas2b.fact1.n, maas2c.fact1.n), mynames=c("Mindfulness", "Communalities"))
rownames(maas.fact1.av) <- maas.questions
print(xtable(maas.fact1.av, label="tab:maasfact1average", caption="One Factor Solution Averaged over Splits B and C, MAAS"), sanitize.text.function= function (x) x)
@ 


Table \ref{tab:maasfact1average} shows the one factor loadings averaged over Splits B and C. The solution only explained 36\% of the variance in the item loadings, which is in line with the first sample, though much lower than the original published research in which the MAAS was developed. 



<<maas2bfact5, echo=FALSE, results=hide>>=
maas2b.fact5 <- fa(maasitems2b, 5, rotate="oblimin", fm="pa")
print(FactorXtab(maas2b.fact5, names=c("Distractability", "LackPresAware", "LackSomaticAware", "Inattention", "LackPresFocus"),  label="tab:tcq2bmaasfact5", caption="Factor Loadings, MAAS Five Factor Solution, Sample Two, Split B"), scalebox=0.8, sanitize.text.function= function (x) x)
@
This five factor solution for Split B (Appendix~\ref{cha:append-1:-supp}, Table~\ref{tab:tcq2bmaasfact5}, page \pageref{tab:tcq2bmaasfact5}) explained 54\% of the variance in the sample.

PA1: ``Q5'',  ``Q6'',  ``Q7'',  ``Q8'',  ``Q9'',  ``Q10''.  This factor has come through in most of the previous solutions, and can again be termed distractability.

PA2: ``Q1'', ``Q2'', ``Q3''.  Again, these items have clustered together previously, and this factor is again termed lack of present awareness.

PA3: ``Q4'', ``Q5''.  This factor is again termed lack of somatic awareness.

PA4: ``Q13'', ``Q14''.  This factor can best be termed as lack of attention.

PA5: ``Q10'', ``Q11'', ``Q12'', ``Q14'', ``Q15''.  This factor again can be termed distractability.

Although there is significantly more variance explained by this solution, again it does not lend any more conceptual clarity to the instrument, suggesting that it is not worth the extra parameters estimated. Note that Q1-3 clustered together in previous multi-factor solutions, so there may be a different construct underlying these questions, but this is not clearly replicable across the entire sample. 

<<maas2cfact5, echo=FALSE, results=hide>>=
maas2c.fact5 <- fa(maasitems2c, 5, rotate="oblimin", fm="pa")
print(FactorXtab(maas2c.fact5, names=c("LackPresFocus", "LackPresAware", "LackAtt", "LackSomAware", "LackAware"), label="tab:hom2cmaasfact5", caption="Factor Loadings, MAAS Five Factor Solution, Sample Two, Split C"), scalebox=0.8, sanitize.text.function= function (x) x)
@
The five factor solution for split C (Appendix~\ref{cha:append-1:-supp}, Table~\ref{tab:hom2cmaasfact5}, Page~\pageref{tab:hom2cmaasfact5}) broke down as follows:

PA1: "Q7"  "Q8"  "Q9"  "Q10" "Q11" "Q12" "Q14"
All of the items in this factor relate to a lack of attention to the present, and it can probably be best termed lack of present focus. 

PA2: "Q9"  "Q13"
Q9 loads on both PA1 and PA2, and as PA2 has really only Q13 loading to any major extent on it, no interpretation of this factor was performed. It was named lack of present awareness. 

PA4: "Q2"  "Q3"  "Q14"
This factor can probably best be termed lack of attention. 

PA5: "Q1" "Q4" "Q5"
Most of these items relate to lack of bodily attention, and this can probably best be termed lack of somatic awareness. 

PA3: "Q4" "Q6" "Q7" "Q8"
Note that MAASQ4 loads slightly on both PA3 and PA5, and is not considered in the interpretation here. These items can probably best be termed lack of awareness. 




<<maas2bsemon2c, echo=FALSE, results=hide, cache=TRUE>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Unawareness")
unawareness <- paste(maas, c(1:15), sep="")
Maas1model2b <- mxModel(name="MAAS12b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Unawareness", to=unawareness),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2c)), type="cov", numObs=370)
                      )
maas1fit2b <- mxRun(Maas1model2b)
maas1summ2b <- summary(maas1fit2b)
@ 


<<maas2csemon2b, echo=FALSE, results=hide, cache=TRUE>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Unawareness")
unawareness <- paste(maas, c(1:15), sep="")
Maas1model2c <- mxModel(name="MAAS12c",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Unawareness", to=unawareness),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2b)), type="cov", numObs=370)
                      )
maas1fit2c <- mxRun(Maas1model2c)
maas1summ2c <- summary(maas1fit2c)
@ 




<<maas5sem2b, echo=FALSE, results=hide, cache=TRUE>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Lack of Bodily Awareness", "Distractability", "Lack of present focus", "Lack of somatic awareness", "Lack of attention")
bodyunaware <- paste(maas, c(6:10), sep="")
distractability <- paste(maas, c(11:15), sep="")
lackpresfocus <- paste(maas, c(1:3), sep="")
lacksomaware <- paste(maas, c(4,5), sep="")
lackattention <- paste(maas, c(13,14), sep="")
Maas5model2b <- mxModel(name="MAAS52b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Lack of Bodily Awareness", to=bodyunaware),
                        mxPath(from="Distractability", to=distractability),
                        mxPath(from="Lack of present focus", to=lackpresfocus),
                        mxPath(from="Lack of somatic awareness", to=lacksomaware),
                        mxPath(from="Lack of attention", to=lackattention),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2c)), type="cov", numObs=370)
                      )
maas5fit2b <- mxRun(Maas5model2b)
maas5summ2b <- summary(maas5fit2b)
@

<<maas5sem2c, echo=FALSE, results=hide, cache=TRUE, cache=TRUE>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("lack of present focus", "lack of present awareness", "lack of attention", "Lack of somatic awareness", "lack of awareness")
bodyunaware <- paste(maas, c(1, 4, 5), sep="")
lackaware <- paste(maas, c(6:8), sep="")
lackpresfocus <- paste(maas, c(7:12, 14), sep="")
lacksomaware <- paste(maas, c(4,5), sep="")
lackattention <- paste(maas, c(2, 3, 14), sep="")
Maas5model2c <- mxModel(name="MAAS52c",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Lack of somatic awareness", to=bodyunaware),
                        mxPath(from="lack of present awareness", to=lackpresfocus),
                        mxPath (from="lack of attention", to=lackattention),
                        mxPath (from="lack of awareness", to=lackaware),
                        mxPath (from="lack of present focus", to=lackpresfocus),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2b)), type="cov", numObs=370)
                      )
maas5fit2c <- mxRun(Maas5model2c)
maas5summ2c <- summary(maas5fit2c)
@

<<maas2bsemcompare, echo=FALSE, results=tex>>=
maas2b.semcomp <- mxCompare(base=maas1fit2b, comparison=c(maas5fit2b))
print(xtable(maas2b.semcomp,label="tab:maas2bsemcompare", caption="Comparison of Factor Structures for MAAS 2B Solutions, Tested on Split B"), scalebox=0.9, include.rownames=FALSE)
@

As can be seen from Table \ref{tab:maas2bsemcompare}, the one factor solution again performs best, further increasing our confidence in its adequacy.


<<maas2csemcompare, echo=FALSE, results=tex>>=
maas2c.semcomp <- mxCompare(base=maas1fit2c, comparison=c(maas5fit2c))
print(xtable(maas2c.semcomp,label="tab:maas2csemcompare", caption="Comparison of Factor Structures for MAAS 2C Solutions, Tested on Split C"), scalebox=0.9, include.rownames=FALSE)
@

As can be seen from Table \ref{tab:maas2csemcompare}, the one factor model was again superior to the five factor model in Split C. 


\subsection{IRT Analyses - MAAS}
\label{sec:irt-analyses-maas}

\subsubsection{Sample One}
\label{sec:sample-one}



The MAAS was then considered from a IRT perspective.  Firstly, the instrument was examined using Mokken analysis to check if it could be considered one scale, and whether or not there were violation of monotonicity.

<<maasassumptioncheck, echo=FALSE, results=hide>>=
maas.scales <- aisp(na.omit(maasitems))
print(xtable(as.matrix(maas.scales),label="tab:maasassumptioncheck", caption="Item Attribution to Scales, MAAS, Sample One"))
@

The Mokken analysis suggests that two items should be dropped from the scale, items 2 and 6\footnote{because they do not appear to be part of the scale}~\cite{van2007mokken}. This leaves a thirteen item scale for further analysis. Note that items 2 and 6 has extremely low communalities in Table~\ref{tab:maasfact1average}, which reinforces the decision to remove these items from the scale. 

There were no violations of the monotonicity assumption for the reduced scale. The item coefficients (ItemH) are quite low, many of them hang around 0.30, which is the minimum allowed. 

<<maasreduced, echo=FALSE, results=hide, cache=TRUE>>=
maas.irt <- paste(maas, c(1,3:5,7:15), sep="")
maas.irt <- maasitems[,maas.irt]
@


<<maasitemord, echo=FALSE, results=hide, cache=TRUE>>=
maas.iio <- check.iio(na.omit(maas.irt))
print(xtable(maas.iio[["violations"]],label="tab:maasitemord", caption="Invariant Item Ordering Check Results for MAAS"))
@


<<maasmonotonicity, echo=FALSE, results=hide, cache=TRUE>>=
maas.mono <- check.monotonicity(na.omit(maas.irt))
print(xtable(summary(maas.mono),label="tab:maasmono", caption="Monotonicity Check Results for MAAS"))
@

<<maasitemspcm, echo=FALSE, results=hide, cache=TRUE>>=
maas.pcm.rasch <- gpcm(na.omit(maas.irt), constraint="rasch")
maas.pcm.1pl <- gpcm(na.omit(maas.irt), constraint="1PL")
maas.pcm.2pl <- gpcm(na.omit(maas.irt), constraint="gpcm")
@

<<maasirtraschprint, echo=FALSE, results=hide>>=
print(xtable(coef(maas.pcm.rasch), label="tab:maaspcmrasch", caption="MAAS Partial Credit Model Coefficients, Rasch"))
@ 

%% As can be seen from Table \ref{tab:maaspcmrasch}, the Rasch partial credit model is not a good fit to the data. There are numerous violations of the increasing ability scores assumption. Next, a one parameter model (with discrimination estimated from the data) was fitted to the maas items. 

<<maasirtraschprint, echo=FALSE, results=hide>>=
print(xtable(coef(maas.pcm.1pl), label="tab:maaspcm1pl", caption="MAAS Partial Credit Model Coefficients, One Parameter Model"))
@ 

%% The estimated one parameter model is shown in Table \ref{tab:maaspcm1pl}, and it can clearly be seen that it suffers from the same problems as the rasch model fitted above. Finally, a two parameter partial credit model was fitted to these items. 

<<maasirtraschprint, echo=FALSE, results=hide>>=
print(xtable(coef(maas.pcm.2pl), label="tab:maaspcm2pl", caption="MAAS Partial Credit Model Coefficients, Two Parameter Model"))
@ 

%% Table \ref{tab:maaspcm2pl} clearly shows that even the two parameter model does not provide a good fit to the data.

The next stage of analysis for the Mindfulness scale was to fit one and two parameter Graded Response Models to the data. 

<<maasgrmfit, echo=FALSE, results=hide, cache=TRUE>>=
maas.grm.1pl <- grm(maas.irt, constrained=TRUE, Hessian=TRUE)
maas.grm.2pl <- grm(maas.irt, constrained=FALSE , Hessian=TRUE)
@

%% \begin{figure}
<<maasgrm1plplot, echo=FALSE, results=hide>>=
maasp <- ggplotGRM(maas.grm.1pl)
print(maasp)
@   
%%  ~\caption{MAAS Graded Response Model (One Parameter) Ability Thresholds}
%%   \label{fig:maasgrm1plplot}
%% \end{figure}


<<maasgrm1plprint, echo=FALSE, results=tex>>=
maasq <- paste("MAASQ", 1:15, sep="")
maasquestions[,"item"] <- maasq
maascoef <- coefirt(maas.grm.1pl, se=TRUE)
items <- rownames(maascoef)
maascoef2 <- as.data.frame(maascoef)
maascoef2[,"items"] <- with(maascoef2, items)

maascoef.items <- merge(maasquestions, maascoef2, by.x="item", by.y="items")
items.split <- with(maascoef.items, strsplit(as.character(Item), split="Q"))
qs <- sapply(items.split, "[[", 2)
maascoef.items[,"qs"] <- as.numeric(as.character(qs))
maascoef.items2 <- maascoef.items[,2:8]
print(xtable(maascoef, caption="Coefficient Estimates for MAAS One Parameter Graded Response Model, Sample One", label="tab:maasgrm1pl"), sanitize.colnames.function=function (x) x, scalebox=0.7)
@ 

Table~\ref{tab:maasgrm1pl} shows the estimated ability thresholds and discrimination parameter for the one parameter Graded Response Model on the MAAS. The discrimination parameter is moderate, as are the ability estimates, suggesting that this scale may not be suitable for respondents particularly high in mindfulness. 

Next, a two parameter Graded Response Model was examined for the same scale. 

%% \begin{figure}
<<maasgrm2plot, echo=FALSE, results=hide>>=
maas2plp <- ggplotGRM(maas.grm.1pl)
print(maas2plp)
@   
%%  ~\caption{MAAS Graded Response Model (Two parameter) Item Ability Thresholds}
%%   \label{fig:maasgrm2plplot}
%% \end{figure}

<<maasgrm2plprint, echo=FALSE, results=hide>>=
print(xtable(coefirt(maas.grm.2pl, se=TRUE), caption="Coefficient Estimates for MAAS Two Parameter Graded Response Model", label="tab:maasgrm2pl"), sanitize.colnames.function=function (x) print(x),scalebox=0.7)
@ 


The two parameter Graded Response Model (Appendix~\ref{cha:append-1:-supp}, Table~\ref{tab:maasgrm2pl}, Page~\pageref{tab:maasgrm2pl}) had the following features. Q14 has the highest discriminatory power, and that Q11 has the highest ability threshold, while Q1 has the lowest. Q11 refers to listening to others while engaging in other tasks, and its ability estimates suggest that it is a good question for pinpointing the abilities of respondents high on the construct of mindfulness. 

The item information curves for each item in one and two parameter models can be seen in Figure~\ref{fig:iicmaasgrm}. Note that items 7, 8, 9, 10 appear to convey far more information over the participant ability levels. However, as seen below, this model was not supported on unseen data. 

\begin{figure}
<<iicmaasgrm, echo=FALSE, fig=TRUE>>=
par(mfrow=c(1, 2))
plot(maas.grm.1pl, type="IIC")
plot(maas.grm.2pl, type="IIC")
@   
  \caption{Item Information Curves for MAAS One Parameter GRM (left) and Two Parameter GRM (right) (Sample One)}
  \label{fig:iicmaasgrm}
\end{figure}

<<maasanovacomp, echo=FALSE, results=hide, eval=FALSE>>=
anova.rasch.maas.1pl <- anova.gpcm(maas.pcm.rasch, maas.pcm.1pl)
anova.1pl.maas.2pl <- anova.gpcm(maas.pcm.1pl, maas.pcm.2pl)
anova.rasch.maas.2pl <- anova.gpcm(maas.pcm.rasch, maas.pcm.2pl)
@

%% The three models were subjected to ANOVA comparison and the 1 parameter model was significantly ($p\le0.001$) better than the Rasch model, and the two parameter model was significantly better than the one parameter model ($p\le0.001$). However, the ultimate test is the ability of the model to predict out-of-sample data. 




<<hom1maasgrmtest, echo=FALSE, results=tex>>=
maas.irt2a <- maasitems2a[,paste(maas, c(1,3:5,7:15), sep="")]
hom1.maas.grm.1pl.test <- testIRTModels(maas.grm.1pl, maas.irt2a, gpcmconstraint=NULL, grmconstraint=TRUE) 
hom1.maas.grm.2pl.test <- testIRTModels(maas.grm.2pl, maas.irt2a, gpcmconstraint=NULL, grmconstraint=FALSE) 
hom1.maas.grm.all <- rbind(hom1.maas.grm.1pl.test, hom1.maas.grm.2pl.test)
rownames(hom1.maas.grm.all) <- c("One Parameter GRM", "Two Parameter GRM")
print(xtable(hom1.maas.grm.all, caption="Performance of MAAS One and Two Parameter GRM's on Unseen Data (Sample Two, Split A)", label="tab:hom1maasgrmtest"))
@ 

As can be seen from Table~\ref{tab:hom1maasgrmtest}, the one parameter GRM provided a better fit to the unseen  data, in contrast to the likelihood-based method. This suggests again that a simpler model for the MAAS, in line with previous research, appears to be a better model for the items. 

\subsubsection{Sample Two}
\label{sec:irt-analyses}



First, the assumptions underlying item response theory modelling are checked. 

<<maas2bcheck, echo=FALSE, results=hide, cache=TRUE>>=
maas2b.iio <- check.iio(na.omit(maasitems2b))
maas2b.mono <- check.monotonicity(na.omit(maasitems2b))
@ 

<<maas2bitemord, echo=FALSE, results=hide>>=
print(xtable(maas2b.iio[["violations"]], caption="Item Ordering Assumption Check, MAAS Split B", label="tab:maas2bitemord"))
maas2b.s <- maasitems2b[,paste(maas, c(1:4, 7:15), sep="")]
@ 

Questions 5 and 6 failed the item ordering assumptions for Split B and so are removed from the scale before further analysis. Note that Q6 was problematic in the factor analytical solutions too, although Q5 was in line with retained items in the factor analysis. The reduced scale had no violations of the monotonicity assumption. 

<<maas2ccheck, echo=FALSE, results=hide, cache=TRUE>>=
maas2c.iio <- check.iio(na.omit(maasitems2c))
maas2c.mono <- check.monotonicity(na.omit(maasitems2c))
maas2c.s <- maasitems2c[,paste(maas, c(1:4, 7:10, 12:14), sep="")]
@ 

For Split C, items 5 and 11 fail the item ordering check, and so are removed. The reduced scale had no failures of the monotonicity assumption, so modelling continues with the reduced scale. 

%% Firstly, three partial credit models are fit to Split B.


Next, one and two parameter graded response models were fit to Split B. 

<<maas2bgrm, echo=FALSE, results=hide, cache=TRUE>>=
maas2b.grm.1pl <- grm(maas2b.s, constrained=TRUE, Hessian=TRUE)
maas2b.grm.2pl <- grm(maas2b.s, constrained=FALSE, Hessian=TRUE)
@ 


<<maas2bgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coefirt(maas2b.grm.1pl,se=TRUE), caption="Coefficient Estimates for MAAS One Parameter Graded Response Model, Split B", label="tab:maas2bgrm1pl"), sanitize.colnames.function=function (x) x,scalebox=0.7)
@ 


It can be seen from Table \ref{tab:maas2bgrm1pl} that there were no problems with the parameter estimates for this model. 

<<maas2bgrm2plprint, echo=FALSE, results=hide>>=
print(xtable(coefirt(maas2b.grm.2pl,se=TRUE), caption="Coefficient Estimates for MAAS Two Parameter Graded Response Model, Split B", label="tab:maas2bgrm2pl"), sanitize.colnames.function=function (x) print(x),scalebox=0.7)
@ 

The two parameter GRM model (not shown)  had no problems with the coefficient estimates for this solution, and the estimates show the usual trade off between discrimination and ability estimate parameters. 

\begin{figure}
<<iicmaasgrm2b, echo=FALSE, fig=TRUE>>=
par(mfrow=c(1, 2))
plot(maas2b.grm.1pl, type="IIC")
plot(maas2b.grm.2pl, type="IIC")
par(mfrow=c(1, 1))
@   
  \caption{Item Information Curves for MAAS One Parameter GRM (left) and Two Parameter GRM (right) (Sample Two, Split B)}
  \label{fig:iicmaasgrm2b}
\end{figure}

The item information curves for the Split B data (Figure~\ref{fig:iicmaasgrm2b}) show a similar breakdown to those from Sample One. This highlights the benefits of fitting both these models, as if the two parameter model were true then approximately similar performance could be obtained from a much smaller scale. Unfortunately for this theory, the more complex model was not supported across any of the splits. 

Next, we examine the performance of these models on unseen data.

<<maas2bgrmtest, echo=FALSE, results=hide, cache=TRUE>>=
maas.irt.notb <- maasitems.notb[,paste(maas, c(1:4, 7:15), sep="")]
maas2b.grm.1pl.test <- testIRTModels(maas2b.grm.1pl, maas.irt.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
maas2b.grm.2pl.test <- testIRTModels(maas2b.grm.2pl, maas.irt.notb, gpcmconstraint=NULL, grmconstraint=FALSE)
maas2b.grm.test.all <- rbind(maas2b.grm.1pl.test, maas2b.grm.2pl.test)
rownames(maas2b.grm.test.all) <- c("One Parameter", "Two Parameter")

@ 

<<maasgrmcompprint, echo=FALSE, results=tex>>=
print(xtable(maas2b.grm.test.all, caption="Performance of One and Two Parameter Graded Response Models for MAAS on Unseen Data (Splits A and C)", label="tab:maas2bgrmtest"))
@ 

As can be seen from Table \ref{tab:maas2bgrmtest}, the one parameter model provided the best fit to the unseen data. 

Next, one and two parameter Graded Response Models are fit to Split C.

<<maas2cgrm, echo=FALSE, results=hide, cache=TRUE>>=
maas2c.grm.1pl <- grm(maas2c.s, constrained=TRUE, Hessian=TRUE)
maas2c.grm.2pl <- grm(maas2c.s, constrained=FALSE, Hessian=TRUE)
@ 

<<maas2cgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coefirt(maas2c.grm.1pl,se=TRUE), caption="Coefficient Estimates for MAAS One Parameter Graded Response Model, Split C", label="tab:maas2cgrm1pl"), sanitize.colnames.function=function (x) x,scalebox=0.7)
@ 


As can be seen from Table \ref{tab:maas2cgrm1pl}, the coefficient estimates appear reasonable. The discrimination parameter is relatively low, suggesting that this scale is good for all levels of abilities, even though the highest estimated difficulty parameter is only 2.05, for Q13 which is ``I often find myself occupied with the future or the past'', which is a relatively concise summary of the entire construct of mindlessness. 

Next, a two parameter model was fit to this data. 

<<maas2cgrm2plprint, echo=FALSE, results=hide>>=
print(xtable(coefirt(maas2c.grm.2pl,se=TRUE), caption="Coefficient Estimates for MAAS, Two Parameter Graded Response Model, Split C", label="tab:maas2cgrm2pl"), sanitize.colnames.function=function (x) print(x),scalebox=0.7)
@ 

The two parameter model (Appendix~\ref{cha:append-1:-supp}, Page~\pageref{tab:maas2cgrm2pl}, Table~\ref{tab:maas2cgrm2pl}), is not that much different from the one parameter model. Of interest is that Q13 remains the most difficult question, but its discrimination parameter has come down, suggesting that it behaves similarly for participants of all ability levels. Q8 has the highest discrimination parameter of all the items and is ``I rush through activities without being really attentive to them'' and it appears that this question is the best at discrimination between those higher and lower on the construct of mindfulness. 

\begin{figure}
<<iicmaasgrm2c, echo=FALSE, fig=TRUE>>=
par(mfrow=c(1, 2))
plot(maas2c.grm.1pl, type="IIC")
plot(maas2c.grm.2pl, type="IIC")
par(mfrow=c(1, 1))
@   
  \caption{Item Information Curves for MAAS One Parameter GRM (left) and Two Parameter GRM (right) (Sample Two, Split C)}
  \label{fig:iicmaasgrm2c}
\end{figure}

Again, the item information curves for the two parameter model (Figure~\ref{fig:iicmaasgrm2c}) show that items 3, 5, 6, 7, 8 and 11 appear to convey much more information across the range of the test, suggesting that a reduced scale could convey more information with fewer items. However, the out of sample tests did not support the additional parameters for this model. 


The final step in the analysis of the MAAS scale is to test the performance of the models on unseen  data. 

<<maas2cgrmtest, echo=FALSE, results=hide, cache=TRUE>>=
maas.irt.notc <- maasitems.notc[,paste(maas, c(1:4, 7:10, 12:14), sep="")]
maas2c.grm.1pl.test <- testIRTModels(maas2c.grm.1pl, maas.irt.notc, gpcmconstraint=NULL, grmconstraint=TRUE)
maas2c.grm.2pl.test <- testIRTModels(maas2c.grm.2pl, maas.irt.notc, gpcmconstraint=NULL, grmconstraint=FALSE)
maas2c.grm.test.all <- rbind(maas2c.grm.1pl.test, maas2c.grm.2pl.test)
rownames(maas2c.grm.test.all) <- c("One Parameter", "Two Parameter")
@ 

<<maas2cgrmcompprint, echo=FALSE, results=tex>>=
print(xtable(maas2c.grm.test.all, caption="Performance of MAAS One and Two Parameter Graded Response Models on Unseen Data (Splits A and B)", label="tab:maas2cgrmtest"))
@ 

As can be seen from Table \ref{tab:maas2cgrmtest}, the one parameter model provided a better fit to the unseen data (though neither model was particularly good). To finalise the examination of the MAAS, in all cases a one factor FA model and one parameter GRM provided the best fit to the data. Additionally, Q6 was removed by all of the IRT models, suggesting that the responses to this question were not consistent with the responses to all of the other questions, and which questions this item's inclusion in the instrument (at least for this particular population). 

\subsection{Back testing}
\label{sec:maas-bt}

Finally, a back testing procedure was applied, where the best fitting models from the second sample were refit on data from the first sample. Both splits showed that a one factor solution and a one parameter Graded Response Model provided the best performance on unseen data. 

<<maasbacktestsem1full, echo=FALSE, results=hide>>=
manifests.maas <- paste0(maas, 1:15)
latents.maas <- "Mindfulness"
maas.bt.full <- mxModel(name="MAASBackTestFull", 
                        type="RAM",
                        manifestVars=manifests.maas,
                        latentVars=latents.maas,
                        mxPath(from=latents.maas, to=manifests.maas),
                        mxPath(from=latents.maas, arrows=2, free=FALSE,
                               values=1),
                        mxPath(from=manifests.maas, arrows=2),
                        mxData(observed=cov(na.omit(maasitems)), type="cov", numObs=356))
maas.bt.full.fit <- mxRun(maas.bt.full)
@ 

<<maasbacktestreduced1, echo=FALSE, results=hide, cache=TRUE>>=

maasitems.red1 <- paste0(maas, c(1:4, 7:15))
maasitems.s1 <- maasitems[,maasitems.red1]
latents <- "Mindfulness"
maas.bt.reduced1 <- mxModel("MAASBackTestReduced1",
                            type="RAM",
                            manifestVars=maasitems.red1,
                            latentVars=latents,
                            mxPath(from=latents, to=maasitems.red1),
                            mxPath(from=latents, arrows=2, free=FALSE,
                                   values=1),
                            mxPath(from=maasitems.red1, arrows=2),
                            mxData(observed=cov(na.omit(maasitems.s1)), type="cov", numObs=356))
maas.bt.red1.fit <- mxRun(maas.bt.reduced1)
@ 


<<maasbacktestreduced2, echo=FALSE, results=hide, cache=TRUE>>=
maasitems.red2 <- paste0(maas, c(1:4, 7:10, 12:14))
maasitems.s2 <- maasitems[,maasitems.red2]
latents.maas <- "Mindfulness"
maas.bt.reduced2 <- mxModel(name="MAASBackTestReduced2", 
                            type="RAM",
                            manifestVars=maasitems.red2,
                            latentVars=latents.maas,
                            mxPath(from=latents.maas, to=maasitems.red2),
                            mxPath(from=maasitems.red2, arrows=2),
                            mxPath(from=latents.maas, arrows=2, free=FALSE,
                                   values=1),
                            mxData(observed=cov(na.omit(maasitems.s2)), type="cov",
                                   numObs=356))
maas.bt.reduced2.fit <- mxRun(maas.bt.reduced2)
@ 

<<maasbacktestsemcompare, echo=FALSE, results=tex>>=
print(xtable(mxCompare(base=maas.bt.full.fit, comp=c(maas.bt.red1.fit, maas.bt.reduced2.fit)), label="tab:maasbacktestsemcompare", caption="Full and Reduced Models for MAAS Tested on Sample One Data"), scalebox=0.8, include.rownames=FALSE)
@ 

As can be seen from Table \ref{tab:maasbacktestsemcompare}, the reduced model without questions 5, 6 and 15  provided the best fit to the unseen data. 




The next step in the back testing procedure was to test the IRT models on Sample One. 


<<maasbacktestirt, echo=FALSE, results=hide, cache=TRUE>>=

maas.bt.grm.1pl2b <- testIRTModels(maas2b.grm.1pl, maasitems.s1, gpcmconstraint=NULL, grmconstraint=TRUE)
maas.bt.grm.1pl2c <- testIRTModels(maas2c.grm.1pl, maasitems.s2, gpcmconstraint=NULL, grmconstraint=TRUE)
@ 

<<maasbacktestresprint, echo=FALSE, results=tex>>=
maas.bt.grm.test.all <- rbind(maas.bt.grm.1pl2b, maas.bt.grm.1pl2c)
rownames(maas.bt.grm.test.all) <- c("Split B", "Split C")
print(xtable(maas.bt.grm.test.all, label="tab:maasbtgrmtest", caption="Backtesting of MAAS IRT Models on Sample One Data"))
@ 


It can be seen that the model developed from Split C (without items 5,6 and 15) provides the best fit to the unseen data, and this model will be used to generate scores for the experimental participants.


\section{Life Orientation Test, Revised}
\label{sec:life-orient-test}

The next step in the analysis was to investigate the structure of the LOT-R with respect to factor analysis and IRT methods. 

<<lotritems, echo=FALSE, results=tex>>=
lotrquestions <- read.csv("lotritems.csv", colClasses="character", header=FALSE)
names(lotrquestions) <- c("Item", "Question")
print(xtable(lotrquestions, label="tab:lotritems", caption="Life Orientation Test, Revised Item Content"), include.rownames=FALSE)
@ 

Table~\ref{tab:lotritems} shows the content of the items for the LOT-R. 

\subsection{Factor Analyses, Sample One}
\label{sec:fact-analys-sample-1}



Parallel Analysis indicated that two factors should be extracted, while the MAP criterion suggested one.  Therefore, both one and two factor solutions were extracted from the matrix and their results examined for adequacy and interpretability.

%% \subsection{LOTR One Factor Solution}
%% \label{sec:lotr-one-factor}


<<lotr1fact, echo=FALSE, results=tex>>=
lotr.fact.1<-factor.pa(na.omit(lotritems), 1, rotate="oblimin")
lotr1.xtab <- FactorXtab(lotr.fact.1, names=c("Optimism"),label="tab:hom1lotr1fact", caption="Factor Loadings, One Factor Solution, LOT-R, Sample One")
rownames(lotr1.xtab) <- lotritems.paste
print(lotr1.xtab, sanitize.text.function= function (x) x)
@

Table~\ref{tab:hom1lotr1fact} shows the loadings for the one factor solution. The communalities are relatively low, which is a sign that perhaps this solution is not optimal. As can be seen below, the two factor solution increases the communalities, but at the cost of potentially unwarranted complexity. 


The non-normed fit index was equal to \Sexpr{round(lotr.fact.1[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(lotr.fact.1[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(lotr.fact.1[["RMSEA"]][2],3)} to \Sexpr{round(lotr.fact.1[["RMSEA"]][3],3 )}.  This solution does not seem optimal, as the RMSEA is well outside the recommended bounds, and the NNFI is quite low.  %% In addition, the communalities are not very high, with over half the variance being left out of the solution.




%% \subsection{LOTR Two Factor Solution}
%% \label{sec:lotr-two-factor}

<<lotr3fact, echo=FALSE, results=tex>>=
lotr.fact.2<-factor.pa(na.omit(lotritems), 2, rotate="oblimin")
lotr2.xtab <- FactorXtab(lotr.fact.2, names=c("Pessimism", "Optimism"), label="tab:tcq1lotr2fact", caption="Factor Loadings, Two Factor Solution, LOT-R, Sample One")
rownames(lotr2.xtab) <- lotritems.paste
print(lotr2.xtab, sanitize.text.function= function (x) x)
@
The two factor solution (Table~\ref{tab:tcq1lotr2fact}) broke down as follows:
PA2: ``Q1'',  ``Q4'',  ``Q10''. These items are all the positively framed items, and so this factor can best be termed as Optimism. 

PA1: ``Q3'', ``Q7'', ``Q9'', ``Q10'' This factor consisted of the pessimism items, and so can best be termed Pessimism. Note that Item 10 has extremely poor loadings on both factors, which is surprising given that it can often be taken as an indicator for the entire construct. Additionally, Q7 and Q10 have a small cross-loading on the optimism factor, and the communalities are quite low, suggesting that this model is not particularly well-fitting for this sample. 


%% \subsection{CFA for LOTR}
%% \label{sec:cfa-lotr}

<<lotr1sem, echo=FALSE, results=hide, cache=TRUE>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism")
optimism <- paste(lotr, c(1,3,4,7,9,10), sep="")
Lotr1model <- mxModel(name="LOTR1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems)), type="cov", numObs=364)
                      )
lotr1fit <- mxRun(Lotr1model)
lotr1summ <- summary(lotr1fit)
@


<<lotr3sem, echo=FALSE, results=hide, cache=TRUE>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism", "Pessimism")
optimism <- paste(lotr, c(1,4,10), sep="")
pessimism <- paste(lotr, c(3,7, 9, 10), sep="")
Lotr2model <- mxModel(name="LOTR2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from="Pessimism", to=pessimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems)), type="cov", numObs=364)
                      )
lotr2fit <- mxRun(Lotr2model)
lotr2summ <- summary(lotr2fit)
@

<<lotrcompare, echo=FALSE, results=tex>>=
lotrcomp <- mxCompare(base=lotr1fit, comparison=lotr2fit)
lotrcomp.xtab <- xtable(lotrcomp,label="tab:semlotrcomp", caption="Comparison of CFA for the LOT-R Sample One Models on Sample One")
print(lotrcomp.xtab)
@

As can be seen from Table \ref{tab:semlotrcomp}, the one factor solution provided the best fit to the data.  Therefore, this solution will be tested on the second sample.

%% \subsubsection{Confirmatory Analyses, Sample Two}
%% \label{sec:lotr-cfa-sample}



<<lotr1sem2, echo=FALSE, results=hide, cache=TRUE>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism")
optimism <- paste(lotr, c(1,3,4,7,9,10), sep="")
Lotr1model2 <- mxModel(name="LOTR1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2a)), type="cov", numObs=313)
                      )
lotr1fit2 <- mxRun(Lotr1model2)
lotr1summ2 <- summary(lotr1fit2)
@


<<lotr3sem, echo=FALSE, results=hide, cache=TRUE>>=
Lotr2model2 <- mxModel(Lotr2model, 
                       name="LOTR2Model2",
                      mxData(
                          observed=cov(na.omit(lotritems2a)), 
                          type="cov", numObs=313))
lotr2fit2 <- mxRun(Lotr2model2)
lotr2summ2 <- summary(lotr2fit2)
@

<<lotrcompare2, echo=FALSE, results=tex>>=
lotrcomp2 <- mxCompare(base=lotr1fit2, comparison=lotr2fit2)
lotrcomp.xtab2 <- xtable(lotrcomp2,label="tab:lotrcompare2", caption="Comparison of CFA Results for  LOTR Sample One, One and Two Factor Models on a Subset of Sample Two (Split A)")
print(lotrcomp.xtab2)
@

Table \ref{tab:lotrcompare2}, shows that the one factor model provides the best fit to the subsample of data used to examine the models performance on new data.

\subsection{Factor Analyses, Sample Two}
\label{sec:sample-two-1}


<<lotr2bparallel, echo=FALSE, results=hide, cache=TRUE>>=
sink("tmp.txt")
lotr2b.parallel <- fa.parallel(na.omit(lotritems2b))
lotr2b.vss <- VSS(na.omit(lotritems2b))
sink(NULL)
@

<<lotr2cparallel, echo=FALSE, results=hide, cache=TRUE>>=
sink("tmp.txt")
lotr2c.parallel <- fa.parallel(na.omit(lotritems2c))
lotr2c.vss <- VSS(na.omit(lotritems2c))
sink(NULL)
@
<<lotrparallelplot2b, echo=FALSE, results=hide>>=
sink("tmp.txt")
par(mfrow=c(1,2))
## print(lotr2b.parallel)
## print(lotr2b.vss)
sink(NULL)
@ 

Again, the parallel analysis criterion suggests two factors, while the MAP criterion suggests one, so both solutions will be examined and interpreted for both splits, as the results were broadly similar. 

<<lotr2bfact1, echo=FALSE, results=hide>>=
lotr2b.fact1 <- fa(lotritems2b, 1, rotate="oblimin", fm="pa")
print(FactorXtab(lotr2b.fact1, names=c("Optimism"),label="tab:hom2blotr1", caption="One Factor Solution, LOT-R, Sample Two, Split B"), sanitize.text.function= function (x) x)
@


<<lotr2cfact1, echo=FALSE, results=hide>>=
lotr2c.fact1 <- fa(lotritems2c, 1, rotate="oblimin", fm="pa")
print(FactorXtab(lotr2c.fact1,names=c("Optimism"),label="tab:hom2clotr1", caption="One Factor Solution, LOT-R, Sample Two, Split C"), sanitize.text.function= function (x) x)
@

<<lotrfact1average, echo=FALSE, results=tex>>=
lotr2b.fact1 <- FactorNames(lotr2b.fact1, names=c("Optimism"))
lotr2c.fact1 <- FactorNames(lotr2c.fact1, names=c("Optimism"))
lotr.fact1.av <- FactorAverage(sols=list(lotr2b.fact1, lotr2c.fact1), mynames=c("Optimism", "Communalities"))
rownames(lotr.fact1.av) <- lotritems.paste
print(xtable(lotr.fact1.av, label="tab:lotrfact1average", caption="Average One Factor Solution for LOT-R Across Splits B and C"), sanitize.text.function = function (x) x)
@ 

Table~\ref{tab:lotrfact1average} show the averaged coefficients across both splits. Note that the communalities are, on average, much higher than those observed in the first sample. This may be due to the larger sample size or potentially due to the pseudo-random sampling method used in the pen and paper sample. The two solutions are somewhat different. The communalities for Q1 are much higher in Split C than in Split B, while those for Q4 are much lower in Split C than in Split B. To some extent this probably represents sampling error, but it is quite strange that the two negatively worded items should show much of the variance across samples. The averaging produces a solution which appears much more stable. 


<<lotr2bfact2, echo=FALSE, results=hide>>=
lotr2b.fact2 <- fa(lotritems2b, 2, rotate="oblimin", fm="pa")
print(FactorXtab(lotr2b.fact2, names=c("Pessimism", "Optimism"), label="tab:hom2blotr2", caption="Two Factor Solution, LOT-R, Split B"), sanitize.text.function= function (x) x)
@

<<lotr2cfact2, echo=FALSE, results=hide>>=
lotr2c.fact2 <- fa(lotritems2c, 2, rotate="oblimin", fm="pa")
print(FactorXtab(lotr2c.fact2, names=c("Pessimism", "Optimism"), label="tab:hom2clotr2", caption="Two Factor Solution, LOT-R, Split C"), sanitize.text.function= function (x) x)
@

<<lotrfact2average, echo=FALSE, results=tex>>=
lotr2b.fact2 <- FactorNames(lotr2b.fact2, names=c("Optimism", "Pessimism"))
lotr2c.fact2 <- FactorNames(lotr2c.fact2, names=c("Optimism", "Pessimism"))
lotr.fact2.av <- FactorAverage(sols=list(lotr2b.fact2, lotr2c.fact2), mynames=c("Optimism", "Pessimism", "Communalities"))
rownames(lotr.fact2.av) <- lotritems.paste
print(xtable(lotr.fact2.av, label="tab:lotrfact2average", caption="Average Two Factor Solution for LOT-R Across Splits B and C"), sanitize.text.function = function (x) x)
@ 

Table~\ref{tab:lotrfact2average} shows the averaged solution across Splits B and C. In general, though the major outline of the structure is the same, Split B's solution is much cleaner than that of Split C. For example, in Split C it is unclear whether or not Q10 belongs to the first or second factor. Therefore, it would seem advisable to prefer the structure from Split B, but this will be tested. 

<<lotr1sem2b, echo=FALSE, results=hide, cache=TRUE>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism")
optimism <- paste(lotr, c(1,3,4,7,9,10), sep="")
Lotr1model2b <- mxModel(name="LOTR1b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2c)), type="cov", numObs=370)
                      )
lotr1fit2b <- mxRun(Lotr1model2b)
lotr1summ2b <- summary(lotr1fit2b)
@


<<lotr1sem2c, echo=FALSE, results=hide, cache=TRUE>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism")
optimism <- paste(lotr, c(1,3,4,7,9,10), sep="")
Lotr1model2c <- mxModel(name="LOTR1c",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2b)), type="cov", numObs=370)
                      )
lotr1fit2c <- mxRun(Lotr1model2c)
lotr1summ2c <- summary(lotr1fit2c)
@

<<lotr2sem2b, echo=FALSE, results=hide, cache=TRUE>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism", "Pessimism")
optimism <- paste(lotr, c(1,4,10), sep="")
pessimism <- paste(lotr, c(3,7,9), sep="")
Lotr2model2b <- mxModel(name="LOTR2b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from="Pessimism", to=pessimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2c)), type="cov", numObs=370)
                      )
lotr2fit2b <- mxRun(Lotr2model2b)
lotr2summ2b <- summary(lotr2fit2b)
@

<<lotr2sem2c, echo=FALSE, results=hide, cache=TRUE>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism", "Pessimism")
optimism <- paste(lotr, c(1,4,10), sep="")
pessimism <- paste(lotr, c(3,7,9), sep="")
Lotr2model2c <- mxModel(name="LOTR2c",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from="Pessimism", to=pessimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2b)), type="cov", numObs=370)
                      )
lotr2fit2c <- mxRun(Lotr2model2c)
lotr2summ2c <- summary(lotr2fit2c)
@

<<lotrcompare2b, echo=FALSE, results=tex>>=
lotrcomp2b <- mxCompare(base=lotr1fit2b, comparison=lotr2fit2b)
lotrcomp.xtab2b <- xtable(lotrcomp2b,label="tab:hom2blotrcomp", caption="Comparison of LOT-R Split B Factor Solutions, Tested on Split C")
print(lotrcomp.xtab2b, include.rownames=FALSE)
@


<<lotrcompare2c, echo=FALSE, results=tex>>=
lotrcomp2c <- mxCompare(base=lotr1fit2c, comparison=lotr2fit2c)
lotrcomp.xtab2c <- xtable(lotrcomp2c,label="tab:hom2clotrcomp", caption="Comparison of LOT-R Split C Factor Solutions, Tested on Split B")
print(lotrcomp.xtab2c, include.rownames=FALSE)
@ 

Tables \ref{tab:hom2blotrcomp} and \ref{tab:hom2clotrcomp} show the performance of the one and two factor solutions on the respective splits. In both solutions, the one factor solution performed better. 





\subsection{IRT Analyses, LOT-R}
\label{sec:lotr-irt}

\subsubsection{Sample One}
\label{sec:sample-one-1}



Firstly, the scale analysis was conducted to determine which items fit best together.

<<lotrscales, echo=FALSE, results=hide>>=
lotr.scales <- aisp(na.omit(lotritems))
print(xtable(as.matrix(lotr.scales),label="tab:lotrscales"))
@

All of the items meet the assumptions of a uni-dimensional scale (reinforcing the superiority of a one factor model seen in the earlier analyses).  Next, the item orderings were examined.

<<lotritemord, echo=FALSE, results=hide, cache=TRUE>>=
lotr.iio <- check.iio(na.omit(lotritems))
print(xtable(lotr.iio[["violations"]],label="tab:lotritemord"))
@


Q1 was removed from the scale in order to meet the assumptions of the model (the IIO assumption).Note that this item had quite a low communality in the FA analyses also.  There were no violations of monotonicity in the sample.

<<lotrmono, echo=FALSE, results=hide, cache=TRUE>>=
lotr.mono <- check.monotonicity(na.omit(lotritems))
print(xtable(summary(lotr.mono),label="tab:lotrmono"))
@


<<lotrreduced, echo=FALSE, results=hide, cache=TRUE>>=
lotr.paste <- paste(lotr, c(3,4,7,9,10), sep="")
lotr.irt <- lotritems[,lotr.paste]
@

After this process of model checking, a five item scale remained for further analysis. 

<<lotrmodelsirt, echo=FALSE, results=hide, cache=TRUE>>=
lotr.pcm.rasch <- gpcm(na.omit(lotr.irt), constraint="rasch")
lotr.pcm.1pl <- gpcm(na.omit(lotr.irt), constraint="1PL")
lotr.pcm.2pl <- gpcm(na.omit(lotr.irt), constraint="gpcm")
@

<<lotrpcmraschprint, echo=FALSE, results=hide>>=
print(xtable(coef(lotr.pcm.rasch), label="tab:lotrpcmrasch", caption="Rasch Partial Credit Model for Life Orientation Test, Revised"))
@ 

%% As shown in Table \ref{tab:lotrpcmrasch}, the model does not provide a good fit to the data. There are numerous violations of the ordering assumptions of the model. Next, a one parameter model was fit to the data. 

<<lotrpcm1plprint, echo=FALSE, results=hide>>=
print(xtable(coef(lotr.pcm.1pl), label="tab:lotrpcm1pl", caption="One parameter Partial Credit Model for Life Orientation Test, Revised"))
@ 

%% As can be seen in Table \ref{tab:lotrpcm1pl}, there are again some violations of the ability ordering assumption (LOTR4, category 3). Next, a two parameter PCM was fitted to the LOTR items. 

<<lotrpcm1plprint, echo=FALSE, results=hide>>=
print(xtable(coef(lotr.pcm.2pl), label="tab:lotrpcmgpcm", caption="Two Parameter Partial Credit Model for Life Orientation Test, Revised"))
@ 

%% The coefficients in Table \ref{tab:lotrpcmgpcm} show that again, LOTR4 ensures that the model does not fit correctly. 

<<lotranovacomp, echo=FALSE, results=hide, cache=TRUE>>=
anova.rasch.lotr.1pl <- anova.gpcm(lotr.pcm.rasch, lotr.pcm.1pl)
anova.1pl.lotr.2pl <- anova.gpcm(lotr.pcm.1pl, lotr.pcm.2pl)
anova.rasch.lotr.2pl <- anova.gpcm(lotr.pcm.rasch, lotr.pcm.2pl)
@

%% The results of the model comparison showed that the rasch model was not significantly different from the one parameter model ($p=0.862$), but that the two parameter model provided a significantly better fit to the data ($p \le 0.001$), even with a penalty for the extra parameters.

%% However, the difference in likelihoods was extremely small between the Rasch and two parameter models (20.15), and the BIC suggested that the Rasch model was a better fit.  One issue for the BIC is that it presumes that a true model exists amongst the candidate models, which is almost certainly not the case in this (or indeed any other psychological research) case.
The next stage in the analysis of the LOT-R was the fitting of one and two parameter Graded Response Models. 

<<lotrgrmfit, echo=FALSE, results=hide, cache=TRUE>>=
lotr.grm.1pl <- grm(lotr.irt, constrained=TRUE, Hessian=TRUE)
lotr.grm.2pl <- grm(lotr.irt, constrained=FALSE, Hessian=TRUE)
@ 
%% \begin{figure}
<<lotrgrm1plplot, echo=FALSE, results=hide>>=
lotr1plgrm <- ggplotGRM(lotr.grm.1pl)
print(lotr1plgrm)
@   
%%  ~\caption{One Parameter Graded Response Model for LOTR Item Ability Thresholds}
%%   \label{fig:lotr1plgrm}
%% \end{figure}

An analysis of the one parameter GRM for the LOT-R showed that there were no violations of item ordering, and that LOTR4 is the hardest item to endorse, while LOTR3 appears to be the easiest. 

%% \begin{figure}
<<lotr2plgrmplot, echo=FALSE, results=hide>>=
lotr2plgrm <- ggplotGRM(lotr.grm.2pl)
print(lotr2plgrm)
@   
%%  ~\caption{Two Parameter Graded Response Model Item Ability Threshold Plot}
%%   \label{fig:lotr2plgrm}
%% \end{figure}


The next step was to assess which of the models provided the best fit to the data using a likelihood ratio test.

<<anovagrmlotr, echo=FALSE, results=hide, cache=TRUE>>=
anova.grm.lotr <- anova(lotr.grm.1pl, lotr.grm.2pl)
@ 

The two parameter model was significantly better than the one parameter model ($p=0.001$), but the AIC suggested that the one parameter model provided a better overall fit to the data. 

<<lotr2plestimates, echo=FALSE, results=tex>>=
print(xtable(coefirt(lotr.grm.2pl,se=TRUE),label="tab:lotr2plestimates", caption="Coefficient Estimates for LOTR Two Parameter Graded Response Model"), sanitize.colnames.function=function (x) x,scalebox=0.7)
@

The estimates for the two parameter model can be seen in Table \ref{tab:lotr2plestimates}. Note that there are problematic estimates for Q4 at the third response point, suggesting that this model is not particularly useful. Again note how the FA analyses showed some problems with these questions, increasing our confidence in this analysis (for this particular sample, at least). 










Finally, the performance of the one and two parameter IRT models for the LOT-R were assessed. 

<<hom1lotrgrmtest, echo=FALSE, results=tex>>=
lotr.irt2a <- lotritems2a[,paste(lotr, c(3,4,7,9,10), sep="")]
hom1.lotr.grm.1pl.test <- testIRTModels(lotr.grm.1pl, lotr.irt2a, gpcmconstraint=NULL, grmconstraint=TRUE)
hom1.lotr.grm.2pl.test <- testIRTModels(lotr.grm.2pl, lotr.irt2a, gpcmconstraint=NULL, grmconstraint=FALSE)
hom1.lotr.grm.all <- rbind(hom1.lotr.grm.1pl.test, hom1.lotr.grm.2pl.test)
rownames(hom1.lotr.grm.all) <- c("One Parameter GRM", "Two Parameter GRM")
print(xtable(hom1.lotr.grm.all, caption="Performance of LOTR One and Two Parameter GRM\'s on Unseen Data (Sample Two, Split A)", label="tab:hom1lotrgrmtest"))
@ 


As can be seen from Table \ref{tab:hom1lotrgrmtest}, the one parameter model provided a better fit to the unseen data.






%% The next part of the analyses was examining the predictive ability of the IRT models developed on sample one.



\subsubsection{Sample Two}
\label{sec:sample-two-2}



Next, a similar procedure was followed for the second sample. 

<<lotr2bcheck, echo=FALSE, results=hide, cache=TRUE>>=
lotr2b.iio <- check.iio(na.omit(lotritems2b))
lotr2b.mono <- check.monotonicity(na.omit(lotritems2b))
@ 

The LOT-R for Split B had no problems with either item ordering or monotonicity, suggesting that some of the problems observed earlier may have been due to variability from the particular sample. 



Next, one and two parameter Graded Response Models were fit to the data. 

<<lotr2bgrm, echo=FALSE, results=hide, cache=TRUE>>=
lotr2b.grm.1pl <- grm(lotritems2b, constrained=TRUE, Hessian=TRUE)
lotr2b.grm.2pl <- grm(lotritems2b, constrained=FALSE, Hessian=TRUE)
@ 


<<lotr2bgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coefirt(lotr2b.grm.1pl,se=TRUE), caption="Coefficient Estimates for One Parameter Graded Response Model, LOT-R, Split B", label="tab:lotr2bgrm1pl"), sanitize.colnames.function=function (x) x,scalebox=0.7)
@ 

Table \ref{tab:lotr2bgrm1pl} clearly shows that there were no obvious problems with this model. The parameter estimates are relatively low in comparison with other scales, suggesting that these items were easier to endorse. 

<<lotr2bgrm2plprint, echo=FALSE, results=hide>>=
print(xtable(coefirt(lotr2b.grm.2pl,se=TRUE), caption="Coefficient Estimates for Two Parameter Graded Response Model, LOT-R, Split B", label="tab:lotr2bgrm2pl"), sanitize.colnames.function=function (x) print(x), scalebox=0.7)
@ 


The two parameter GRM (Appendix~\ref{cha:append-1:-supp}, Table~\ref{tab:lotr2bgrm2pl}, Page~\pageref{tab:lotr2bgrm2pl}) suggested that the ability estimates have risen while the discrimination parameters have fallen for the majority of the items (except for 7). 


\begin{figure}
<<iiclotr1, echo=FALSE, fig=TRUE>>=
par(mfrow=c(1, 2))
plot(lotr2b.grm.1pl, type="IIC")
plot(lotr2b.grm.2pl, type="IIC")
par(mfrow=c(1, 1))
@   
  \caption{Item Information Curves, LOT-R, Sample Two, Split B for One Parameter GRM (left) and Two Parameter GRM (right)}
  \label{fig:iiclotrgrm}
\end{figure}

The item information curves for Sample One suggest that Q4 (``I'm always optimistic about the future'') (Figure~\ref{fig:iiclotrgrm}) could actually stand as a reasonably good proxy for the entire scale. Again, this more complicated model was not supported on unseen data. 

Finally, we assess the performance of each of these models on unseen data. 

<<lotr2bgrmtest, echo=FALSE, results=tex>>=
lotr2b.grm.1pl.test <- testIRTModels(lotr2b.grm.1pl, lotritems.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
lotr2b.grm.2pl.test <- testIRTModels(lotr2b.grm.2pl, lotritems.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
lotr2b.grm.test.all <- rbind(lotr2b.grm.1pl.test, lotr2b.grm.2pl.test)
rownames(lotr2b.grm.test.all) <- c("One Parameter", "Two Parameter")
@ 

<<lotr2bgrmcompprint, echo=FALSE, results=tex>>=
print(xtable(lotr2b.grm.test.all, caption="Performance of One and Two Parameter Graded Response Models on Unseen Data (Splits A and C)", label="tab:lotr2bgrmtest"))
@ 

As can be seen from Table \ref{tab:lotr2bgrmtest}, the one parameter model performed best on the unseen data. 

<<lotr2caisp, echo=FALSE, results=hide, cache=TRUE>>=
lotr2c.aisp <- aisp(na.omit(lotritems2c))
@ 

<<lotr2ccheck, echo=FALSE, results=hide, cache=TRUE>>=
lotr2c.iio <- check.iio(na.omit(lotritems2c))
lotr2c.mono <- check.monotonicity(na.omit(lotritems2c))
lotr2c.s <- lotritems2c[,paste(lotr, c(3,4,7,9,10), sep="")]
@ 

An examination of the item ordering assumption showed that Q1 did not fit this model for Split C, and so was removed from the scale. This seems to suggest that the issues with Q1 are common across both samples, and that variability was the cause of the lack of this issue, in Split B.   There were no failures of the monotonicity assumption, and thus the modelling could commence. 



One and two parameter Graded Response Models were then fit to the items.

<<lotr2cgrm, echo=FALSE, results=hide, cache=TRUE>>=
lotr2c.grm.1pl <- grm(lotr2c.s, constrained=TRUE, Hessian=TRUE)
lotr2c.grm.2pl <- grm(lotr2c.s, constrained=FALSE, Hessian=TRUE)
@ 

<<lotr2cgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coefirt(lotr2c.grm.1pl,se=TRUE), caption="Coefficient Estimates for LOT-R One Parameter Graded Response Model, Split C", label="tab:lotr2cgrm1pl"), sanitize.colnames.function=function (x) x, scalebox=0.7)
@ 

Table \ref{tab:lotr2cgrm1pl} shows the estimated difficulty parameters
for the one parameter Graded Response Model. It can be seen that the
discrimination parameter is quite high, and that the most difficult
question is Q10 which is ``Overall, I expect more good things to
happen to me than bad''. The ``easiest'' question is Q7, which is one
of the negatively phrased questions, suggesting that the two of these
questions would be enough to garner a rough estimate of ability from
participants.


<<lotr2cgrm2plprint, echo=FALSE, results=hide>>=
print(xtable(coefirt(lotr2c.grm.2pl,se=TRUE), caption="Coefficient Estimates for LOT-R, Two Parameter Graded Response Model", label="tab:lotr2cgrm2pl"), sanitize.colnames.function=function (x) print(x), scalebox=0.7)
@ 

When the estimates for the two parameter GRM (Appendix~\ref{cha:append-1:-supp}, Table~\ref{tab:lotr2cgrm2pl}, Page~\pageref{tab:lotr2cgrm2pl}) were examined, it can be seen that Q7 is  the most discriminating question, while still having the lowest ability estimates, suggesting that it is a very good question for separating out optimism and pessimism. Q10 is still the most difficult, but not as discriminating as Q7. 

Finally, the performance of these two models is tested against unseen data. 

<<lotr2cgrmtest, echo=FALSE, results=tex>>=
lotr.irt.notc <- lotritems.notc[,paste(lotr, c(3,4,7,9,10), sep="")]
lotr2c.grm.1pl.test <- testIRTModels(lotr2c.grm.1pl, lotr.irt.notc, gpcmconstraint=NULL, grmconstraint=TRUE)
lotr2c.grm.2pl.test <- testIRTModels(lotr2c.grm.2pl, lotr.irt.notc, gpcmconstraint=NULL, grmconstraint=FALSE)
lotr2c.grm.test.all <- rbind(lotr2c.grm.1pl.test, lotr2c.grm.2pl.test)

rownames(lotr2c.grm.test.all) <- c( "One Parameter", "Two Parameter")

@ 

<<lotr2cgrmcompprint, echo=FALSE, results=tex>>=
print(xtable(lotr2c.grm.test.all, caption="Performance of LOT-R Split C One and Two Parameter Graded Response Models on Unseen Data", label="tab:lotr2cgrmtest"))

@ 

As can be seen from Table \ref{tab:lotr2cgrmtest}, the one parameter model provides the best fit to the unseen data. 

\subsection{Back testing - LOTR}
\label{sec:backtesting-lotr}



Each of the splits showed that a one factor solution provided the best fit to the data, but the IRT approach from Split B showed that Q1 did not provide a good fit to the scale. Therefore, for the back testing process the two models examined were both one factor models, but the second one had Q1 removed from the scale.

<<lotrbacktestsem1full, echo=FALSE, results=hide>>=
optimism <- paste(lotr, c(1, 3, 4, 7, 9, 10), sep="")
latents <- c("Optimism")
manifests <- optimism
lotr.full.bt.model <- mxModel(name="LOTRBacktestFull", 
                              type="RAM",
                              manifestVars=manifests,
                              latentVars=latents,
                             mxPath(from=latents, to=optimism),
                              mxPath(from=manifests, arrows=2),
                              mxPath(from=latents, arrows=2, free=FALSE, 
                                     values=1),
                              mxData(observed=cov(na.omit(lotritems)), type="cov", numObs=356))
lotr.bt.full.fit <- mxRun(lotr.full.bt.model)
lotr.mod.formative <- mxModel(name="LOTRFullBTFormative", model=lotr.full.bt.model, mxPath(from=optimism, to=latents))
lotr.bt.mod.form.fit <- mxRun(lotr.mod.formative) ##doesn't complete
@ 

<<lotrbacktestsem1reduced, echo=FALSE, results=hide, cache=TRUE>>=
manifests.s <- paste0(lotr, c(3, 4, 7,9,10))
lotritems.s <- lotritems[,paste0(lotr, c(3,4,7,9,10))]
lotr.bt.small <- mxModel(name="LOTRBackTestReduced",
    type="RAM", 
                         manifestVars=manifests.s,
                         latentVars=latents,
                            mxPath(from=latents, to=manifests.s),
                              mxPath(from=manifests.s, arrows=2),
                              mxPath(from=latents, arrows=2, free=FALSE, 
                                     values=0.01),
                              mxData(observed=cov(na.omit(lotritems.s)), type="cov", numObs=356)) 
lotr.bt.small.fit <- mxRun(lotr.bt.small)
@ 

<<lotrbacktestsemcompare, echo=FALSE, results=tex>>=
print(xtable(mxCompare(lotr.bt.small.fit, lotr.bt.full.fit), label="tab:lotrbtsemcompare", caption="Performance of Full and Reduced Model for LOT-R on Sample One Data"), scalebox=0.8, include.rownames=FALSE)
omxGraphviz(lotr.bt.full.fit, dotFilename="lotrfullbacktest.dot")
omxGraphviz(lotr.bt.small.fit, dotFilename="lotrsmallbacktest.dot")
@ 

Table \ref{tab:lotrbtsemcompare} shows the results of the full and reduced one factor model on the Sample One Data. It can clearly be seen that the reduced model provides a better fit, even though the RMSEA is quite high (0.09). All other fit indices are quite good, however. Therefore, this reduced scale will be used in the experimental portion of the research. 

Next, the fit of the different IRT models was examined using the same approaches as before. 

<<lotrbacktestgrm, echo=FALSE, results=hide>>=
lotrbacktest2b.grm.1pl <- testIRTModels(lotr2b.grm.1pl, lotritems, gpcmconstraint=NULL, grmconstraint=TRUE)
lotrbacktest2c.grm.1pl <- testIRTModels(lotr2c.grm.1pl, lotritems.s, gpcmconstraint=NULL, grmconstraint=TRUE)
lotrbt.test.all <- rbind(lotrbacktest2b.grm.1pl, lotrbacktest2c.grm.1pl)
rownames(lotrbt.test.all) <- c("One Parameter Split B", "One Parameter Split C")

@ 

<<lotrbtprint, echo=FALSE, results=tex>>=
print(xtable(lotrbt.test.all, caption="Performance of LOT-R Split C One and Two Parameter Graded Response Models on Unseen Data", label="tab:lotrbtgrmtest"))
@ 

As can be seen from Table \ref{tab:lotrbtgrmtest}, the model developed on Split C with only five items fit the data from Sample One better than did the full model. This further shows both the utility of this approach, and its usefulness in selection of models for the experimental portion of the research. 






\section{Health, Optimism and Mindfulness Analysis}
\label{sec:infer-stat}

Following the investigation of the scales from a psychometric point of view, the remaining hypotheses of the study were addressed. 


  
\begin{landscape}
<<scalecorr, echo=FALSE, results=tex>>=
scales2 <- hom[,66:75]
scale.cor <- corr.test(scales2, use="complete", method="spearman")
cors <- scale.cor$r
cors.xtab <- xtable(as.data.frame(cors), label="tab:hom2scalecor",  caption="Spearman Correlations between Scale Totals across both Samples")
cors.xtab[lower.tri(cors.xtab, diag=FALSE)] <- NA
print(cors.xtab, scalebox=0.75)
@ 
\end{landscape}




As can be seen from Table~\ref{tab:hom2scalecor}, the optimism hypothesis
was not supported.  Contrary to predictions, optimism was negatively
correlated with health (using Spearman's correlation coefficient). 
In fact, optimism correlated negatively with all of the other totals, suggesting that something unusual happened in the sample. This finding is discussed further throughout this chapter, Chapter~\ref{cha:primary-research} and Chapter~\ref{cha:general-discussion}. 


Participants of both genders showed the relationship in the same direction, with participants reporting greater health reporting less optimism. The result is a general trend across all subgroups divided by college, suggesting that it is the result of a general pattern across the sample rather than being driven by some small number of aberrant observations. Indeed, when participants were stratified by College of study, the same trend was apparent suggesting that the relationship was consistent across all sub-groups. 




It can be seen from Table~\ref{tab:hom2scalecor} that the relationship between health and mindfulness was positive, and of the same magnitude at that observed between health and optimism. 


Gender did not appear to have a substantial effect on mindfulness totals, although it is interesting to note that the range of health scores reported was much greater in the female participants.



MAAS scores were associated with greater health as expected, as can be seen from Table~\ref{tab:hom2scalecor}


\subsection{Sample One, Relationships between Health, Optimism and Mindfulness}

<<cvsetup, echo=FALSE, results=hide, cache=TRUE>>=
hom1.full <- na.omit(hom1)
hom1.partition <- with(hom1.full, createDataPartition(optimism, list=FALSE, p=0.8))
hom1.train.opt <- hom1.full[hom1.partition,]
hom1.test.opt <- hom1.full[-hom1.partition,]
mytrain <- trainControl(method="cv")
@ 

<<healthsetup, echo=FALSE, results=hide, cache=TRUE>>=
hom1.health.train.ind <- with(hom1.full, createDataPartition(generalhealth, p=0.75, list=FALSE))
hom1.health.train <- hom1.full[hom1.health.train.ind,]
hom1.health.test <- hom1.full[-hom1.health.train.ind,]
healthpred.train <- hom1.health.train[,c(4, 66:72, 74:75)]
healthall.train <- hom1.health.train[,c(10:40, with(hom1.health.train,grep("^MAAS|^LOTR", x=names(hom1.health.train))))]

health.train <- with(hom1.health.train, generalhealth)
healthpred.test <- hom1.health.test[,c(4, 66:72, 74:75)]
healthall.test <- hom1.health.test[,c(10:40, with(hom1.health.train,grep("^MAAS|^LOTR", x=names(hom1.health.test))))]

health.test <- with(hom1.health.test, generalhealth)
@ 


<<healthregfirst, echo=FALSE, results=hide, cache=TRUE>>=
health.mod1<-lm(generalhealth~mindfulness+physfun+optimism+energyfat+emwellbeing+Age+pain+Status+socialfunctioning+rolelim+rolelimem, na.action="na.omit", data=hom1.health.train)
health.step <- stepAIC(health.mod1, direction="both", k=3)
health.step.test <- lm(generalhealth~optimism+energyfat+pain+rolelim, data=hom1.health.test)

@

<<healthregfirstprint, echo=FALSE, results=tex>>=

print(xtable(apareg(health.step.test), caption="Coefficients for General Health Regression Model with predictors chosen by Stepwise Selection on unseen data, Sample One", label="tab:hom1healthsteptest"), sanitize.text.function = function (x) x)
@ 
Stepwise regression was used for the selection of predictors. Typically, such an approach is not optimal in research, as the p-values are biased by the search process. This is also a problem when variables are manually added or removed from a model, as in standard exploratory practice. In this research, stepwise regression methods were used appropriately, as all p-values are reported on an independent test set, which means that they are unbiased. Additionally, stepwise provides a mathematical justification (in this case, the AIC) for the addition or removal of variables from the model, which was useful in this research given the surprising, non-theoretically predicted results. Another benefit of this approach (in contrast to other selection methods such as lasso and ridge regression) is that stepwise provides a p-value for each coefficient, in line with APA conventions. 
As can be seen from Table~\ref{tab:hom1healthsteptest}, optimism, role limitations and energy fatigue were retained as predictors in the model. None of the variables were significant, with the exception of energy/fatigue and role limitations. 

%% Next, a lasso regression model was fit to the same dataset. 


<<healthlasso, echo=FALSE, results=hide>>=
health.lasso <- penalisedRegression(x=as.matrix(healthpred.train), y=health.train,  testdata=healthpred.test, newy=health.test, alpha=1, type="coefficients")
healthall.lasso <- penalisedRegression(x=as.matrix(healthall.train), y=health.train,  testdata=healthall.test, newy=health.test, alpha=1, type="coefficients")

health.lasso.pred <- penalisedRegression(x=as.matrix(healthpred.train), y=health.train,  testdata=healthpred.test, newy=health.test, alpha=1, type="response")
print(xtable(as.matrix(health.lasso), caption="Coefficients for Lasso Regression on General Health on unseen data", label="tab:hom1healthlasso"))
@ 

%% As can be seen from Table \ref{tab:hom1healthlasso}, optimism appears to be the best predictor in this case, along with energy fatigue and role limitations, supporting the set of predictors chosen by stepwise selection above. 

<<healthridge, echo=FALSE, results=hide>>=
health.ridge <- penalisedRegression(x=as.matrix(healthpred.train), y=health.train,  testdata=healthpred.test, newy=health.test, alpha=0, type="coefficients")
health.ridge.pred <- penalisedRegression(x=as.matrix(healthpred.train), y=health.train,  testdata=healthpred.test, newy=health.test, alpha=0, type="response")
print(xtable(as.matrix(health.ridge), caption="Coefficients for Ridge Regression on General Health on unseen data", label="tab:hom1healthridge"))
@ 

%% As can be seen from Table \ref{tab:hom1healthridge}, thee model for ridge regression is quite similiar, save that many of the coefficients are not shrunk nearly as much using this methodolody. Nonetheless, the overall magnitudes and relationships between the sizes of coefficients are quite similiar. Note the small coefficient for emotional well-being, which would not appear to agree with the raw correlations (see \ref{fig:pairsplot}). This can be explained as a artefact of the procedures used in the penalised regression, as when two predictors are extremely correlated, the lasso and ridge procedure will tend to assign all of the weight to one of them \cite{friedman2009elements}. 

\subsection{Regression Model Predictions}
\label{sec:regr-model-pred}

<<optsetup2a, echo=FALSE, results=hide, cache=TRUE>>=
scales2a.full <- na.omit(scales2a)
scales2a.full2 <- scales2a.full[,-11]
opt.test2a <- with(scales2a.full, optimism)
opt.train.ind <- with(scales2a.full, createDataPartition(optimism, p=0.75, list=FALSE))
scales2a.opt.train <- scales2a.full[opt.train.ind,]
scales2a.opt.test <- scales2a.full[-opt.train.ind,]
opt.train2a <- with(scales2a.opt.train, optimism)
opt.pred.train2a <- scales2a.opt.train[,-11]
## opt.test2a <- with(scales2a.opt.test, optimism)
opt.pred.test2a <- scales2a.opt.test[,-11]
@ 

<<opttrain, echo=FALSE, results=hide, cache=TRUE>>=
testpred <- hom1.test.opt[,c("Age", "pain", "mindfulness", "socialfunctioning", "rolelim", "rolelimem", "emwellbeing", "physfun", "energyfat", "generalhealth")]
trainopt <- with(hom1.train.opt, optimism)
trainpred <- hom1.train.opt[,c(4, 66:74)]
testopt <- with(hom1.test.opt, optimism)
@ 

<<optlassotest, echo=FALSE, results=hide>>=
opt.lasso.test2a <- penalisedRegression(x=trainpred, y=trainopt,  testdata=scales2a.full2, newy=opt.test2a, alpha=1, nfolds=10, type="coefficients")
opt.lasso.test2a.pred <- penalisedRegression(x=trainpred, y=trainopt,  testdata=scales2a.full2, newy=opt.test2a, alpha=1, nfolds=10, type="response")
print(xtable(as.matrix(opt.lasso.test2a), caption="Coefficients for Optimism Regression from Study One on Sample Two (Split A)", label="tab:homoptlasso2a"))
opt.lasso.test2a.resp <- penalisedRegression (x=trainpred, y=trainopt,  testdata=scales2a.full2, newy=opt.test2a, alpha=1, nfolds=10, type="response")
@ 

%% As can be seen from Table \ref{tab:homoptlasso2a}, the general trend for coefficients was similar across the two samples. 

%% Next, the accuracy of the predictions was assessed using a RMSEA approach. For the optimism lasso data. The error in the estimation was equal to 0.755, suggesting that the predictions were approximately 25\% accurate. This can be seen more clearly in Figure \ref{fig:plotoptlassopred} below. As can be seen there is no real systematic error in the predictions, they are just inaccurate.


%% \begin{figure}
<<plotlassopredobs, echo=FALSE, results=hide>>=
print (ggplot (opt.lasso.test2a.resp, aes (x=pred, y=obs))+geom_point ()+geom_smooth (method="lm"))
@   
%%  ~\caption{Predicted versus observed values of Optimism in second sample, Split A, lasso regression}
%%   \label{fig:plotoptlassopred}
%% \end{figure}


%% Next, the ridge regression model was applied to the new data. 

<<optridge2a, echo=FALSE, results=hide>>=
opt.ridge.test2a <- penalisedRegression(x=trainpred, y=trainopt,  testdata=scales2a.full2, newy=opt.test2a, alpha=0, nfolds=10, type="coefficients")
print(xtable(as.matrix(opt.ridge.test2a), caption="Coefficients for Optimism Regression from Study One on Sample Two (Split A)", label="tab:homoptridge2a"))
opt.ridge.test2a.resp <- penalisedRegression (x=trainpred, y=trainopt,  testdata=scales2a.full2, newy=opt.test2a, alpha=0, nfolds=10, type="response")
rmsea.ridge.opt2a <- rmsea (opt.ridge.test2a.resp)
@ 

%% As can be seen from Table \ref{tab:homoptridge2a} mindfulness again appears to be the most useful predictor of optimism, and in general the coefficients are quite similar to those from Study One. The RMSEA approach showed that the ridge regression model had an error of 0.749, which is extremely similar to that of the lasso model. 

%% \begin{figure}
<<optridgeplot2a, echo=FALSE, results=hide>>=
print (ggplot (opt.ridge.test2a.resp, aes (x=pred, y=obs))+geom_point()+geom_smooth())
@   
%%  ~\caption{Ridge Regression Predictions, Optimism, Split 2A}
%%   \label{fig:ridgepredplot2a}
%% \end{figure}


%% As can be seen from Figure~\ref{fig:ridgepredplot2a}, again the errors do not appear to be systematically biased, but rather spread both evenly above and below the line of perfect prediction. 

<<optstep, echo=FALSE, results=hide, cache=TRUE>>=
opt.reg.first <- lm(optimism~(generalhealth+Age+College+pain+mindfulness+socialfunctioning+rolelim+rolelimem+emwellbeing+physfun+energyfat), data=hom1.train.opt)
opt.reg.simple <- lm(optimism~1, data=hom1.train.opt)
opt.step <- stepAIC(opt.reg.first, direction="both", k=2)
opt.step.pred.resp <- predict(opt.step, newdata=hom1.test.opt, type="response")
opt.step.pred.coef <- predict(opt.step, newdata=hom1.test.opt, type="terms")
step.pred.obs <- data.frame(pred=opt.step.pred.resp, obs=hom1.test.opt[,"optimism"])

@ 

The model developed by stepwise selection was applied to the new data. 

<<optsteppred, echo=FALSE, results=hide>>=
opt.step.test <- lm(optimism~generalhealth+energyfat+emwellbeing, data=hom1.test.opt)
print(xtable(apareg(opt.step), caption="Coefficients for Stepwise Selected Regression Model on Optimism", label="tab:hom1stepopttest"), sanitize.text.function = function (x) x)
@ 

<<optstep2a, echo=FALSE, results=hide, cache=TRUE>>=
opt.step.test2a <- lm(optimism~generalhealth+energyfat+emwellbeing, data=scales2a.full)
opt.step.pred.resp <- predict (opt.step.test, newdata=scales2a.full, type="response")
opt.step.pred.obs <- data.frame (pred=opt.step.pred.resp, obs=opt.test2a)
rmsea.opt.step2a <- rmsea (opt.step.pred.obs)
@ 
<<optstep2aprint, echo=FALSE, results=tex>>=
print(xtable(apareg(opt.step.test), caption="Coefficients for Stepwise Selected Model on Test Data, General Health (Sample 2A)", label="tab:homstepopttest2a"), sanitize.text.function = function (x) x)
@ 



As can be seen from Table~\ref{tab:homstepopttest2a}, the stepwise model has an extremely significant coefficient for emotional well being, and non-significant coefficients for  energy fatigue and general health. The coefficient signs are all in line with those from study one. This confirmation between the model fit on a subset of Sample One and that fit on a subset of Sample Two increases the confidence that we should have in this solution.  




\subsection{Regression Analyses, Sample Two}

Given the correlation matrix reported shown above in Table~\ref{tab:hom2scalecor},  regression analyses were run on the three major variables (General Health, Mindfulness and Optimism) to determine which other variables were involved in the effect.

\subsection{Optimism}


<<optstepprint, echo=FALSE, results=tex>>=
print(xtable(apareg(opt.step), caption="Coefficients for Stepwise Selected Regression Model for Optimism", label="tab:hom1stepopt"), sanitize.text.function = function (x) x)
@ 

%% As Table \ref{tab:hom1optstep} shows, 



In the training sample, the stepwise selected models kept three predictors.
The results of the selected predictors on the test sample are shown in Table~\ref{tab:hom1stepopt}.
General health is retained, suggesting that while it has an impact, they are moderated by the effect of emotional well being ($ p \le 0.01$). 
For further discussion of this relationship, see Chapters \ref{cha:primary-research} and \ref{cha:general-discussion}, as well as Section~\ref{sec:discussion}. 

<<optridge, echo=FALSE, results=hide>>=
opt.ridge <- penalisedRegression(x=trainpred, y=trainopt, newy=testopt, testdata=testpred, alpha=0, nfolds=10, type="coefficients")
opt.ridge.pred <- penalisedRegression(x=trainpred, y=trainopt, newy=testopt, testdata=testpred, alpha=0, nfolds=10, type="response")
ridge.xtab <- xtable(as.matrix(opt.ridge), caption="Coefficients for Ridge Regression of Optimism on other variables", label="tab:hom1optridge")
digits(ridge.xtab) <- 4
print(ridge.xtab)
@

%% Next, a form of penalised regression known as ridge regression (or L2 regression) was used. Ridge regression penalises the coefficients by pushing them towards zero in the process of searching for the best model. However, it will not remove variables from the model, regardless of how small their coefficients get. It can be seen from Table \ref{tab:hom1optridge} that all of the coefficients are quite small, and mostly negative (which is the same as was seen in the OLS fit). Note that the coefficients shown are from the test data set, not used in the construction of the model. From this model, it would appear that mindfulness and emotional well being are the only useful predictors of optimism. 



%% The predictive ability of the ridge regression model was quite poor, as the model appeared to systematically underpredict the values for optimism. 

%% Next, a lasso regression model was fitted to the data. The lasso includes a penalty on the coefficients, similarly to ridge, but in contrast to ridge regression, lasso will remove predictors from the model.

<<optlasso, echo=FALSE, results=hide>>=
opt.lasso <- penalisedRegression(x=trainpred, y=trainopt,  testdata=hom1.test.opt, alpha=1, nfolds=10, type="coefficients")
opt.lasso.pred <- penalisedRegression(x=trainpred, y=trainopt, newy=testopt,  testdata=as.matrix(testpred), alpha=1, nfolds=10, type="response")
opt.lasso.xtab <- xtable(as.matrix(opt.lasso), caption="Coefficients for Lasso Regression on Optimism scores. Note that coefficients are for the fit of the chosen model on new data", label="tab:hom1optlasso")
digits(opt.lasso.xtab) <- 4
print(opt.lasso.xtab)
lasso.opt.pred <- penalisedRegression(x=trainpred, y=trainopt,  testdata=as.matrix(testpred), alpha=1, nfolds=10, type="link")
@ 

%% Table \ref{tab:hom1optlasso} shows that the lasso procedure has eliminated most of the predictors from the model. Only general health, energy fatigue and emotional well-being remain following the selection procedure. Note that mindfulness appears to be related to optimism under this model, but not when an OLS model is fitted with stepwise selection. Note that the coefficient of emotional well-being is larger in the lasso fit than in the model fitted with ridge regression. 





<<optloess, echo=FALSE, results=hide, cache=TRUE, eval=FALSE>>=
opt.loess <- loess(optimism~generalhealth*mindfulness*emwellbeing, data=hom1.train.opt)
## opt.loess.fit <- tuneLoess(optimism~generalhealth*mindfulness*emwellbeing, data=hom1.train.opt, tuneLength=10, newdata=hom1.test.opt)
opt.loess.pred <- predict(opt.loess, newdata=hom1.test.opt, type="response")
## opt.loess.pred.obs <- data.frame(pred=opt.loess.pred, obs=hom1.test.opt)
## train.gam.loess <- train(optimism~generalhealth*mindfulness*emwellbeing, data=hom1.train.opt, method="gamLoess", tuneLength=10)
@ 

<<optreglm, echo=FALSE, results=hide, cache=TRUE>>=
opt.reg.first <- lm(optimism~(generalhealth+Age+College+pain+mindfulness+socialfunctioning+rolelim+rolelimem+emwellbeing+physfun+energyfat), data=hom1.train.opt)
opt.reg.sum <- summary(opt.reg.first)
opt.reg.xtab <- xtable(opt.reg.sum, label="tab:optregfirst", caption="Maximal Model for Regression on Optimism")
print(opt.reg.xtab, table.placement="ht")
lm.pred.opt <- predict(opt.reg.first, hom1.test.opt, type="response")
@


%% In table \ref{tab:optregfirst} the summary of the regression results can be seen, the model is significant, with an $F$ value of \Sexpr{round(opt.reg.sum[["fstatistic"]],3)}  and an $R^2$ of \Sexpr{round(opt.reg.sum[["adj.r.squared"]],3)}.  


%% Below, in Table \ref{tab:optregfinal} can be seen the final model,
%% including all significant predictor variables.

<<optregfinal, echo=FALSE, results=hide, cache=TRUE>>=
opt.part <- createMultiFolds(na.omit(hom1$optimism), k=10)
optfolds <- TrainTestSets(opt.part, hom1)
opt.reg.final <- lm(optimism~generalhealth+mindfulness+emwellbeing+Age, data=hom1)
opt.reg.fin.stand <- standardize(opt.reg.final)
opt.fin.sum <- apareg(opt.reg.final)
opt.fin.xtab <- xtable(opt.fin.sum, label="tab:optregfinal", caption="Final Regression Model for Optimism")
print(opt.fin.xtab , table.placement="ht", sanitize.text.function = function (x) x)
@



As has been shown above, the  model showed that emotional well being and general health were important, and general health was not removed from the model, though it did not have a particularly large coefficient. Note that because the p-values reported for the regressions were calculated based on a subset of the data not used for selection of the predictors, these can be regarded as unbiased by the search process. 




\subsection{Mindfulness Regressions}

A similiar procedure as described above for optimism was employed
in the midfulness regressions.  The results are shown below.

<<mindsetup, echo=FALSE, results=hide, cache=TRUE>>=
mind.ind <- with(hom1.full, createDataPartition(mindfulness, p=0.8, times=1, list=FALSE))
hom1.mind.train <- hom1.full[mind.ind, ]
hom1.mind.test <- hom1.full[-mind.ind,]
mind.train <- with(hom1.mind.train, mindfulness)
mind.pred.train <- hom1.mind.train[,c(4,67:73,75)]
mind.test <- with(hom1.mind.test, mindfulness)
mind.pred.test <- hom1.mind.test[,c(4,67:73,75)]
@ 

<<maasregfirst, echo=FALSE, results=hide, cache=TRUE>>=
maas.reg.first <- lm(mindfulness~ generalhealth + optimism + pain+ socialfunctioning+physfun+rolelim+rolelimem+emwellbeing+energyfat+Age, data=hom1.mind.train)
mind.step <- stepAIC(maas.reg.first, data=hom1.mind.train, direction="both", k=3)
mind.step.test <- lm(mindfulness~rolelim+rolelimem+emwellbeing+energyfat, data=hom1.mind.test)

mind.step.pred <- predict(mind.step, hom1.mind.test, type="response")
mind.step.pred.obs <- data.frame(pred=mind.step.pred, obs=hom1.mind.test[["optimism"]])
@

<<mindstepprint, echo=false, results=tex>>=
print(xtable(apareg(mind.step.test), caption="Coefficients for Stepwise Selected Regression on Mindfulness", label="tab:hom1mindsteptest"), sanitize.text.function = function (x) x)
@ 

As can be seen from Table \ref{tab:hom1mindsteptest}, the stepwise selection process retained role limitation, emotional role limitations, emotional well being and energy fatigue, even though only emotional well-being remained significant. This model would seem to suggest that mindfulness is related to health through emotional well-being. This is interesting, in that in terms of face validity this would not be expected, although it is reported to be a relatively common outcome of MBSR. 




<<mindlasso, echo=FALSE, results=hide>>=
mind.lasso <- penalisedRegression(x=mind.pred.train, y=mind.train,  testdata=as.matrix(mind.pred.test), alpha=1, type="coefficients")
mind.lasso.xtab <- xtable(as.matrix(mind.lasso), caption="Coefficients on Unseen Data, Mindfulness Regression (lasso penalisation)", label="tab:hom1mindlasso")
digits(mind.lasso.xtab) <- 4
print(mind.lasso.xtab)
mind.lasso.pred <- penalisedRegression(x=mind.pred.train, y=mind.train,  testdata=as.matrix(mind.pred.test), newy=mind.test, alpha=1, type="response")
@ 

%% As shown in Table \ref{tab:hom1mindlasso}, the largest coefficient on mindfulness was optimism, and in line with the correlations seen above, it is negative. Energy Fatigue and Emotional Well being appear to be important in this particular model. 

%% Next, a ridge regression model was fit to this data. 

<<mindridge, echo=FALSE, results=hide>>=
mind.ridge <- penalisedRegression(x=mind.pred.train, y=mind.train,  testdata=as.matrix(mind.pred.test), alpha=0, type="coefficients")
mind.ridge.xtab <- xtable(as.matrix(mind.ridge), caption="Coefficients on Unseen Data, Mindfulness Regression (ridge penalisation)", label="tab:hom1mindridge")
digits(mind.ridge.xtab) <- 4
print(mind.ridge.xtab)
mind.ridge.pred <- penalisedRegression(x=mind.pred.train, y=mind.train,  testdata=as.matrix(mind.pred.test), newy=mind.test, alpha=0, type="response")
@ 

%% Table \ref{tab:hom1mindridge} shows the coefficients for the ridge regression, and they are almost identical to those of the lasso (at reasonable levels of precision). Note that in both penalised regression models, the coefficient for optimism was much higher than that for any other variable, suggesting that optimism scores were the most influential variable included in the model (c.f. Section \ref{sec:model-relat-betw} and Chapters \ref{cha:primary-research}). Next, a stepwise selection model procedure was employed on the dataset. 



\section{Modeling the relationship between health, optimism and mindfulness}
\label{sec:model-relat-betw}

The next step in the analytic procedure was to examine the relationships between health, optimism and mindfulness somewhat more deeply, drawing on the work carried out so far with the data. 

The first step was to assess the fit of the factor models and the regression models together, in a Structural Equation Model. This model was  fit using the summary scores. The regression model was formed by using the predictors from the stepwise regressions (on unseen data). 


\subsection{Optimism and Health}
\label{sec:optimism-health}

The most unexpected finding of this piece of research was the direction of the relationship between the LOT-R and the RAND MOS General Health scale. This section examines a number of different models, to determine which of the solutions is more likely, given the data. 

The models used in this section were as follows:

\begin{itemize}
\item A model where both health and mindfulness, along with the other regression predictors, directly affect optimism;

\item A model where the effect of health on optimism is mediated through the other predictor variables;

%% \item Each of these models was fit both to the summary scores and the latent factors.
\end{itemize}

The regression model suggested that Age, Energy/Fatigue, Emotional Well Being, General Health and Mindfulness were associated with optimism, so these variables are included in the model. 

<<lotrranddirect, echo=FALSE, results=hide>>=
hom1.opt.sem <- hom1[,c("Age", "energyfat", "emwellbeing", "mindfulness", "generalhealth", "optimism")]
predictors <- c("Age", "energyfat", "emwellbeing", "generalhealth", "mindfulness")
response <- "optimism"
variables <- c(predictors, response)
lotr.rand.direct <- mxModel(name="LOTRRandDirect",
                            type="RAM",
                            mxData(observed=na.omit(hom1.opt.sem),
                                   type="raw" ,
                                   means=NA,
                                   numObs=NA),
                            manifestVars=variables,
                            mxPath(from=variables, arrows=2, free=FALSE, values=1),
                       ##      mxPath(from="mindfulness", to="generalhealth", free=TRUE, values=1, arrows=2),
                       ## mxPath(from="mindfulness", to="emwellbeing", free=TRUE, values=1, arrows=2),     
                       ##      mxPath(from="mindfulness", to="energyfat", free=TRUE, values=1, arrows=2),
                       ##      mxPath(from="mindfulness", to="Age", free=TRUE, values=1, arrows=2),
                            mxPath(from="mindfulness", to=c("Age", "emwellbeing", "energyfat", "generalhealth", "optimism" ), connect="unique.pairs", arrows=2),
                            mxPath(from="generalhealth", to=c("Age", "emwellbeing", "energyfat", "mindfulness", "optimism"), connect="unique.pairs", arrows=2),
                            mxPath(from="Age", to=c("emwellbeing", "energyfat", "mindfulness", "optimism", "generalhealth"), connect="unique.pairs", arrows=2),
                            mxPath(from="optimism", to=c("emwellbeing", "energyfat", "mindfulness", "generalhealth", "Age"), connect="unique.pairs", arrows=2),
                            mxPath(from="emwellbeing", to=c("optimism", "generalhealth", "energyfat", "mindfulness", "Age"), connect="unique.pairs", arrows=2),
                            mxPath(from="energyfat", to=c("optimism", "generalhealth", "mindfulness", "emwellbeing", "Age"), connect="unique.pairs", arrows=2),
## mxPath(from="generalhealth", to="emwellbeing", free=TRUE, values=1, arrows=2),
## mxPath(from="generalhealth", to="energyfat", free=TRUE, values=1, arrows=2),
##                             mxPath(from="generalhealth", to="energyfat", free=TRUE, values=1, arrows=2),
##                             mxPath(from="generalhealth", to="Age", free=TRUE, values=1, arrows=2),
##                             mxPath(from="emwellbeing", to="energyfat", free=TRUE, values=1, arrows=2),
##                             mxPath(from="emwellbeing", to="Age", free=TRUE, values=1, arrows=2),
                            mxPath(from="one", to=variables, arrows=1, free=FALSE, values=1),
                            mxPath(from="Age", to="optimism", arrows=1),
                            mxPath(from="energyfat", to="optimism", arrows=1),
                            mxPath(from="emwellbeing", to="optimism", arrows=1),
                            mxPath(from="mindfulness", to="optimism", arrows=1),
                            mxPath(from="generalhealth", to="optimism",arrows=1))
                            
## mxPath(from=predictors, to=respnse))

@ 

<<opthealthdirectfit, echo=FALSE, results=hide>>=
opt.direct.fit <- mxRun(model=lotr.rand.direct)
omxGraphviz(opt.direct.fit, dotFilename="lotrdirect.dot")
@ 

%% \begin{figure}
%% \includegraphics{lotrdirect}  
%%  ~\caption{Direct Model for Optimism and Health}
%%   \label{fig:lotrdirect}
%% \end{figure}



<<opthealthmaaasmdediated, echo=FALSE, results=hide>>=
opt.health.indirect.maas <- mxModel(lotr.rand.direct, mxPath(from=predictors, to=response), remove=TRUE)
rand.lotr.indirect.done <- mxModel(opt.health.indirect.maas,
                                   name="LOTRRandIndirectMAAS",
                                   mxPath(from="generalhealth", to="mindfulness"),
                                   mxPath(from="Age", to="mindfulness"),
                                   mxPath(from="energyfat", to="mindfulness"),
                                   mxPath(from="emwellbeing", to="mindfulness"),
                                   mxPath(from="mindfulness", to="optimism"), independent=TRUE)

@ 

<<fitopthealthmediated1, echo=FALSE, results=hide>>=
rand.lotr.indirect.maas.fit <- mxRun(rand.lotr.indirect.done)
omxGraphviz(rand.lotr.indirect.maas.fit, dotFilename="lotrindirectmaas.dot")
@ 

%% \begin{figure}
%% \includegraphics{lotrindirectmass}  
%%  ~\caption{Optimism and Health Mediated by Mindfulness}
%%   \label{fig:lotrhealthmind}
%% \end{figure}


<<randindirectemwelleing, echo=FALSE, results=hide>>=
rand.indirect.emwellbeing <- mxModel(rand.lotr.indirect.done, remove=TRUE, mxPath(from=predictors, to=response))
rand.indirect.emwellbeing2 <- mxModel(rand.indirect.emwellbeing,
                                      name="RandIndirectEmWellBeing", 
                                      mxPath(from="Age", to="emwellbeing", arrows=1),
                                      mxPath(from="energyfat", to="emwellbeing"),
                                      mxPath(from="generalhealth", to="emwellbeing"),
                                      mxPath(from="mindfulness", to="emwellbeing"),
                                      mxPath(from="emwellbeing", to="optimism"))

@ 

<<opthealthmediatedfit2, echo=FALSE, results=hide>>=
lotr.indirect.emwellbeing <- mxRun(rand.indirect.emwellbeing2)
omxGraphviz(lotr.indirect.emwellbeing, dotFilename="lotrindirectemwellbeing.dot")
@ 


%% \begin{figure}
%%   \includegraphics{lotrindirectemwellbeing}
%%  ~\caption{Optimism and Health, Mediated by Emotional Well Being}
%%   \label{fig:lotrhealthemwellbeing}
%% \end{figure}

<<lotrindirectenergyfat, echo=FALSE, results=hide>>=
lotr.indirect <- mxModel(lotr.rand.direct, remove=TRUE, mxPath(from=predictors, to=response))
lotr.indirect.energyfat <- mxModel(lotr.indirect,
                                   name="LOTRIndirectEnergyFatigue",
                                   mxPath(from="Age", to="energyfat"),
                                   mxPath(from="emwellbeing", to="energyfat"),
                                   mxPath(from="generalhealth", to="energyfat"),
                                   mxPath(from="mindfulness", to="energyfat"),
                                   mxPath(from="energyfat", to="optimism"))

@ 

<<opthealthmediatedfit3, echo=FALSE, results=hide>>=
lotr.indirect.energyfat.fit <- mxRun(lotr.indirect.energyfat)
omxGraphviz(lotr.indirect.energyfat.fit, dotFilename="lotrindirectenergyfat.dot")
@ 

%% \begin{figure}
%%   \includegraphics{lotrindirectenergyfat}
%%  ~\caption{Optimism and Health Mediated by Energy/Fatigue}
%%   \label{fig:lotrhealthenergyfat}
%% \end{figure}

<<lotrindirectgenhealth, echo=FALSE, results=hide>>=
lotr.indirect.genhealth <- mxModel(lotr.indirect, 
                                   name="LOTRIndirectGenHealth",
                                   mxPath(from="Age", to="generalhealth"),
                                   mxPath(from="energyfat", to="generalhealth"),
                                   mxPath(from="emwellbeing", to="generalhealth"),
                                   mxPath(from="mindfulness", to="generalhealth"),
                                   mxPath(from="generalhealth", to="optimism"))

@ 

<<lotrindirectfit4, echo=FALSE, results=hide>>=
lotr.indirect.genhealth.fit <- mxRun(lotr.indirect.genhealth)
omxGraphviz(lotr.indirect.genhealth.fit, dotFilename="lotrindirectgenhealth.dot")
@ 


%% \begin{figure}
%%   \includegraphics{lotrindirectgenhealth}
%%  ~\caption{Optimism Directly Affected by Health}
%%   \label{fig:optindirectgenhealth}
%% \end{figure}

<<lotrmediatedhealth, echo=FALSE, results=hide, cache=TRUE>>=
lotr.health.med <- mxModel(lotr.rand.direct, remove=TRUE, mxPath(from="generalhealth", to="optimism"))
lotr.health.med2 <- mxModel(lotr.rand.direct, 
                            name="LOTRHealthMediated",
                            mxPath(from="generalhealth", to="mindfulness"),
                            mxPath(from="mindfulness", to="optimism"))
lotr.health.med.fit <- mxRun(lotr.health.med2)
omxGraphviz(lotr.health.med.fit, dotFilename="lotrindirectmindfulness.dot")
@ 

%% \begin{figure}
%%   \includegraphics{lotrindirectmindfulness}
%%  ~\caption{Optimism and Health Mediated by Mindfulness}
%%   \label{fig:lotrhealthmind2}
%% \end{figure}

<<lotrmodcomp, echo=FALSE, results=tex>>=
lotr.comp <- mxCompare(base=opt.direct.fit, comp=c(rand.lotr.indirect.maas.fit,lotr.indirect.emwellbeing, lotr.indirect.energyfat.fit, lotr.indirect.genhealth.fit, lotr.health.med.fit))
print(xtable(lotr.comp, label="tab:lotrmodcomp1", caption="Comparison of Different Models between Optimism and Other Variables, Sample One"), scalebox=0.7, include.rownames=FALSE)
@ 

The different models for how optimism is related to the other variables (directly versus mediated) were compared and the results are shown in Table \ref{tab:lotrmodcomp1}. As can be seen from this table, the best fitting model was that where Emotional Well Being mediated the relationship between optimism and health. Note that all of these models fitted quite poorly, so this finding needs to be tested on the second sample. 


%% \section{Sample Two Results}

%% Sample Two was collected through online methods, as described above and had large amounts of missing data concentrated in RAND Q13-16, which was assumed to be MCAR (Missing Completely At Random). 

%% The aims for the second sample collected were as follows:

%% \begin{itemize}
%% \item To replicate the finding of a negative relationship between optimism and health

%% \item To determine if this relationship was still moderated by emotional well-being. 

%% \item To improve the models for the MAAS and the LOT-R to aid in the prediction of placebo in the experimental portion of the research. 
%% \end{itemize}


%% \begin{figure}
<<pairsplot2, echo=FALSE, results=hide>>=
scales2 <- hom[,66:75]
pairs.panels(scales2)
@   
%%   \caption{Correlations between Scale Totals, Sample Two}
%%   \label{fig:pairsplot2}
%% \end{figure}




%% \section{Regression Analyses}
%% \label{sec:regression-analyses}

%% Similiar regression analyses were carried out on the test sample from Study 2 as were carried out on the first sample, focusing mainly on the relationship of optimism to the other variables in the sample, as this was the major finding from Sample One. Similarly to the earlier work, stepwise linear regression models, were applied to the data using a training and test set design.

<<optsetup2b, echo=FALSE, results=hide, cache=TRUE>>=
scales2b.full <- na.omit(scales2b)
scales2b.full2 <- scales2b.full[,-11]
opt.test2b <- with(scales2b.full, optimism)
opt.train.ind <- with(scales2b.full, createDataPartition(optimism, p=0.75, list=FALSE))
scales2b.opt.train <- scales2b.full[opt.train.ind,]
scales2b.opt.test <- scales2b.full[-opt.train.ind,]
opt.train2b <- with(scales2b.opt.train, optimism)
opt.pred.train2b <- scales2b.opt.train[,-11]
opt.test2b <- with(scales2b.opt.test, optimism)
opt.pred.test2b <- scales2b.opt.test[,-11]
@ 

<<optlasso2b, echo=FALSE, results=hide, cache=TRUE>>=
opt.lasso.test2b <- penalisedRegression(x=opt.pred.train2b, y=opt.train2b,  testdata=opt.pred.test2b, newy=opt.test2b, alpha=1, nfolds=10, type="coefficients")
opt.lasso.test2b.pred <- penalisedRegression(x=opt.pred.train2b, y=opt.train2b,  testdata=opt.pred.test2b, newy=opt.test2b, alpha=1, nfolds=10, type="response")
print(xtable(as.matrix(opt.lasso.test2b), caption="Coefficients for Optimism Regression from Study One on Sample Two (Split A)", label="tab:homoptlasso2b"))
opt.lasso.test2b.resp <- penalisedRegression (x=opt.pred.train2b, y=opt.train2b,  testdata=opt.pred.test2b, newy=opt.test2b, alpha=1, nfolds=10, type="response")
@ 

<<optlassoprint, echo=FALSE, results=hide>>=
print(xtable(as.matrix(opt.lasso.test2b), caption="Coefficients for Lasso Regression, Split B", label="tab:optlasso2b"))
@ 


%% Table \ref{tab:optlasso2b} shows the estimated coefficients for the lasso regression model. It can be seen that the pattern is quite similar to that from Sample one, with Age, emotional well being, general health and mindfulness being retained by the model, with the same sign as in the previous sample. 

<<optridge2b, echo=FALSE, results=hide, cache=TRUE>>=
opt.ridge.test2b <- penalisedRegression(x=opt.pred.train2b, y=opt.train2b,  testdata=opt.pred.test2b, newy=opt.test2b, alpha=1, nfolds=10, type="coefficients")
opt.ridge.test2b.pred <- penalisedRegression(x=opt.pred.train2b, y=opt.train2b,  testdata=opt.pred.test2b, newy=opt.test2b, alpha=1, nfolds=10, type="response")
print(xtable(as.matrix(opt.ridge.test2b), caption="Coefficients for Optimism Regression from Study One on Sample Two (Split A)", label="tab:homoptridge2b"))
opt.ridge.test2b.resp <- penalisedRegression (x=opt.pred.train2b, y=opt.train2b,  testdata=opt.pred.test2b, newy=opt.test2b, alpha=1, nfolds=10, type="response")
@ 

<<optridge2bprint, echo=FALSE, results=hide>>=
print(xtable(as.matrix(opt.ridge.test2b), caption="Coefficients for Ridge Regression Model on Optimism, Split B", label="tab:optridge2b"))
@ 

%% Table \ref{tab:optridge2b} shows the estimated coefficients for the ridge regression model on Split B, and it can be seen that the coefficients are broadly in line with those from the lasso fit, and also with the coefficients from Sample One. The fit of the SEM models for this relationship are described below. 

%% INSERT REGRESSION ANALYSES HERE



\


\section{Sample Two Structural Equation Models}
\label{sec:relat-betw-health}

<<lavaanmods, echo=FALSE, results=hide>>=
direct <- 'generalhealth~physfun+rolelim+energyfat+emwellbeing+pain+mindfulness+optimism'
direct.sem <- sem(model=direct, data=scales2)
emwb.mediated <- 'generalhealth~emwellbeing
emwellbeing~rolelim+energyfat+pain+mindfulness+optimism'
emwb.med.sem <- sem(model=emwb.mediated, data=scales2)
mind.mediated <- 'generalhealth~mindfulness
mindfulness~rolelim+energyfat+pain+emwellbeing+optimism'
mind.med.sem <- sem(model=mind.mediated, data=scales2)
@ 

<<lavaanfits, echo=FALSE, results=tex>>=
direct.fitm <- fitMeasures(direct.sem, c("chisq", "df", "pvalue", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"))
emwb.med.fitm <- fitMeasures(emwb.med.sem, c("chisq", "df", "pvalue", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"))
mind.med.fitm <- fitMeasures(mind.med.sem, c("chisq", "df", "pvalue", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"))
health.lav.fits.all <- rbind(direct.fitm, emwb.med.fitm, mind.med.fitm)
rownames(health.lav.fits.all) <- c("Direct", "EmWBMediated", "MindfulnessMediated")
print(xtable(health.lav.fits.all, caption="Comparison of Models for Health, Sample Two", label="tab:healthsemcomp2"), scalebox=0.8)
@ 

As can be seen from Table~\ref{tab:healthsemcomp2}, the best fitting model is one in which Emotional Well Being mediates the relationship between health and optimism, in line with the results shown in Sample One. 




\section{Discussion}
\label{sec:discussion}

A number of interesting findings, some expected and others not, have arisen from this study. Firstly, in contrast to published research, optimism and health were negatively associated. This relationship appears to be mediated through Emotional Well Being, which may point to some earlier work suggesting that the optimism-health connection was mediated through negative affect. This result would seem to contribute some new information to this debate. 

Secondly, the factor structures for the RAND-MOS, the MAAS and the LOT-R were more or less confirmed, and more importantly from the perspective of this thesis, reduced versions of these scales were developed which were more predictive on new data. 



\subsection{Optimism and Health}
\label{sec:optimism-health-1}



The most striking finding is the large negative correlation between
optimism and self reported health.  The sample in this study is quite
large, so the result is unlikely to be a statistical fluke.  That being
said, the result is problematic to explain, given the large amount
of evidence of beneficial effects of optimism on health\cite{rasmussen2009optimism}.

However, there have been some findings where higher optimism
has not has been associated with health outcomes.  There are two major
explanations for the curious and unexpected phenomenon,given the links
established by a recent meta-analysis~\cite{rasmussen2009optimism}.
The first centres on the dimensionality of the optimism construct.
Many believe that these results are caused by optimism self report
measures being reflections of two interlinked constructs, optimism
and pessimism~\cite{Herzberg2006} as established in a factor analytic
study in a sample of over 46,000 participants.  Some authors claim that
the apparently contradictory results suggest that the pessimism part
of the construct is the driver of the effects on health, and that
the correlations between the two constructs decline with age.  This
viewpoint was partially supported by the recent meta-analysis which
found that pessimism had a larger effect on health, though the difference
between optimism and pessimism was not significant~\cite{rasmussen2009optimism}.
The other viewpoint argues that the effects of optimism on health
are mediated by negative and positive affect, and that high levels
of negative affect can either negate or reverse the optimism-health
link~\cite{Baker2007}.  The aforementioned Baker study found that
the optimism health link was entirely mediated by negative affect.
Nonetheless, the balance of the evidence suggests that optimism has
beneficial consequences for health and healthy behaviours.

%% An explanation for this finding might be that it was the result
%% of high levels of negative affect in the population.  However, this
%% variable was not measured, so such an explanation can be regarded
%% as speculative at best. 

It is worth noting however, that the original
literature of the beneficial effects of optimism on health focused
on cellular immunity, which is obviously quite different from self
reported health.A problem with this explanation is that reports in the
literature indicate that the optimism-health link is larger when self
report methods are used~\cite{rasmussen2009optimism}.  

%% Age may alsohave been a factor, as the regression weight for this vaiable was negative, which suggests that the relationship may have been different
%% if this research had been carried out in a sample with a broader distribution of ages.  We do not have a good explanation for this finding.

A more likely explanation (especially given the results reported in Chapter~\ref{cha:primary-research}) is that this unexpected finding was caused by the order in which the measures were administered. In the experimental study, these measures were administered in opposite order, and the correlations became positive. This would suggest that something about answering the questions of the MAAS impacted the scores on the LOT-R. Given that the MAAS is framed in terms of mindlessness, then it seems plausible that answering these questions and becoming aware of how they do not often meet these standards depressed scores on the optimism measure. This theory is additionally supported by the results of the SEM model which suggested that Emotional Well-Being (which could be regarded as a proxy for optimism, given the similarity of the questions) mediated this relationship. 

%% The Structual Equation Model reported for Sample One is a good place to start in this regard. The only models which converged were those which moderated the effect of health on optimism through either Emotional Well Being and Mindfulness. Although the fit of the emotional well being  model was marginally superior, there are good reasons to believe that mindfulness is more important (c.f. the experimental portion of the research). 


\subsection{Mindfulness}
\label{sec:mindfulness}

Another interesting finding which arose from this research is the
impact of mindfulness scores on other health variables.  MAAS scores
correlated positively with all of the health sub-scales, very significantly
in the case of emotional well-being.  This may suggest that brief mindfulness
interventions may be of use for improving overall population health,
both physical and mental.  That being said, it is important to note  that the issues surrounding the mindfulness construct
itself and its relations with mindfulness meditation practice need
to be resolved before such strong conclusions can be drawn.

Another fascinating finding in this research is the strong negative correlation
between mindfulness and optimism.  This study appears to have been the first to assess
these constructs using self-report measures, and this finding was not expected to occur.
MBSR programs have been found to increase optimism in a number of studies~\cite{Carson2004},
but our results seem to show that mindfulness and optimism may be inversely related - but see discussion above for a counterpoint.


There are a number of reasons why this could be so.  Optimism is defined as generalised positive outcome expectancies about the future,
while mindfulness is defined as non-judgemental awareness of the content of thoughts.
It seems plausible that increased mindfulness could lead persons to become less optimistic,
 as their new-found awareness of their own thought patterns and behaviours makes them aware
that events have not always worked out well.  This increased awareness could temper future
assessments of the future, and decrease optimism as measured by the Life Orientation Test Revised. However, work in the experimental sample casts new light on this theory, and is discussed further in Chapter \ref{cha:primary-research}. 

\subsection{Psychometric Analysis}
\label{sec:psych-analys}



This study also confirms the proposed one factor structure for the MAAS, in line with previous research.
This sample also appears to show that the LOT-R can be modelled without loss of information with
just one factor.  A   replicable and parsimonious 8 factor structure for the RAND MOS was demonstrated, 
and our results cast further doubt on the notion that these factors are uncorrelated.

It is worth noting that in all cases, parallel analysis did not provide a good measure of
the best number of factors to retain.  For all three measures, the MAP criterion provided
a more accurate metric.  This may have resulted as parallel analysis procedures tend
to sample from a normal distribution, and this condition was not met for any of our
variables.  The use of multiple decision criteria on a regular basis
in factor analytic research would help to understand which method suits a particular
application best.

Additionally, this study found that a combination of IRT analyses and factor analytic analyses provided the best fitting models for both the MAAS and the LOT-R. In both cases, a model fitted using the scales developed from the use of Mokken analysis and IRT fit indices provided better performance on unseen data than did the models developed exclusively through factor analysis. 

This provides evidence that the combination of these two methods is more useful than either of them alone, and it has allowed for the building of a useful model tailored to the population under study in this thesis which can be used in the experimental portion of the research. The success of these models is discussed further in Chapter \ref{cha:primary-research}.

%% A major limitation of this study was the exclusive use of self report
%% measures, and a student sample.  That being said, the sample was large
%% and representative of the general student population.  Given that over
%% 70\% of Irish people of this age group now attend college, it could
%% be argued that this sample is relatively representative of Irish young
%% people at large.  This, however, is somewhat speculative and further
%% research would need to investigate this proposition further.

%% In conclusion, this study points towards the importance of considering
%% psychosocial variables and their impact on health, and suggests that
%% further research is needed to examine how these psychological variables
%% are mediated by culture into differential biological outcomes.

In conclusion, this portion of the thesis aimed to develop tailored measures for important covariates of the relationship between placebo and implicit and explicit expectancies, and this has been achieved. Additionally, this portion of the research sheds a new perspective on the relationships between self-reported health, optimism and mindfulness. 


<<writefileforexperiment, echo=FALSE, results=hide, cache=TRUE>>=
save.image("healthforthesislatest.rda")
@ 


%%% Local Variables:
%%% TeX-master: "PlaceboMeasurementByMultipleMethods"
%%% TeX-default-extension: "Rnw"
%%% End:
