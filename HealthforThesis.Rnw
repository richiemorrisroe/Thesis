;;; HealthforThesis.Rnw --- HealthforThesis Sweave file

;;; Commentary:
;;This is part of my thesis 

<<importdata, echo=FALSE, results=hide>>=
hom<-read.csv("HOM data for RFINAL.csv")
hom <- hom[,1:65]
write.csv(hom, "homfinal.csv")
hom1 <- hom[with(hom, CollectMeth=="Paper"),]
hom2 <- hom[with(hom, CollectMeth=="Online"),]
## setOptions("xtable.include.rownames"=FALSE)
## setOption("xtable.format.args", list=(big.mark=","))
@ 
\section{Aims of the Research}

As discussed in the literature review, optimism is well known as a predictor of health, both self reported and objectively assessed and mindfulness, while a much newer construct, has also been associated with health in a number of studies.  In addition, optimism has been associated with the placebo response in a number of studies, while mindfulness (and related constructs, such as need for cognition) have been proposed as mediators of the relationship between explicit and implicit measures.
The primary aim of this part of thesis was to develop and test psychometric models that could be used to predict scores according to IRT and factor analysis criteria in the experimental part of the research.

The major hypotheses of this part of the thesis were as follows:
\begin{itemize}
\item Optimism and mindfulness would be positively associated with health.

\item The RAND MOS would have 8 first order factors, and two second order factors, which would be correlated

\item The MAAS would have one factor

\item The LOT-R would have one factor.
\end{itemize}

The major types of analysis carried out are described in the Methodology chapter, as are the sample sizes and sampling methods. 

The response rate for Sample One (paper) was approximately
90\% of those asked, while the response rate for Sample Two (Online)
was 10\%.


%% \section{Results}

\part{Sample 1}

\section{Missing Data Analysis}

The first step in the analysis was the assessment of how much data was missing.

<<loadpackages, echo=FALSE, results=hide>>=
require(ggplot2)
require(psych)
require(xtable)
require(OpenMx)
require(car)
require(arm)
require(caret)
require(nFactors)
require(bcv)
@

<<sourcefunc, echo=FALSE, results=hide>>=
source("func.R")
@




<<scorescalestransform, echo=FALSE, results=hide>>=
grep.rand <- grep("^RANDQ", x=names(hom))
randitems <- hom[,grep.rand]
rand <- "RANDQ"
randset1 <-paste(rand, c(1, 2, 20, 22, 34, 36), sep="")
randset1 <- hom[,randset1]
randrecode1 <- RecodeMany(randset1, vars=c("RANDQ1", "RANDQ2","RANDQ20", "RANDQ34","RANDQ36"), Recodings=("1=100;2=75;3=50;4=25;5=0"))
randset2 <- paste(rand, c(3:12), sep="")
randset2 <- hom[,randset2]
randrecode2 <- RecodeMany(randset2, vars=c("RANDQ3", "RANDQ4","RANDQ5","RANDQ6","RANDQ7","RANDQ8","RANDQ9", "RANDQ10","RANDQ11","RANDQ12"),Recodings="1=0;2=50;3=100")
randset3 <- paste(rand, c(13:19), sep="")
randset3 <- hom[,randset3]
randrecode3 <- RecodeMany(randset3, vars=c("RANDQ13", "RANDQ14", "RANDQ15", "RANDQ16","RANDQ17","RANDQ18", "RANDQ19"), Recodings="1=0;2=100")
randset4 <- paste(rand, c(21,23,26,27,30), sep="")
randset4 <- hom[,randset4]
randrecode4 <- RecodeMany(randset4, vars=c("RANDQ21", "RANDQ23", "RANDQ26","RANDQ27","RANDQ30"), Recodings="1=100;2=80;3=60;4=40;5=20;6=0")
randset5 <- paste(rand, c(24,25,28,29,31), sep="")
randset5 <- hom[,randset5]
randrecode5 <- RecodeMany(randset5, vars=c("RANDQ24", "RANDQ25", "RANDQ28", "RANDQ29", "RANDQ31"), Recodings="1=0;2=20;3=40;4=60;5=80;6=100")
randset6 <- paste(rand, c(32,33,35), sep="")
randset6 <- hom[,randset6]
randrecode6 <- RecodeMany(randset6,vars=c("RANDQ32","RANDQ33","RANDQ35"), Recodings="1=0;2=25;3=50;4=75;5=100")
randrecoding <- ls(pattern="randrecode")
randrecoding.df <- as.data.frame(lapply(randrecoding, function (x) get(x)))
randsortpaste <- paste(rand, c(1:36), sep="")
randitems.unscored <- hom[,grep.rand]
randitems.scored <- randrecoding.df[,randsortpaste]
hom[,grep.rand] <- randitems.scored
hom <- createSumScores(hom) 
hom1 <- hom[hom$CollectMeth=="Paper",]
hom2 <- hom[hom$CollectMeth=="Online",]
@




<<missingdata, echo=FALSE, results=hide>>=
paper.missing <- sapply(hom1, function (x) sum(is.na(x)))
online.missing <- sapply(hom2, function (x) sum(is.na(x)))
@
\begin{figure}
<<label=papermissingplot, echo=FALSE, fig=TRUE>>=
ggplot(as.data.frame(paper.missing),aes(x=paper.missing, y=..density..))+layer(geom="bar", binwidth=1)
@
  \caption{Histogram of Missing values in paper sample}
  \label{fig:papermissingplot}
\end{figure}



As can be seen from Figure \ref{fig:papermissingplot}, there are quite low levels of missing data for the sample collected by paper.


However, the situation is very different with the second sample, as can be seen from Figure \ref{fig:onlinemissingplot}, where there are a number of items which have between six and eight hundred missing values.  It would perhaps be wise to investigate this further, as this amount of missing values in a small set of the data may cause problems further on.

<<missingvaluestable, echo=FALSE, results=tex>>=
missing.many <- online.missing[online.missing>300]
missing.many.df <- as.data.frame(missing.many)
names(missing.many.df) <- "Number of Missing Observations"
missing.many.xtab <- xtable(missing.many.df, label="tab:missingmanytable", caption="Number of Missing Observations for RAND MOS items with greater than 10 percent missingness")
print(missing.many.xtab)
@

It can be seen from Table \ref{tab:missingmanytable} that the problems with missing data are concentrated in four consecutive questions, RAND questions 13 through 16. These questions all load on the Role Limitations subscale, explaining why this subscale shows up with lots of missing data.  The consecutive nature of the data suggests that the reason for this may be that participants believed that it was only necessary to answer one of the questions, though that does not explain why so many people did not answer the first question.

Next, we will impute the missing data using a multiple imputation procedure (as discussed in the Methodology).

<<impute, eval=FALSE, echo=FALSE, results=hide>>=
require(mice)
hom.imp <- mice(hom[,2:length(hom)], m=10) #remove first column as it causes problems with the imputation
hom.comp1 <- complete(hom.imp, 1)
hom.comp2 <- complete(hom.imp, 2)
hom.comp3 <- complete(hom.imp, 3)
hom.comp4 <- complete(hom.imp, 4)
hom.comp5 <- complete(hom.imp, 5)
hom.comp6 <- complete(hom.imp, 6)
hom.comp7 <- complete(hom.imp, 7)
hom.comp8 <- complete(hom.imp, 8)
hom.comp9 <- complete(hom.imp, 9)
hom.comp10 <- complete(hom.imp, 10)
homlist <- list(hom.comp1, hom.comp2, hom.comp3, hom.comp4, hom.comp5, hom.comp6, hom.comp7, hom.comp8, hom.comp9, hom.comp10)
for (i in 1:length(homlist)) {
  homi <- homlist[[i]]
  write.csv(homlist[[i]], file=paste("homcomp", i, ".csv", sep=""))
}
  
@ 

<<readcomphom, echo=FALSE, results=hide>>=
homfiles <- list.files(pattern="homcomp")
homlist <- lapply(homfiles, read.csv)
@ 

\section{Descriptive Statistics}
% \section{Methods}



To begin the analysis, frequencies, means and ranges were calculated
for all the variables of interest.  The results of this analysis can
be seen in Table \ref{tab:sumstatscales} , below.

<<sumstats, echo=FALSE,results=tex>>=
hom.tot <- hom1[,66:75]
tot.sum <- summary(hom.tot)
tot.xtab <- xtable(tot.sum, label="tab:sumstatscales", caption="Summary Statistics for Health Scales, Mindfulness and Optimism Measures")
print(tot.xtab, scalebox=0.75, include.rownames=FALSE)
@



In Table \ref{tab:democollect} the breakdown of the demographics
of the sample by Collection Method is shown.

<<demostats, echo=FALSE, results=tex>>=
hom.demo <- hom1[,2:8]
hom.demo.xtab <- xtable(summary(hom.demo), label="tab:democollect", caption="Demographic Statistics for Sample One (paper sample)")
print(hom.demo.xtab, scalebox=0.6, include.rownames=FALSE)
@


Mindfulness levels were quite high, while optimism levels were
at the half way point of the scale. Health levels were quite high for all of the subscales, which makes sense given the non-clinical sample involved in this research. 



\section{Inferential Statistics}

Following these preliminary analyses, the main hypotheses can now
be addressed.



\begin{figure}
<<pairsplot, echo=FALSE, fig=TRUE>>=
pairs.panels(na.omit(hom.tot))
@
\caption{Pairs plot for Scale totals of Health, Optimism and Mindfulness Data.  Top triangle has correlations scaled by their size, bottom triangle has scatterplots with locally weighted regression lines, diagonal has histograms with density estimation.  GH=Gen Health, PF=Physical Funct, RL=Role Lim, RLE=Emotional Role Lim, EmWB=Emotional Well Being}
\label{fig:pairsplot}
\end{figure}

<<corrmatrix, echo=FALSE, results=tex>>=
hom.tot.cor <- cor(hom.tot, use="pairwise.complete.obs", method="spearman")
hom.tot.cor <- as.data.frame(hom.tot.cor)
names(hom.tot.cor) <- c("Physical Functioning", "Role Limitations", "Emotional Role Limitations", "Energy Fatigue", "Social Functioning", "Pain", "General Health", "Mindfulness", "Optimism")
hom.tot.cor.xtab <- xtable(hom.tot.cor, label="tab:scalecorr", caption="Correlations Between Scales GH=Gen Health, PF=Physical Funct, RL=Role Lim, RLE=Emotional Role Lim, EmWB=Emotional Well Being.  *=p<0.05, **=p<0.01, ***=p<0.001")
print(hom.tot.cor.xtab, scalebox=0.6)
@



As can be seen from Table \ref{tab:scalecorr}, the optimism hypothesis
was not supported.  Contrary to predictions, optimism was negatively
correlated with health.
Possible explanations are examined in the Discussion section.  In fact, optimism correlated negatively with all of the other totals, suggesting that something strange happened in the sample.

\begin{figure}
<<optplot1, echo=FALSE, fig=TRUE>>=
optplot1 <- ggplot(hom1, aes(x=generalhealth, y=optimism))+layer(geom="smooth", method="lm")
print(optplot1)
@  
  \caption{Plot of General Health against optimism with a linear regression smooth line. Dark areas on plot represent the confidence intervals (95\%)}
  \label{fig:optplot1}
\end{figure}


The figure above \ref{fig:optplot1}, shows a linear regression of optimism against general health in the sample collected by paper.  It can be clearly seen that the relationship is negative, an unexpected and surprising occurence.  %a fact which is borne out in the examination of Figure \ref{fig:optplot2} below.




\begin{figure}
<<optplotgend, echo=FALSE, fig=TRUE>>=
optplotgend <- ggplot(hom1, aes(x=generalhealth, y=optimism))+layer(geom="smooth", method="lm")+facet_grid(.~Gender)
print(optplotgend)
@  
  \caption{Plot of General Health against Optimism stratified by Gender using a linear regression smooth. Dark edges represent errors of estimation}
  \label{fig:optplotgend}
\end{figure}


It can be seen from Figure \ref{fig:optplotgend} that participants of both genders showed the relationship in the same direction, with participants reporting greater health reporting less optimism.  Below, the effect of college course is examined, in case this could be skewing results.

\begin{figure}
<<optplotcollege, echo=FALSE, fig=TRUE>>=
optplotcoll <- ggplot(hom1, aes(x=generalhealth, y=optimism,colour=College))+layer(geom="smooth", method="lm")## +facet_grid(~.College)
print(optplotcoll)
@  
  \caption{Plot of General Health against Optimism stratified by Faculty of respondents. Dark areas of plot represent errors of estimates. }
  \label{fig:optplotcollege}
\end{figure}



Again, we see that the result is a general trend across all subgroups divided by college, suggesting that it is the result of a general pattern across the sample rather than being driven by some small number of abberant observations.


\begin{figure}
<<optmaasplot, echo=FALSE, fig=TRUE>>=
optplot.maas<- ggplot(na.omit(hom1), aes(x=generalhealth, y=optimism, colour=mindfulness, size=mindfulness))+geom_point(method="lm")
print(optplot.maas)
@  
  \caption{Scatterplot of General Health against Optimism, stratified by Mindfulness }
  \label{fig:optplotmaas}
\end{figure}


It can be seen from Figure \ref{fig:optplotmaas} that higher levels of mindfulness are associated with higher levels of Health and also with lower levels of Optimism.

\begin{figure}
<<lotrageplot, echo=FALSE, fig=TRUE>>=
lotrage <- ggplot(na.omit(hom1), aes(x=Age, y=optimism, ))+layer(geom="smooth",method="lm")
print(lotrage)
@  
  \caption{Regression Line of Optimism against Age (linear regression smooth). Dark areas on plot surrounding line represent confidence intervals}
  \label{fig:lotrageplot}
\end{figure}


Above, in Figure \ref{fig:lotrageplot} it can be seen that Optimism levels decreased as a function of age, but this finding should be taken with caution as the majority of partiticipants in this study were between 18 and 25, and those who were not are not likely to be typical of the general population (as they are all students).

Below, we examine if the relationship between mindfulness and health can account for the unexpected effects of Optimism.
\begin{figure}
<<healthmaassamp, echo=FALSE, fig=TRUE>>=
healthmaas.samp <- ggplot(na.omit(hom), aes(x=generalhealth, y=mindfulness, colour=CollectMeth))+layer(geom="smooth", method="lm")
print(healthmaas.samp)
@  
  \caption{General Health against Mindfulness, Stratified by Method of Collection, linear regression smooth, dark areas on plots represent confidence intervals}
  \label{fig:healthmaasmethplot}
\end{figure}


It can be seen from Figure \ref{fig:healthmaasmethplot} that the relationship between health and mindfulness was slightly stronger in the paper sample, but not significantly so.
\begin{figure}
<<healthmaasgend, echo=FALSE, fig=TRUE>>=
healthmaas.gend <- ggplot(na.omit(hom1), aes(x=generalhealth, y=mindfulness, colour=Gender))+layer(geom="smooth", method="lm")
print(healthmaas.gend)
@  
  \caption{General Health against Mindfulness Stratified by Gender using a linear regression smooth. Dark areas represent errors of estimation}
  \label{fig:healthmaasgend}
\end{figure}


Again, from Figure \ref{fig:healthmaasgend} it can be seen that Gender did not appear to have a substantial effect on mindfulness totals, although it is interesting to note that the range of health scores reported was much greater in the female participants.
\begin{figure}
<<healthmaas, echo=FALSE, fig=TRUE>>=
healthmaas1 <- ggplot(na.omit(hom1), aes(x=generalhealth, y=mindfulness, colour=College))+layer(geom="smooth", method="lm")
print(healthmaas1)
@  
  \caption{General Health against Mindfulness stratified by College using a linear regression smooth}
  \label{fig:healthmaascoll}
\end{figure}


As can be seen from Figure \ref{fig:healthmaascoll} the relationship between general health and mindfulness levels is positive, and constant across the different groups of students.

\begin{figure}
<<maasage, echo=FALSE, fig=TRUE>>=
maas.age <- ggplot(na.omit(hom1), aes(x=Age, y=mindfulness, ))+layer(geom="smooth", method="lm")
print(maas.age)
@  
  \caption{Age against Mindfulness Scores using a linear regression smooth}
  \label{fig:maasage}
\end{figure}


MAAS scores were associated with greater health as expected, as can be seen from Table \ref{tab:scalecorr}.

\section{Regression Analyses}

Given the correlation matrix reported above in Table \ref{tab:scalecorr}, regression analyses were run on the three major variables (General Health, Mindfulness and Optimism) to determine which other variables were involved in the effect.

\subsection{Optimism}

%  A maximal model approach was
% taken for this regression.  The fit was carried out using lasso regression (as described in the methodology) and the $\lambda$ parameter was selected by using ten-fold cross-validation.


<<optreglm, echo=FALSE, results=tex>>=
opt.reg.first <- lm(optimism~(generalhealth+Age+College+pain+mindfulness+socialfunctioning+rolelim+rolelimem+emwellbeing+physfun+energyfat), data=hom1)
opt.reg.sum <- summary(opt.reg.first)
opt.reg.xtab <- xtable(opt.reg.sum, label="tab:optregfirst", caption="Maximal Model for Regression on Optimism")
print(opt.reg.xtab, table.placement="ht")
@


In table \ref{tab:optregfirst} the summary of the regression results can be seen, the model is significant, with an $F$ value of \Sexpr{round(opt.reg.sum[["fstatistic"]],3)}  and an $R^2$ of \Sexpr{round(opt.reg.sum[["adj.r.squared"]],3)}.  


Below, in Table \ref{tab:optregfinal} can be seen the final model,
including all significant predictor variables.

<<optregfinal, echo=FALSE, results=tex>>=
opt.part <- createMultiFolds(na.omit(hom1$optimism), k=10)
optfolds <- TrainTestSets(opt.part, hom1)
opt.reg.final <- lm(optimism~generalhealth+mindfulness+emwellbeing+Age, data=hom1)
opt.reg.fin.stand <- standardize(opt.reg.final)
opt.fin.sum <- summary(opt.reg.final)
opt.fin.xtab <- xtable(opt.fin.sum, label="tab:optregfinal", caption="Final Regression Model for Optimism")
print(opt.fin.xtab , table.placement="ht")
@


The model was significant, F(15, 528) = \Sexpr{round(opt.fin.sum[["fstatistic"]], 3)}, and the adjusted $R^2$ was equal to \Sexpr{round(opt.fin.sum[["adj.r.squared"]],3)}
The model seems to show that the only important predictors of Optimism in the dataset are General Health, Mindfulness, Emotional Well Being and Age.  This corroborates the plots that were shown above for these variables. The strongest predictor was emotional well being, with age and general health being of similiar magnitude. 

The residuals were homoscedastic and normally distributed, meeting the assumptions of the model.





\subsection{Mindfulness Regressions}

A similiar procedure as described above for optimism was employed
in the midfulness regressions.  The results are shown below.

The results of the maximal model are shown in Table \ref{tab:maasfirst} below.

<<maasregfirst, echo=FALSE, results=tex>>=
maas.reg.first <- lm(mindfulness~ generalhealth + optimism + pain+ socialfunctioning+physfun+rolelim+rolelimem+emwellbeing+energyfat+College+Age+Gender+UGPG, data=hom1)
maas.first.sum <- summary(maas.reg.first)
maas.first.xtab <- xtable(maas.first.sum, caption="Maximal Regression Model for Mindfulness", label="tab:maasfirst")
print(maas.first.xtab, table.placement="ht")
@



The model was significant, F(\Sexpr{round(maas.first.sum[["fstatistic"]][[2]], 3)},\Sexpr{round(maas.first.sum[["fstatistic"]][3], 3)}) = \Sexpr{round(maas.first.sum[["fstatistic"]][1],3)} , and the adjusted $R^2$ for the model was equal to \Sexpr{round(maas.first.sum[["adj.r.squared"]], 3)}.  

After eliminating all non significant variables, the final model was as shown in Table \ref{tab:maasmodfin}.

<<maasmodfin, echo=FALSE, results=tex>>=
maas.reg.fin <- lm(mindfulness~optimism+energyfat+emwellbeing+Age+rolelim+rolelimem, data=hom1)
maas.fin.sum <- summary(maas.reg.fin)
maas.fin.xtab <- xtable(maas.fin.sum, label="tab:maasmodfin", caption="Final Regression Model for Mindfulness")
print(maas.fin.xtab, table.placement="ht")
@


The model was significant, F(7, 557) = \Sexpr{round(maas.fin.sum[["fstatistic"]],3)} , while the adjusted $R^2$ was equal to \Sexpr{round( maas.fin.sum[["adj.r.squared"]], 3)} . The residuals of the model were plotted, and they revealed no significant failures of the assumptions. 




\subsection{Health Regressions}


<<healthregfirst, echo=FALSE, results=tex>>=
health.mod1<-lm(generalhealth~mindfulness+physfun+optimism+energyfat+emwellbeing+College+Gender+Age+pain+Status+socialfunctioning+rolelim+rolelimem, na.action="na.omit", data=hom1)
health.sum1<-summary(health.mod1)
health1.xtab<-xtable(health.sum1,label="tab:healthregfirst", caption="Maximal Regression Model for Health")
print(health1.xtab, table.placement="ht")
@



The model (shown in Table \ref{tab:healthregfirst}) was significant,F(\Sexpr{round(health.sum1[["fstatistic"]][2], 3)},\Sexpr{round(health.sum1[["fstatistic"]][3], 3)}) = \Sexpr{round(health.sum1[["fstatistic"]][1], 3)} , and the adjusted $R^2$ for the model was equal to \Sexpr{round(health.sum1[["adj.r.squared"]], 3)}.  


<<healthmodfin, echo=FALSE, results=tex>>=
health.modfin<-lm(generalhealth~physfun+optimism+energyfat+pain, na.action="na.omit", data=hom1)
health.sumfin<-summary(health.modfin)
healthfin.xtab<-xtable(health.sumfin, label="tab:healthfinal", caption="Final Regression Model for General Health")
print(healthfin.xtab, table.placement="ht")
@

The following predictors were
retained in the final model (shown in Table \ref{tab:healthfinal}
below), which had a marginally better adjusted R-squared (R-squared=.3049)
than the first model.

The model was significant,F(\Sexpr{round(health.sumfin[["fstatistic"]][2], 3)},\Sexpr{round(health.sumfin[["fstatistic"]][3], 3)}) = \Sexpr{round(health.sumfin[["fstatistic"]][1], 3)} , and the adjusted $R^2$ for the model was equal to \Sexpr{round(health.sumfin[["adj.r.squared"]], 3)}.  
Plotting of the residuals showed no significant failures of the model assumptions. 



\section{Psychometric Analyses}

\subsection{Number of Factors to retain}

<<scaleitems, echo=FALSE, results=hide>>=
## rand.grep <- grep("^RAND", x=names(hom1))
## randitems <- hom1[,rand.grep]
maas.grep <- grep("^MAASQ", x=names(hom1))
maasitems <- hom1[,maas.grep]
lotr.grep <- grep("^LOTRQ", x=names(hom1))
lotritems <- hom1[,lotr.grep]
@
<<retainfactors, echo=FALSE, results=tex>>=
sink("tmp.txt")
rand.nscree <- nScree(x=cor(na.omit(randitems.scored), use="pairwise.complete.obs"), model="factors")
sink(NULL)
print(xtable(summary(rand.nscree), label="tab:randretain", caption="Comparison of Criteria to retain factors"))

@

As shown in Table \ref{tab:randretain}, the optimal co-ordinates criteria and the acceleration curve suggest that there are two factors, while the parallel analysis criterion and the kaiser criterion suggest eight factors. These latter two measures may be picking up on the higher order factor structure of the items, as the RAND is typically modelled as having two higher order factors (physical and mental health).

Another approach which can be applied to select the number of factors is a cross-validation method. As discussed in the Methodology, there are two main approaches here, either item-subject CV (known as Gabriel\'s method) or the typical ten fold validation common in  machine learning (Wold\'s method). Both of these approaches were examined for the RAND items, and the results are shown below. In the gabriel approach, a leave two of item and subject out was used, while ten fold cross-validation was used for the Wold method. 

<<randfactorcv, echo=FALSE, results=hide>>=
rand.fact.gabriel <- cv.svd.gabriel(na.omit(randitems.scored), krow=2, kcol=2, maxrank=18)
rand.fact.wold <- cv.svd.wold(na.omit(randitems.scored), k=10, maxrank=12)
@ 

<<randcvwold, echo=FALSE, results=hide>>=
print(Svdcv(rand.fact.wold, label="tab:randfactwold", caption="Estimate of number of factors to retain based on 10-fold Cross Validation"))
@ 

It can be seen from Table \ref{tab:randfactwold} that the Wold method of cross validation suggests that four factors should be retained for further analysis. This seems in line with the results of the other criteria.



<<randcvgabriel, echo=FALSE, results=hide>>=
print(Svdcv(rand.fact.gabriel, label="tab:randfactgabriel", caption="Estimate of number of factors to retain based on leave out 2 item/subject Cross-validation"))
@

It can be seen from Table \ref{tab:randfactgabriel} that the Gabriel method of cross-validation suggests that there are thirteen factors which underlie this scale. This seems relatively implausible, but it will be tested. 

To summarise, two, four, eight and thirteen factor structures will be examined for the RAND MOS and their performance assessed on unseen data (from sample 2) to determine which of these provides the best fit. 

Next, the various metrics for determining the number of factors were applied to the Mindfulness scale. 

<<retainmaas, echo=FALSE,results=hide >>=
sink("tmp.txt")
maas.nscree <- nScree(x=cor(na.omit(maasitems), use="pairwise.complete.obs"), model="factors")
sink(NULL)
@ 


<<printretainmaas, echo=FALSE, results=tex>>=
print(xtable(summary(maas.nscree), label="tab:maasretain", caption="Comparison of Criteria to retain factors, MAAS"))
@ 

Looking at the eigenvalues, it is clear that one factor explains the majority of the variance. However, both the Kaiser criterion and parallel analysis suggest that three factors should be retained, while the VSS and the Minimum Average Partial criterion procedures suggest a one factor solutions.

<<maasfactorcv, echo=FALSE, results=tex>>=
maasitems <- as.data.frame(lapply(maasitems, as.numeric))
maas.fact.gabriel <- cv.svd.gabriel(na.omit(maasitems), krow=2, kcol=2, maxrank=10)
maas.fact.wold <- cv.svd.wold(na.omit(maasitems), k=10, maxrank=7)
@


<<maascvwold, echo=FALSE, results=tex>>=
print(Svdcv(maas.fact.wold, label="tab:maasfactwold", caption="Estimate of number of factors to retain based on 10-fold Cross Validation"))
@ 

The Gabriel method of cross-validation, (shown in Table \ref{tab:maasfactwold}), agrees with the other criteria in suggesting that one factor should be retained. 


<<maascvgabriel, echo=FALSE, results=tex>>=
print(Svdcv(maas.fact.gabriel, label="tab:maasfactgabriel", caption="Estimate of number of factors to retain based on leave out 2 item/subject Cross-validation"))
@ 

It can be seen from Table \ref{tab:maasfactgabriel} that the Wold method of cross-validation suggests that there are seven factors which underlie this scale. This seems relatively implausible, but it will be tested. 

Therefore, one, three and seven factor solutions will be examined and their performance assessed on unseen data. 


<<retainlotr, echo=FALSE,results=tex >>=
lotr.nscree <- nScree(x=cor(na.omit(lotritems), use="pairwise.complete.obs"), model="factors")
print(xtable(summary(lotr.nscree), label="tab:lotrretain", caption="Comparison of Criteria to retain factors, LOTR"))
@ 

As shown in Table \ref{tab:lotrretain},all of the methods for determining the correct number of factors to retain suggest that a one factor solution fits the data matrix best. 

<<lotrfactorcv, echo=FALSE, results=tex>>=
lotritems <- as.data.frame(lapply(lotritems, as.numeric))
lotr.fact.gabriel <- cv.svd.gabriel(na.omit(lotritems), krow=3, kcol=3, maxrank=3)
lotr.fact.wold <- cv.svd.wold(na.omit(lotritems), k=10, maxrank=3)
@


<<lotrcvwold, echo=FALSE, results=tex>>=
print(Svdcv(lotr.fact.wold, label="tab:lotrfactwold", caption="Estimate of number of factors to retain based on 10-fold Cross Validation"))
@ 

The Wold method of cross-validation, (shown in Table \ref{tab:lotrfactwold}), agrees with the other criteria in suggesting that one factor should be retained. 


<<lotrcvgabriel, echo=FALSE, results=tex>>=
print(Svdcv(lotr.fact.gabriel, label="tab:lotrfactgabriel", caption="Estimate of number of factors to retain based on leave out 2 item/subject Cross-validation"))
@ 

It can be seen from Table \ref{tab:lotrfactgabriel} that the Gabriel method of cross-validation suggests that there are three factors which underlie this scale. This seems relatively implausible, but it will be tested. 

Therefore, one and three factor solutions will be assessed for the LOT-R, and their performance evaluated on unseen data. 

<<mapall, echo=FALSE, results=hide>>=
MAP.rand<-VSS(na.omit(randitems.scored), n=12, rotate="oblimin", fm="ml", plot=FALSE )
MAP.maas<-VSS(na.omit(maasitems),rotate="oblimin", fm="ml", plot=FALSE )
MAP.lotr<-VSS(na.omit(lotritems),rotate="promax", fm="gls", plot=FALSE )
@

%As can be seen from Figure ~\ref{fig:rand1}, the 8 factor structure was replicated.  However, the MAP criterion suggests a four factor solution, so both of these proposed solutions were examined and tested.




\subsection{RAND MOS}

Two, four, eight and thirteen factor solutions were extracted and interpreted from the RAND MOS items. 

<<rand2fact, echo=FALSE, results=tex>>=
rand.fact.2<-fa(na.omit(randitems.scored), 2,fm="ml", rotate="promax")
print(FactorXtab(rand.fact.2, label="tab:rand2fact", caption="Factor Loadings, RAND MOS Two Factor Solution, Sample One"))
@

PA1: "RANDQ1"  "RANDQ14" "RANDQ16" "RANDQ17" "RANDQ18" "RANDQ19" "RANDQ20" "RANDQ21" "RANDQ22" "RANDQ23" "RANDQ24" "RANDQ25" "RANDQ26" "RANDQ27"
"RANDQ28" "RANDQ29" "RANDQ30" "RANDQ31" "RANDQ32" "RANDQ33" "RANDQ34"
"RANDQ35" "RANDQ36". Essentially this factor appears to contain all of the scales except for Physical Functioning, which loads on Factor 2. We can best term this factor as General and Emotional Health.

PA2:"RANDQ3"  "RANDQ4"  "RANDQ5"  "RANDQ6"  "RANDQ7"  "RANDQ8"  "RANDQ9"  "RANDQ10" "RANDQ11" "RANDQ12". This factor maps exactly to the Physical Functioning Scale, and so retains that name.



The non-normed fit index was equal to \Sexpr{round(rand.fact.2$TLI, 3)}
and the RMSEA was equal to \Sexpr{round(rand.fact.2$RMSEA[1],3 )},with confidence intervals from \Sexpr{round(rand.fact.2$RMSEA[2],3 )} to \Sexpr{round(rand.fact.2$RMSEA[3],3 )}.

This factor solution does not appear to be useful, as it has extremely low fit indices (NNFI=0.69), and the breakdown of the factors is rather strange. If the factors had broken down in terms of Physical and Mental Health, then this would have made more sense. The factor loadings were invariant under a number of rotations (varimax, oblimin and promax), so it appears to be a real (if less than interpretable) factor structure. 

<<rand2corr, echo=FALSE, results=tex>>=
print(xtable(rand.fact.2$r.scores, label="tab:tcq1rand4corr", caption="Factor Correlations, RAND MOS Two Factor Solution, Sample One"))
@

It can be seen from Table \ref{tab:tcq1rand4corr} that the factor correlations were quite low for this solution, at 0.17. This suggests that an orthogonal rotation might be more appropriate, but attempting this did not change any of the loadings. 



\paragraph{Rand MOS 4 Factor Solution}

<<rand4fact, echo=FALSE, results=tex>>=
rand.fact.4<-factor.pa(na.omit(randitems.scored), 4, rotate="oblimin")
print(FactorXtab(rand.fact.4, label="tab:rand4fact", caption="Four Factor Solution, RAND MOS, Sample One (Oblimin Rotation)"))
@

The loadings on the four factor solution broke down as follows. 
PA2: "RANDQ3"  "RANDQ4"  "RANDQ5"  "RANDQ6"  "RANDQ7"  "RANDQ8"  "RANDQ9"  "RANDQ10" "RANDQ11" "RANDQ12". This factor maps exactly to the Physical Functioning scale, and so retains that name. 

PA1: "RANDQ17" "RANDQ18" "RANDQ19" "RANDQ20" "RANDQ23" "RANDQ24" "RANDQ25" "RANDQ26" "RANDQ27" "RANDQ28" "RANDQ29" "RANDQ30" "RANDQ31" "RANDQ32". This factor maps to the emotional role limitations, social functioning and emotional well being scales, and so can probably best be termed as Social and Emotional Functioning. 

PA3: "RANDQ13" "RANDQ14" "RANDQ15" "RANDQ16" "RANDQ21" "RANDQ22". This factor maps to the role limitations and pain sub-scales, and so can probably best be termed as physical limitations. 

PA4: "RANDQ1"  "RANDQ3"  "RANDQ21" "RANDQ27" "RANDQ33" "RANDQ34" "RANDQ35" "RANDQ36". This scale maps to the General Health scale, with one item from the Physical Functioning and one item from Energy/Fatigue (27). However, both these items have higher loadings on other factors, and so this factor can probably best be termed as General Health. 



The non-normed fit index was equal to \Sexpr{round(rand.fact.4$TLI, 3)}
and the RMSEA was equal to \Sexpr{round(rand.fact.4$RMSEA[1],3 )},with confidence intervals from \Sexpr{round(rand.fact.4$RMSEA[2],3 )} to \Sexpr{round(rand.fact.4$RMSEA[3],3 )}.

The fit indices are somewhat better for this solution than for the two factor solution, though the NNFI is still quite low. The RMSEA is somewhat too high for comfort, also. 

<<rand4corr, echo=FALSE, results=tex>>=
print(xtable(rand.fact.4$r.scores, label="tab:tcq1rand4corr", caption="Factor Correlations, RAND MOS Four Factor Solution, Sample One"))
@

It can be seen from Table \ref{tab:tcq1rand4corr} that the factor correlations are quite low in this solution also, though they are all high enough to retain the oblique rotations.




\paragraph{RAND MOS 8 Factor Solution}

<<rand8fact, echo=FALSE, results=tex>>=
rand.fact.8<-factor.pa(na.omit(randitems.scored), 8, rotate="oblimin")
print(FactorXtab(rand.fact.8, label="tab:tcq1rand8fact", caption="factor Loadings Eight Factor Solution, RAND MOS, Sample One"))
@

PA2: "RANDQ3"  "RANDQ4"  "RANDQ5"  "RANDQ6"  "RANDQ7"  "RANDQ8"  "RANDQ9"  "RANDQ10" "RANDQ11" "RANDQ12". Again, the first factor extracted maps exactly to the Physical Functioning scale, and retains that name. 

PA1: "RANDQ20" "RANDQ23" "RANDQ24" "RANDQ25" "RANDQ26" "RANDQ27" "RANDQ28" "RANDQ30" "RANDQ32". This scale maps to the Social Functioning, and the Emotional Well Being Scale. There are some items taken from the energy faitigue scale, and as these are the positively worded items, this scale can probably best be termed as Social and Emotional Well Being. 

PA4: "RANDQ1"  "RANDQ33" "RANDQ34" "RANDQ36". These items map exactly to the General Health scale. Item 35 also loads on this scale, and as its loading was 0.29 while the cutoff was 0.30, it can be best characterised as part of that scale. Therefore, this factor can be best termed as General Health.

PA7: "RANDQ17" "RANDQ18" "RANDQ19" "RANDQ20". This scale maps to the Emotional Role Limitations and one item (20) from the Social Functioning scale (which asks about social events that have been missed due to health problems) and so this can probably best be termed as Emotional Role Limitations.

PA3: "RANDQ13" "RANDQ14" "RANDQ15" "RANDQ16". This scale maps exactly to the Role Limitations sub-scale, and so retains that name. 

PA5: "RANDQ29" "RANDQ31". These items are the negative items from the Energy/Fatigue scale, and so this factor can probably best be termed as Fatigue. 

PA6: "RANDQ21" "RANDQ22". These items map exactly to the Pain scale, and so retain that name. 

PA8: "RANDQ2"  "RANDQ23" "RANDQ27". The loadings on this factor are all below .4, which argues against its unproblematic interpretation. In addition, Q2 loads on this factor, when it is not typically associated with any factor. The other two items are the positively worded items from the  Energy/Fatigue scales, and so this factor can best be termed Energy. 



The non-normed fit index was equal to \Sexpr{round(rand.fact.8[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(rand.fact.8[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(rand.fact.8[["RMSEA"]][2],3 )} to \Sexpr{round(rand.fact.8[["RMSEA"]][3],3 )}.

This factor structure definitely makes sense, and the fit indices are relatively acceptable, although the RMSEA is a little higher than would be wanted. In addition, the items map quite well to the subscales, which further reinforces our confidence in this solution.


<<rand8corr, echo=FALSE, results=tex>>=
print(xtable(rand.fact.8[["r.scores"]], label="tab:hom1rand8corr", caption="Factor Correlations, Eight Factor Solution RAND MOS, Sample One"))
@

The factor correlations are moderate (shown in Table \ref{tab:hom1rand8corr}) and in line with expectations. The correlations are definitely too high to use an othrogonal rotation. 



\paragraph{Rand MOS 13 factor Solution}

<<rand13fact, echo=FALSE, results=tex>>=
rand.fact.13<-factor.pa(na.omit(randitems.scored), 13, rotate="oblimin")
print(FactorXtab(rand.fact.13,label="tab:tcq1rand13fact", caption="Factor Loadings, Thirteen Factor Solution RAND MOS, Sample One (oblimin rotation)"), floating.environment="sidewaystable")
@

PA2: "RANDQ6"  "RANDQ7"  "RANDQ9"  "RANDQ10" "RANDQ11" "RANDQ12". This appears to be the majority of the Physical Functioning scale, and therefore retains that name.


PA8: "RANDQ17" "RANDQ18" "RANDQ19" "RANDQ20". This consists of the three Emotional Role Limitations items, and a low loading on the negatively worded social functioning scale. It can therefore be named Emotional Role Limitations.

PA3: "RANDQ13" "RANDQ14" "RANDQ15" "RANDQ16". This factor maps exactly to the Role Limitations scale, and thus retains that name. 

PA7: "RANDQ23" "RANDQ26" "RANDQ27" "RANDQ30". These items map to the positive questions of Emotional Well Being and Energy/Fatigue, and therefore can best be termed as Positive Emotionality. 

PA11: "RANDQ3" "RANDQ4" "RANDQ5" "RANDQ8". These items are all from the Physical Functioning scale, and appear to all relate to relatively heavy exertions. This factor can therefore be termed Physical Exertion.

PA4: "RANDQ1"  "RANDQ34" "RANDQ36". These items all relate to health and are all framed in a positive way. Therefore this factor can be termed Positive Health.

PA5:"RANDQ29" "RANDQ31". These are the negatively framed items from the Energy/Fatigue scale, and can be probably best be termed Fatigue. 

PA6: "RANDQ21" "RANDQ22". These items map exactly to the Pain Scale, and thus retain that name. 

PA1: "RANDQ24" "RANDQ25" "RANDQ28" "RANDQ30". These items mostly relate to the emotional well being scale, and are almost all negatively framed. It can probably best be termed Emotional Problems.


PA10: "RANDQ3" "RANDQ6". These items relate to vigourous activities and climbing stairs. It can probably best be termed as Vigourous Activity.

PA9: "RANDQ33". 

PA12: "RANDQ20"

PA13: "RANDQ24"

The last three factors have only one item loading on them, and three is normally regarded as the minimum for a factor to replicate  %cite Tabachnick & Fidell here.
Therefore we can stop the interpretation here, as this factor does not seem to add much to our understanding of the scale. 

<<rand13corr, echo=FALSE, results=tex>>=
print(xtable(rand.fact.13[["r.scores"]],label="tab:hom1rand13corr", caption="Factor Correlations, Thirteen Factor RAND MOS Solution, Sample One"), scalebox=0.6)
@
As Table \ref{tab:hom1rand13corr} shows, the factors are inter-correlated, but not in any coherent fashion. 





The non-normed fit index was equal to \Sexpr{round(rand.fact.4[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(rand.fact.4[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(rand.fact.4[["RMSEA"]][2],3 )} to \Sexpr{round(rand.fact.4[["RMSEA"]][3],3 )}. Although the factor structure from this solution looks unlikely to be useful (given that there are three factors consisting only of one item each), the fit indices indicate that this is a better solution than any of the other proposed structures. That being said, this structure is unlikely to replicate, due to presumed overfitting. 


\paragraph{CFA for RAND MOS}


<<rand2sem, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "General and Emotional Health")
physfun <- paste(rand, c(3:12), sep="")
genemhealth <- paste(rand,c(1,14,17:36), sep="")
Rand2model <- mxModel(name="RAND2Samp1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="General and Emotional Health", to=genemhealth),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=364)
                      )
rand2fit <- mxRun(Rand2model)
rand2summ <- summary(rand2fit)
@

<<rand4sem, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "Social and Emotional Functioning", "General Health", "Physical Limitations")
physfun <- paste(rand, c(3:12), sep="")
genhealth <- paste(rand, c(1,33:36), sep="")
physlim <- paste(rand, c(13:16,21:22), sep="")
emsocfunc <- paste(rand, c(17:20, 23:32), sep="")
Rand4model <- mxModel(name="RAND4Samp1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="Social and Emotional Functioning", to=emsocfunc),
                      mxPath(from="General Health", to=genhealth),
                      mxPath(from="Physical Limitations", to=physlim),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=364)
                      )
rand4fit <- mxRun(Rand4model)
rand4summ <- summary(rand4fit)
@

<<rand8sem, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "Social and Emotional Wellbeing", "General Health", "Emotional Role Limitations", "Role Limitations", "Fatigue", "Pain", "Energy")
physfun <- paste(rand, c(3:12), sep="")
socemwellbeing <- paste(rand, c(20,23:28, 30, 32), sep="")
genhealth <- paste(rand, c(1,33:36), sep="")
emrolelim <- paste(rand, c(17:20), sep="")
rolelim <- paste(rand, c(13:16), sep="")
energy <- paste(rand, c(29, 31), sep="")
pain <- paste(rand, c(21:22), sep="")
fatigue <- paste(rand, c(2, 23, 27), sep="")
Rand8model <- mxModel(name="RAND8Samp1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="Social and Emotional Wellbeing", to=socemwellbeing),
                      mxPath(from="General Health", to=genhealth),
                      mxPath(from="Emotional Role Limitations", to=emrolelim),
                      mxPath(from="Role Limitations", to=rolelim),
                      mxPath(from="Energy", to=energy),
                      mxPath(from="Pain", to=pain),
                      mxPath(from="Fatigue", to=fatigue),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=364)
                      )
rand8fit <- mxRun(Rand8model)
rand8summ <- summary(rand8fit)
@

<<rand13sem, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning","Emotional Role Limitations","Role Limitations", "Positive Emotionality", "Physical Exertion", "Positive Health","Fatigue","Pain", "Emotional Problems", "Sickness", "Vigorous Activity","RAND20", "RAND24" )
physfun <- paste(rand, c(3:12), sep="")
emrolelim <- paste(rand, c(17:20), sep="")
rolelim <- paste(rand, c(13:16), sep="")
posemotions <- paste(rand, c(23,26,27,30), sep="")
physexertions <- paste(rand, c(3,4:5, 8), sep="")
poshealth <- paste(rand, c(1,34, 36), sep="")
fatigue <- paste(rand, c(29, 31), sep="")
pain <- paste(rand, c(21:22), sep="")
emproblems <- paste(rand, c(24, 25, 28, 30), sep="")
sickness <- paste(rand, 33, sep="")
vigactivity <- paste(rand, c(3,6), sep="")
rand20 <- "RANDQ20"
rand24 <- "RANDQ24"
Rand13model <- mxModel(name="RAND13Samp1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                       mxPath(from="Emotional Role Limitations", to=emrolelim),
                       mxPath(from="Role Limitations", to=rolelim),
                       mxPath(from="Positive Emotionality", to=posemotions),
                      mxPath(from="Physical Exertion", to=physexertions),
                       mxPath(from="Positive Health", to=poshealth),
                       mxPath(from="Pain", to=pain),
                       mxPath(from="Fatigue", to=fatigue),
                       mxPath(from="Emotional Problems", to=emproblems),
                       mxPath(from="Vigorous Activity", to=vigactivity),
                      mxPath(from="Sickness", to=sickness),
                      mxPath(from="RANDQ20", to=rand20),
                       mxPath(from="RANDQ24", to=rand24),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=364)
                      )
rand13fit <- mxRun(Rand13model)
rand13summ <- summary(rand13fit)
@ 

<<randsemcompare, echo=FALSE, results=tex>>=
randsemcomp <- mxCompare(base=rand2fit, comparison=c(rand4fit, rand8fit, rand13fit), all=TRUE)
print(xtable(randsemcomp,label="tab:randsemcompare", caption="SEM Comparison for RAND MOS Factor Solutions, Sample One"), scalebox=0.8)
@

As can be seen from \ref{tab:randsemcompare}, the 8 factor solution appears to fit better (lower AIC), so on the basis of this analysis, this is the solution which should be retained.

\section{Mindfulness Attention Awareness Scale}

For the MAAS, parallel analysis, MAP, VSS, Kaiser\'s rule and the Wold method of cross-validation suggested a one factor solution, while the Gabriel method of cross-validation suggested a seven factor solution.  Therefore, one and seven factor solutions were extracted and the results interpreted, as shown below.

\subsection{MAAS One Factor Solution}
\label{sec:maas-one-factor}



<<maas1fact, echo=FALSE, results=tex>>=
maas.fact.1<-factor.pa(na.omit(maasitems), 1, rotate="oblimin")
print(FactorXtab(maas.fact.1,label="tab:hom1maas1fact", caption="Factor Loadings, One Factor Solution, MAAS, Sample One"))
@

The results of the one factor solution for the MAAS are shown in Table \ref{tab:hom1maas1fact}. 

The non-normed fit index was equal to \Sexpr{round(maas.fact.1[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(maas.fact.1[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(maas.fact.1[["RMSEA"]][2],3 )} to \Sexpr{round(maas.fact.1[["RMSEA"]][3],3)}.



\subsection{MAAS Seven Factor Solution}
\label{sec:maas-seven-factor}

The next factor solution to be examined was the seven factor solution suggested by the Gabriel method of cross-validation.
<<maas7fact, echo=FALSE, results=tex>>=
maas.fact.7<-factor.pa(na.omit(maasitems), 7, rotate="oblimin")
print(FactorXtab(maas.fact.7,label="tab:tcq1maas7fact", caption="Factor Loadings, Seven Factor Solution, MAAS, Sample One"))
@



PA1: "MAASQ9"  "MAASQ10" "MAASQ11" "MAASQ12".These items all relate to automatic behaviour, and this factor can possibly best be termed as Automatic Behaviour.

PA3:"MAASQ13". This factor is unlikely to be useful, as it consists only of one item. The item relates to preoccupations, and so is given that name. 


PA7: "MAASQ7" "MAASQ8". These items both relate to rushing through activities, and can probably best be termed as Hurry. 

PA2: "MAASQ2" "MAASQ3". These items can probably best be termed as lack of present focus.

PA4: "MAASQ15". This item relates to food, and so this factor can best be termed as Food Mindlessness. 

PA5: "MAASQ6". This item can best be termed as Inattention.

PA6:"MAASQ1" "MAASQ4" "MAASQ5" "MAASQ9". These items can probably best be termed as lack of awareness. 

The non-normed fit index was equal to \Sexpr{round(maas.fact.7[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(maas.fact.7[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(maas.fact.7[["RMSEA"]][2],3 )} to \Sexpr{round(maas.fact.7[["RMSEA"]][3],3)}.




\subsection{CFA for MAAS}
\label{sec:cfa-maas}

<<maas7sem, echo=FALSE, results=hide>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Automatic Behaviour", "Preoccupations", "Hurry",  "Lack of Present Focus", "Food Mindlessness", "Inattention", "Lack of Awareness")
automaticbehaviour <- paste(maas, c(9:12), sep="")
preoccupations <- "MAASQ13"
hurry <- paste(maas, c(7:8), sep="")
lackpresfocus <- paste(maas, c(2:3), sep="")
foodmindlessness <- "MAASQ15"
inattention <- "MAASQ6"
lackawareness <- paste(maas, c(1,4:5,9), sep="")
Maas7model <- mxModel(name="MAAS7",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Automatic Behaviour", to=automaticbehaviour),
                      mxPath(from="Preoccupations", to=preoccupations),
                      mxPath(from="Hurry", to=hurry),
                      mxPath(from="Lack of Present Focus", to=lackpresfocus),
                      mxPath(from="Lack of Awareness", to=lackawareness),
                           mxPath(from="Inattention", to=inattention),
                           mxPath(from="Lack of Present Focus", to=lackpresfocus),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems)), type="cov", numObs=364)
                      )
maas7fit <- mxRun(Maas7model)
maas7summ <- summary(maas7fit)
@




<<maas1fit, echo=FALSE, results=hide>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Unawareness")
unawareness <- paste(maas, c(1:15), sep="")
Maas1model <- mxModel(name="MAAS1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Unawareness", to=unawareness),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems)), type="cov", numObs=364)
                      )
maas1fit <- mxRun(Maas1model)
maas1summ <- summary(maas1fit)
@

<<maassemcompare, echo=FALSE, results=tex>>=
maascomp <- mxCompare(base=maas1fit, comparison= maas7fit)
maascomp.xtab <- xtable(maascomp,label="tab:maassemcomp")
print(maascomp.xtab)
                      
@

Factor solutions for one and seven factors were extracted, and the results were subjected to CFA.
The results of the CFA are shown below in Table \ref{tab:maassemcomp}.

From Table \ref{tab:maassemcomp} it can be seen that the best model is the one factor model, which is in line with previous research.
The factor structure is not reported here as all factors loaded on the first factor.  This factor explained 35\% of the variance
in the sample, which is low.  Possible explanations for this are discussed below. 
\subsection{Life Orientation Test, Revised}

Parallel Analysis indicated that two factors should be extracted, while the MAP criterion suggested one.  Therefore, both one and two factor solutions were extracted from the matrix and their results examined for adequacy and interpretability.

\subsection{LOTR One Factor Solution}
\label{sec:lotr-one-factor}


<<lotr1fact, echo=FALSE, results=tex>>=
lotr.fact.1<-factor.pa(na.omit(lotritems), 1, rotate="oblimin")
print(FactorXtab(lotr.fact.1,label="tab:hom1lotr1fact", caption="Factor Loadings, One Factor Solution, LOT-R, Sample One"))
@

Table \ref{tab:hom1lotr1fact} shows the loadings for the one factor solution. The communalities are relatively high, except for question one which is a sign that perhaps this solution is not optimal. 


The non-normed fit index was equal to \Sexpr{round(lotr.fact.1[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(lotr.fact.1[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(lotr.fact.1[["RMSEA"]][2],3)} to \Sexpr{round(lotr.fact.1[["RMSEA"]][3],3 )}.  This solution does not seem optimal, as the RMSEA is well outside the recommended bounds, and the NNFI is quite low.  %% In addition, the communalities are not very high, with over half the variance being left out of the solution.




\subsection{LOTR Three Factor Solution}
\label{sec:lotr-three-factor}

<<lotr3fact, echo=FALSE, results=tex>>=
lotr.fact.3<-factor.pa(na.omit(lotritems), 3, rotate="oblimin")
print(FactorXtab(lotr.fact.3,label="tab:tcq1lotr3fact", caption="Factor Loadings, Three Factor Solution, LOT-R, Sample One"))
@

PA2:"LOTRQ1"  "LOTRQ4"  "LOTRQ10". These items are all the positively framed items, and so this factor can best be termed as Optimism. 

PA1:"LOTRQ3" "LOTRQ7". This factor, and the next, consist of the pessimism items, and so can best be termed Pessimism. 

PA3: "LOTRQ7" "LOTRQ9". This factor can best be termed Pessimism (Reversed). 





\subsection{CFA for LOTR}
\label{sec:cfa-lotr}

<<lotr1sem, echo=FALSE, results=hide>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism")
optimism <- paste(lotr, c(1,3,4,7,9,10), sep="")
Lotr1model <- mxModel(name="LOTR1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems)), type="cov", numObs=364)
                      )
lotr1fit <- mxRun(Lotr1model)
lotr1summ <- summary(lotr1fit)
@


<<lotr3sem, echo=FALSE, results=hide>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism", "Pessimism", "Pessimism2")
optimism <- paste(lotr, c(1,4,10), sep="")
pessimism <- paste(lotr, c(3,7), sep="")
pessimism2 <- paste(lotr, c(7,9), sep="")
Lotr3model <- mxModel(name="LOTR3",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from="Pessimism", to=pessimism),
                      mxPath(from="Pessimism2", to=pessimism2),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems)), type="cov", numObs=364)
                      )
lotr3fit <- mxRun(Lotr3model)
lotr3summ <- summary(lotr3fit)
@

<<lotrcompare, echo=FALSE, results=hide>>=
lotrcomp <- mxCompare(base=lotr1fit, comparison=lotr3fit)
lotrcomp.xtab <- xtable(lotrcomp,label="tab:semlotrcomp", caption="Comparison of CFA for the LOT-R")
print(lotrcomp.xtab)
@

As can be seen from Table \ref{tab:semlotrcomp}, the one factor solution provided the best fit to the data.  Therefore, this solution will be tested on the second sample.

 \section{Item Response Theory Analyses}
\label{sec:item-response-theory}

In addition to the classical test theory analyses carried out, each scale was also subjected to item response theory analyses.  The first step in this process was to use Mokken scaling to test the assumptions required for item response theory modelling (as described in the methodology section).

<<loadirtpackages, echo=FALSE, results=hide>>=
require(mokken)
require(eRm)
require(ltm)
@

<<randcheckassumptions, echo=FALSE, results=hide>>=
rand.scales <- aisp(na.omit(randitems))
@

<<randscales, echo=FALSE, results=tex>>=
print(xtable(rand.scales,label="tab:randscales", caption="Item Selection Procedure Results, RAND MOS Sample One"))
@
As can be seen from Table \ref{tab:randscales}, the item selection procedure suggests that there are five scales in the RAND MOS.  


The scales can be termed as follows:

Scale 1: RANDQ3-RANDQ12: This maps exactly to the physical functioning scale.

Scale 2: RANDQ16-RANDQ20,24,25,28,29,31,32. This scale incorporates the Role Limitations, Social Functioning, Emotional Well Being and Energy scales, and can best be termed as Mental/Emotional Health.

Scale 3: RAND1, RAND20-23,26,27,30,34,36. This scale incorporates the pain scale, items from social functioning, emotional well being and general health. It can probably best be termed as Energy.

Scale 4: RAND13-RAND16. This maps to the role limitations scale, and can probably best be termed as physical limitations.

Scale 5:  RANDQ33, RANDQ35. This maps to the negatively phrased items from the General Health scale, and can best be termed Sickness. However, this scale will not be analysed as there are not enough items to allow for the analytic procedure to work.

<<randirtscales, echo=FALSE, results=hide>>=
irtphysfun <- randitems[,paste(rand, c(3:12), sep="")]
irtemhealth <- randitems[,paste(rand, c(16:20,24,25,28,29,31,32), sep="")]
irtenergy <-  randitems[,paste(rand, c(1,20:23,26:27,30,34,36), sep="")]
irtphyslim <- randitems[,paste(rand, c(13:16), sep="")]
irtsickness <- randitems[,paste(rand, c(33,35), sep="")]
@

<<randphysfun, echo=FALSE, results=hide>>=
irtphys.item.ord <- check.iio(na.omit(irtphysfun))
irtphys.monotonicity <- check.monotonicity(na.omit(irtphysfun))
@


<<randphysfuncitemord, echo=FALSE, results=tex>>=
print(xtable(irtphys.item.ord[["violations"]],label="tab:randphysfuncitemord", caption="IRT Physical Functioning Item Ordering Results"))
@

As can be seen from Table \ref{tab:randphysfuncitemord}, there were no violations of item ordering for this sample.  This demonstrates that there are no violations of monotonicity for this subscale, and suggests that item response theory modelling can proceed, for this scale at least.

<<randemhealth, echo=FALSE, results=hide>>=
irtemhealth.item.ord <- check.iio(na.omit(irtemhealth))
irtemhealth.monotonicity <- check.monotonicity(na.omit(irtemhealth))
@

<<randEmhealthItemord, echo=FALSE, results=tex>>=
print(xtable(irtemhealth.item.ord[["violations"]],label="tab:randemhealthitemord", caption="IRT Emotional Health Scale Item Ordering Results"))
@

As can be seen from Table \ref{tab:randemhealthitemord}, quite a number of items failed the item ordering assumption (RAND20, RAND32) and so were removed from the scale before further analysis. Interestingly enough, these items represent the Social Functioning scale, which may suggest that this scale is not valid from an IRT point of view.  

<<newIrtEmHealth, echo=FALSE, results=tex>>=
irtemhealth2 <- randitems[,paste(rand, c(16:19,25,28,29,31), sep="")]
emhealth.item.ord2 <- check.iio(na.omit(irtemhealth2))
emhealth.mono.2 <- check.monotonicity(na.omit(irtemhealth2))
print(xtable(emhealth.item.ord2[["violations"]], caption="IIO for reduced emotional health scale", label="tab:randemhealth2iio"))
@

As shown in Table \ref{tab:randemhealth2iio}, the removal of items from the emotional health scale appears to have removed the problems with invariant item ordering. The reduced scale had no issues with non-monotonic item responses. 



<<randphyslim, echo=FALSE, results=hide>>=
irtphyslim.item.ord <- check.iio(na.omit(irtphyslim))
irtphyslim.monotonicity <- check.monotonicity(na.omit(irtphyslim))
@

The IIO and monotonicity assumptions were checked for the physical limitations scale, and there were no violations. 



<<randgenhealth, echo=FALSE, results=hide>>=
irtenergy.item.ord <- check.iio(na.omit(irtenergy))
irtenergy.monotonicity <- check.monotonicity(na.omit(irtenergy))
irtenergy2 <- randitems[,paste(rand, c(1,20,22,26,30,34), sep="")]
@

There were two violations of the item ordering assumption for the Energy scale, RAND21 and RAND23, and these items were removed before further analyses.

<<randGenhealthItemord, echo=FALSE, results=tex>>=
print(xtable(irtenergy.item.ord[["violations"]],label="tab:randenergyitemord", caption="IRT RAND Energy Scale, Invariant Item Ordering Results"))
@


<<physfunrasch, echo=FALSE, results=hide>>=
physfun.gpcm.rasch <- gpcm(na.exclude(irtphysfun), constraint="rasch")
physfun.gpcm.1pl <- gpcm(na.exclude(irtphysfun), constraint="1PL")
physfun.gpcm.gpcm <- gpcm(na.exclude(irtphysfun), constraint="gpcm")
@

<<physfunraschprint, echo=FALSE, results=tex>>=
print(xtable(coef(physfun.gpcm.rasch),label="tab:physfunrasch", caption="Coefficient Estimates, Rasch Partial Credit Model, Sample One"))
@

As can be seen from Table \ref{tab:physfunrasch}, the Rasch model was not a good fit to these items, as RANDQ7 and RANDQ12 showed non increasing item difficulty estimates. 

<<physfunraschprint, echo=FALSE, results=tex>>=
print(xtable(coef(physfun.gpcm.1pl),label="tab:randphysfun1pl", caption="Coefficient Estimates, Rand MOS One Parameter Partial Credit Model, Sample One"))
@ 

As shown in Table \ref{tab:randphysfun1pl}, the one parameter model also proved a poor fit to the data, as RANDQ7 and RANDQ12 still show problems with the ordering of abilities. 

<<physfungpcmprint, echo=FALSE, results=tex>>=
  print(xtable(coef(physfun.gpcm.gpcm),label="tab:randphysfungpcm", caption="Coefficient Estimates for RAND Physical Functioning IRT Scale, Two Parameter Partial Credit Model, Sample One"))
@ 

Table \ref{tab:randphysfungpcm} shows that RANDQ12 is still a problematic item for the two parameter model, though RANDQ7 does not show the problems seen earlier. The next step in the modelling process was to fit a one and two parameter GRM.  



<<physfungrm1, echo=FALSE, results=hide>>=
physfun.grm.1pl <- grm(irtphysfun, constrained=TRUE)
physfun.grm.2pl <- grm(irtphysfun, constrained=FALSE)
@ 

<<physfungrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(physfun.grm.1pl), label="tab:physfungrm1", caption="Coefficient Estimates for One Parameter Graded Response Model, IRT Physical Functioning Scale"))
@ 

As can be seen from Table \ref{tab:physfungrm1}, there appear to be no problems with the fit of this model as all of the coefficients are monotonically increasing. Next, a two parameter GRM was fit to this scale. 


<<physfungrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(physfun.grm.2pl), label="tab:randphysfungrm2pl", caption="Coefficient Estimates for One Parameter Graded Response Model, IRT Physical Functioning Scale"))
@ 


From Table \ref{tab:randphysfungrm2pl}, it can be seen that the allowing the discrimination parameter to vary freely makes a large difference to the coefficient estimates. Note that Q7 appears to be the most discriminating question, with a slope of 5.64. The next process was to assess if the two parameter model provided a significant improvement in fit over and above the one parameter model.

The results of this ANOVA showed that the two parameter model was a much better fit to the data ($p \le 0.001$). 

<<grmanova, echo=FALSE, results=hide>>=
grmanova <- anova(physfun.grm.1pl, physfun.grm.2pl)
@ 

However, a better predictor of the usefulness of a model is to examine its fit on unseen data, and this was done in Section \ref{sec:predictions}. 

<<emhealthPCM, echo=FALSE, results=hide>>=
emhealth.pcm.rasch <- gpcm(irtemhealth2, constraint="rasch")
emhealth.pcm.1PL <- gpcm(na.omit(irtemhealth2), constraint="1PL")
emhealth.pcm.gpcm <- gpcm(na.omit(irtemhealth2), constraint="gpcm")
@


<<emhealthRasch, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(emhealth.pcm.rasch)),label="tab:hom1emhealthrasch", caption="Coefficient Estimates for Emotional Health IRT Scale, Rasch Partial Credit Model, Sample One"))
@



As can be seen from Table \ref{tab:hom1emhealthrasch}, the rasch model does not provide a good fit for this data, as shown by the numerous failures of the monotonicity assumption.  Therefore, the next step was to fit a more flexible model, the coefficients of which are shown in Table \ref{tab:emhealth1pl}.  This model kept the constant discrimination parameter, but allowed it to be estimated from the data.

<<emhealth1pl, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(emhealth.pcm.1PL)),label="tab:hom1emhealth1pl", caption="Coefficient Estimates for One Parameter Partial Credit Model, Emotional Health Scale, RAND MOS, Sample One"))
@



In Table \ref{tab:hom1emhealth2pl} can be seen the results of estimating a true two parameter model for this dataset, where the discrimination parameter was estimated seperately for each item.

A likelihood test was carried out between these three models, and the results showed that the true two parameter model provided a much better fit to the data ($p \le 0.001$).  This is not particularly surprising given that it estimates twice as many parameters as the most parsimonious model, and the real test of these model\'s predictive abilities will come when we fit them to unseen data.

Next, one and two parameter Graded Response Models were fit to the data. 

<<hom1emhealthgrm, echo=FALSE, results=hide>>=
hom1.emhealth.grm.1pl <- grm(irtemhealth2, constrained=TRUE)
hom1.emhealth.grm.2pl <- grm(irtemhealth2, constrained=FALSE)
@ 


<<hom1emhealthgrm1plprint, echo=FALSE, results=hide>>=
print(xtable(coef2mat(coef(hom1.emhealth.grm.1pl)), label="tab:hom1memhealthgrm1pl", caption="Coefficient Estimates for One Parameter Graded Response Model, IRT Emotional Health Scale, Sample One"))
@ 

Table \ref{tab:hom1emhealthgrm1pl} shows the coefficient estimates for the one parameter Graded Response Model fitted to the Energy Scale of the RAND MOS. It can be seen that the emotional role limitations questions (16-19) are estimated at quite low ability thresholds, as the sample was non-clinical, and that the questions have a relatively high discrimination parameter, but relatively low ability estimates (as most of the rest of the questions come from the emotional health and energy/fatigue scales). 

Next, a two parameter model was fitted to this scale. 

<<hom1emhealthgrm2plprint, echo=FALSE, results=hide>>=
print(xtable(coef2mat(coef(hom1.emhealth.grm.2pl)), label="tab:hom1emhealthgrm2pl", caption="Coefficient Estimates for Two Parameter Graded Response Model, Emotional Health RAND MOS Scale, Sample One"))
@ 
Table \ref{tab:hom1emhealthgrm2pl} shows the coefficient estimates for the two parameter Graded Response Model, in which all of the signs and ordering of the thresholds have reversed. 



The next scale to be examined is the  Energy scale.  The first analysis undertaken was to fit a partial credit rasch model to the scale.

<<genhealthPCM, echo=FALSE, results=tex>>=
irtenergy.pcm.rasch <- gpcm(na.omit(irtenergy), constraint="rasch")
print(xtable(coef2mat(coef(irtenergy.pcm.rasch)), label="tab:hom1energpcmrasch", caption="Coefficient Estimates for IRT Energy Scale, Rasch Partial Credit Model"))
@


It can be seen from Table \ref{tab:hom1energpcmrasch} that with the exception of Q36, all the items seem appropriately fitting. With Q36, there is no ability level where a response category of four is more probable than any of the other items, which is a failure either of the model or the scale. 




<<genhealthpcm1pl, echo=FALSE, results=tex>>=
irtenergy.pcm.1pl <- gpcm(na.omit(irtenergy), constraint="1PL")
print(xtable(coef2mat(coef(irtenergy.pcm.1pl)), label="tab:hom1energpcm1pl", caption="Coefficients for IRT Energy Scale, One parameter Partial Credit Model"))
@

Table \ref{tab:hom1energpcm1pl} shows the coefficient estimates for the one parameter Partial Credit Model fitted to the energy RAND MOS scale. Suprisingly, the overall discrimination parameter has decreased. The most difficult question is 21, which asks about bodily pain in the past six months. The difficulty of this question would seem to be a function of the sampling procedure (i.e. students) here, rather than a function of the item's properties. Again, Q36 has the problem with the third response category, as does Q22, suggesting that the population is split bimodally on this question, with either high or low responses being more probable than a response in the middle. 


<<genhealthpcm2pl, echo=FALSE, results=hide>>=
irtenergy.pcm.2pl <- gpcm(na.omit(irtenergy), constraint="gpcm")
print(xtable(coef2mat(coef(irtenergy.pcm.2pl)), label="tab:hom1energypcm2pl", caption="Coefficients of IRT Energy Scale PCM, Two Parameter Model"))
@


Table \ref{tab:hom1energpcm2pl} shows the coefficient estimates for the two parameter PCM on this scale. Again, items 36 and 22 have issues with category 3 estimates, and the discrimination parameters are all quite low with over half the scale having discrimination parameters of less than one, meaning that they discriminate less well between high and low ability respondents than would questions meeting the assumptions of a Rasch model. 

<<genhealthpcmcompare, echo=FALSE, results=tex>>=
anova.rasch.1pl <- anova.gpcm(irtenergy.pcm.rasch, irtenergy.pcm.1pl)
anova.1pl.2pl <- anova.gpcm(irtenergy.pcm.1pl, irtenergy.pcm.2pl)
anova.rasch.2pl <- anova.gpcm(irtenergy.pcm.rasch, irtenergy.pcm.2pl)
@

An anova conducted on the three models indicated that the 2 parameter model fit the data best ($p \le 0.001$), subject to the caveats above.

Next, one and two parameter Graded Response Models were fit to the energy scale. 

<<hom1irtenergygrm, echo=FALSE, results=hide>>=
hom1.energy.grm.1pl <- grm(irtenergy, constrained=TRUE)
hom1.energy.grm.2pl <- grm(irtenergy, constrained=FALSE)
@ 


<<hom1energygrm1plprint, echo=FALSE, results=hide>>=
print(xtable(coef2mat(coef(hom1.energy.grm.1pl)), label="tab:hom1energygrm1pl", caption="Coefficient Estimates for One Parameter Graded Response Model on IRT Energy Scale, RAND MOS, Sample One"))
@ 

Table \ref{tab:hom1energygrm1pl} shows the coefficient estimates for the one parameter model on the Energy scale. It can be seen that the Graded Response Model does not have the same issues with category 3 for items 22 and 36, which implies that this issue above was a property of the model rather than the scale. The discrimination parameter is estimated as much higher under this model also, as are the ability thresholds, though the rank ordering remains the same. 

<<hom1energygrm2plprint, echho=FALSE, results=hide>>=
print(xtable(coef2mat(coef(hom1.energy.grm.2pl)), label="tab:hom1energygrm2pl", caption="Coefficient Estimates for Two Parameter Graded Response Model, IRT Energy Scale, RAND MOS, Sample One"))
@ 

Table \ref{tab:hom1energygrm2pl} shows the coefficient estimates for the two parameter Graded Response Model. It can be seen that the discrimination parameters are very different from those of a similar Partial Credit Model (see Table \ref{tab:hom1energygpcm2pl}). Additionally, the ability estimates are much higher on average, with Q22 having the highest ability threshold. 


Next, we assess the usefulness of one and two parameter GRM\'s for the physical limitations scale. 

<<physlimgrm, echo=FALSE, results=hide>>=
hom1.physlim.grm.1pl <- grm(irtphyslim, constrained=TRUE)
hom1.physlim.grm.2pl <- grm(irtphyslim, constrained=FALSE)
@ 

<<physlimgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(hom1.physlim.grm.1pl)), label="tab:hom1physlimgrm1pl", caption="Coefficient Estimates for One Parameter Graded Response Model, Physical Limitations Scale, Sample One"))
@ 

Table \ref{tab:hom1physlimgrm1pl} shows the coefficient estimates for the one parameter Graded Response Model. The discrimination parameter is quite high, suggesting that a positive response on these questions is informative as to a person's ability level. The ability thresholds are quite low, which makes sense given the non-clinical and young nature of the sample. 


The next instrument examined was the MAAS.  Firstly, the instrument was examined using mokken analysis to check if it could be considered one scale, and whether or not there were violation of monotonicity.

<<maasassumptioncheck, echo=FALSE, results=tex>>=
maas.scales <- aisp(na.omit(maasitems))
print(xtable(as.matrix(maas.scales)),label="tab:maasassumptioncheck")
@

As can be seen from Table \ref{tab:maasassumptioncheck}, the mokken analysis suggests that two items should be dropped from the scale, items 2 and 6. This leaves a thirteen item scale for further analysis.

<<maasreduced, echo=FALSE, results=hide>>=
maas.irt <- paste(maas, c(1,3:5,7:15), sep="")
maas.irt <- maasitems[,maas.irt]
@

Next, item ordering was examined for this scale.

<<maasitemord, echo=FALSE, results=tex>>=
maas.iio <- check.iio(na.omit(maas.irt))
print(xtable(maas.iio[["violations"]],label="tab:maasitemord", caption="Invariant Item Ordering Check Results for MAAS"))
@

As can be seen from Table \ref{tab:maasitemord}, there were no violations of item ordering with the reduced scale.

Next, monotonicity was examined for the reduced scale.

<<maasmonotonicity, echo=FALSE, results=tex>>=
maas.mono <- check.monotonicity(na.omit(maas.irt))
print(xtable(summary(maas.mono),label="tab:maasmono", caption="Monotonicity Check Results for MAAS"))
@

As can be seen from Table \ref{tab:maasmono}, there were no violations of the monotonicity assumption for the reduced scale. The item coefficients (ItemH) are quite low, many of them hang around 0.30, which is the minimum allowed. 


<<maasitemspcm, echo=FALSE, results=hide>>=
maas.pcm.rasch <- gpcm(na.omit(maas.irt), constraint="rasch")
maas.pcm.1pl <- gpcm(na.omit(maas.irt), constraint="1PL")
maas.pcm.2pl <- gpcm(na.omit(maas.irt), constraint="gpcm")
@

<<maasirtraschprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas.pcm.rasch), label="tab:maaspcmrasch", caption="MAAS Partial Credit Model Coefficients, Rasch"))
@ 

As can be seen from Table \ref{tab:maaspcmrasch}, the Rasch partial credit model is not a good fit to the data. There are numerous violations of the increasing ability scores assumption. Next, a one parameter model (with discrimination estimated from the data) was fitted to the maas items. 

<<maasirtraschprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas.pcm.1pl), label="tab:maaspcm1pl", caption="MAAS Partial Credit Model Coefficients, One Parameter Model"))
@ 

The estimated one parameter model is shown in Table \ref{tab:maaspcm1pl}, and it can clearly be seen that it suffers from the same problems as the rasch model fitted above. Finally, a two parameter partial credit model was fitted to these items. 

<<maasirtraschprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas.pcm.2pl), label="tab:maaspcm2pl", caption="MAAS Partial Credit Model Coefficients, Two Parameter Model"))
@ 

The Table (\ref{tab:maaspcm2pl}) clearly shows that even the two parameter model does not provide a good fit to the data. The next stage of analysis for the Mindfulness scale was to fit one and two parameter Graded Response Models to the data. 

<<maasgrmfit, echo=FALSE, results=hide>>=
maas.grm.1pl <- grm(maas.irt, constrained=TRUE)
maas.grm.2pl <- grm(maas.irt, constrained=FALSE)
@
\begin{figure}
<<maasgrm1plplot, echo=FALSE, fig=TRUE>>=
maasp <- ggplotGRM(maas.grm.1pl)
print(maasp)
@   
  \caption{MAAS Graded Response Model (One Parameter) Ability Thresholds}
  \label{fig:maasgrm1plplot}
\end{figure}

As shown in Figure \ref{fig:maasgrm1plplot}, there were no obvious scaling violations for this scale. 

<<maasgrm1plprint, echo=FALSE, results=hide>>=
print(xtable(coef(maas.grm.1pl), caption="Coefficient Estimates for MAAS One Parameter Graded Response Model, Sample One", label="tab:maasgrm1pl"))
@ 

Table \ref{tab:maasgrm1pl} shows the estimated ability thresholds and discrimination parameter for the one parameter Graded Response Model on the MAAS. The discrimination parameter is moderate, as are the ability estimates, suggesting that this scale may not be suitable for respondents particularly high in mindfulness. 

Next, a two parameter Graded Response Model was examined for the same scale. 

\begin{figure}
<<maasgrm2plot, echo=FALSE, fig=TRUE>>=
maas2plp <- ggplotGRM(maas.grm.1pl)
print(maas2plp)
@   
  \caption{MAAS Graded Response Model (Two parameter) Item Ability Thresholds}
  \label{fig:maasgrm2plplot}
\end{figure}

<<maasgrm2plprint, echo=FALSE, results=hide>>=
print(xtable(coef(maas.grm.2pl), caption="Coefficient Estimates for MAAS Two Parameter Graded Response Model", label="tab:maasgrm2pl"))
@ 


Table \ref{tab:maasgrm2pl} shows the estimated coefficients for a two parameter Graded Response Model. IT can be seen that Q14 has the highest discriminatory power, and that Q11 has the highest ability threshold, while Q1 has the lowest. Q11 refers to listening to others while engaging in other tasks, and its ability estimates sugegst that it is a good question for pinpointing the abilities of respondents high on the construct of mindfulness. 

<<maasanovacomp, echo=FALSE, results=hide>>=
anova.rasch.maas.1pl <- anova.gpcm(maas.pcm.rasch, maas.pcm.1pl)
anova.1pl.maas.2pl <- anova.gpcm(maas.pcm.1pl, maas.pcm.2pl)
anova.rasch.maas.2pl <- anova.gpcm(maas.pcm.rasch, maas.pcm.2pl)
@

The three models were subjected to anova comparison and the 1 parameter model was significantly ($p \le 0.001$) better than the rasch model, and the 2 parameter model was significantly better than the 1 parameter model ($p \le 0.001$). However, the ultimate test is the ability of the model to predict out-of-sample data. 

The Life Orientation Test was the next instrument to be examined using the IRT approach.

Firstly, the scale analysis was conducted to determine which items fit best together.

<<lotrscales, echo=FALSE, results=tex>>=
lotr.scales <- aisp(na.omit(lotritems))
print(xtable(as.matrix(lotr.scales),label="tab:lotrscales"))
@

As can be seen from Table \ref{tab:lotrscales}, all of the items meet the assumptions of a unidimensional scale.  Next, the item orderings were examined.

<<lotritemord, echo=FALSE, results=tex>>=
lotr.iio <- check.iio(na.omit(lotritems))
print(xtable(lotr.iio[["violations"]],label="tab:lotritemord"))
@


As can be seen from Table \ref{tab:lotritemord}, Q1 needs to be removed from the scale in order to meet the assumptions of the model.

<<lotrmono, echo=FALSE, results=tex>>=
lotr.mono <- check.monotonicity(na.omit(lotritems))
print(xtable(summary(lotr.mono),label="tab:lotrmono"))
@

As can be seen from Table \ref{tab:lotrmono}, there were no violations of monotonicity in the sample.

<<lotrreduced, echo=FALSE, results=hide>>=
lotr.paste <- paste(lotr, c(3,4,7,9,10), sep="")
lotr.irt <- lotritems[,lotr.paste]
@

<<lotrmodelsirt, echo=FALSE, results=hide>>=
lotr.pcm.rasch <- gpcm(na.omit(lotr.irt), constraint="rasch")
lotr.pcm.1pl <- gpcm(na.omit(lotr.irt), constraint="1PL")
lotr.pcm.2pl <- gpcm(na.omit(lotr.irt), constraint="gpcm")
@

<<lotrpcmraschprint, echo=FALSE, results=tex>>=
print(xtable(coef(lotr.pcm.rasch), label="tab:lotrpcmrasch", caption="Rasch Partial Credit Model for Life Orientation Test, Revised"))
@ 

As shown in Table \ref{tab:lotrpcmrasch}, the model does not provide a good fit to the data. There are numerous violations of the ordering assumptions of the model. Next, a one parameter model was fit to the data. 

<<lotrpcm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(lotr.pcm.1pl), label="tab:lotrpcm1pl", caption="One parameter Partial Credit Model for Life Orientation Test, Revised"))
@ 

As can be seen in Table \ref{tab:lotrpcm1pl}, there are again some violations of the ability ordering assumption (LOTR4, category 3). Next, a two parameter PCM was fitted to the LOTR items. 

<<lotrpcm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(lotr.pcm.2pl), label="tab:lotrpcmgpcm", caption="Two Parameter Partial Credit Model for Life Orientation Test, Revised"))
@ 

The coefficients in Table \ref{tab:lotrpcmgpcm} show that again, LOTR4 ensures that the model does not fit correctly. 

<<lotranovacomp, echo=FALSE, results=hide>>=
anova.rasch.lotr.1pl <- anova.gpcm(lotr.pcm.rasch, lotr.pcm.1pl)
anova.1pl.lotr.2pl <- anova.gpcm(lotr.pcm.1pl, lotr.pcm.2pl)
anova.rasch.lotr.2pl <- anova.gpcm(lotr.pcm.rasch, lotr.pcm.2pl)
@

The results of the model comparison showed that the rasch model was not significantly different from the one parameter model ($p=0.862$), but that the two parameter model provided a significantly better fit to the data ($p \le 0.001$), even with a penalty for the extra parameters.

However, the difference in likelihoods was extremely small between the Rasch and two parameter models (20.15), and the BIC suggested that the Rasch model was a better fit.  One issue for the BIC is that it presumes that a true model exists amongst the candidate models, which is almost certainly not the case in this (or indeed any other psychological research) case.

The next stage in the analysis of the LOTR was the fitting of one and two parameter Graded Response Models. 

<<lotrgrmfit, echo=FALSE, results=hide>>=
lotr.grm.1pl <- grm(lotr.irt, constrained=TRUE)
lotr.grm.2pl <- grm(lotr.irt, constrained=FALSE)
@ 
\begin{figure}
<<lotrgrm1plplot, echo=FALSE, fig=TRUE>>=
lotr1plgrm <- ggplotGRM(lotr.grm.1pl)
print(lotr1plgrm)
@   
  \caption{One Parameter Graded Response Model for LOTR Item Ability Thresholds}
  \label{fig:lotr1plgrm}
\end{figure}

Figure \ref{fig:lotr1plgrm} shows the estimated threshold points for each of the categories for each item. It can be seen that there are no violations of item ordering, and that LOTR4 is the hardest item to endorse, while LOTR3 appears to be the easiest. 

\begin{figure}
<<lotr2plgrmplot, echo=FALSE, fig=TRUE>>=
lotr2plgrm <- ggplotGRM(lotr.grm.2pl)
print(lotr2plgrm)
@   
  \caption{Two Parameter Graded Response Model Item Ability Threshold Plot}
  \label{fig:lotr2plgrm}
\end{figure}


Again, Figure \ref{fig:lotr2plgrm} shows no violations of the modelling assumptions for the two parameter GRM. The next step was to assess which of the models provided the best fit to the data using a likelihood ratio test.

<<anovagrmlotr, echo=FALSE, results=hide>>=
anova.grm.lotr <- anova(lotr.grm.1pl, lotr.grm.2pl)
@ 

The two parameter model was significantly better than the one parameter model (p=0.001), but the BIC suggested that the one parameter model provided a better overall fit to the data. 

<<lotr2plestimates, echo=FALSE, results=tex>>=
print(xtable(coef(lotr.pcm.2pl),label="tab:lotr2plestimates", caption="Coefficient Estimates for LOTR Two Paramter Graded Response Model"))
@

The estimates for the two parameter model can be seen in Table \ref{tab:lotr2plestimates}.

\section{Predictions}
\label{sec:predictions}

As discussed in the methodology, one of the problems with psychometric methods is the problem of overfitting.  A solution that was proposed to this problem was cross-validation.  Typical cross-validation holds back some data in order to test the models developed on the rest.  This hold-out sample is typically of the order of 10\%.  However, for factor analyses, around 300 observations are typically needed for accurate estimation of parameters.  Therefore, models were developed on the first sample, and then will be fitted to a subset of the second sample.  This procedure will then be repeated with the data not used for testing in the second sample and the first sample.  This will allow the issues of overfitting to be avoided, and will allow for the most promising models to be applied to the experimental data, which by itself would not be sufficient to engage in any useful psychometric analyses.


The testing will be carried out using a few different methods.  Firstly, the predict scores method will be used for all factor analytic solutions.  Secondly,  CFA models will be fitted to the new data, allowing for comparision of their effectiveness on unseen data.  

A different procedure will be followed for the IRT models, using  three fold cross validation used to build an IRT model on each of the ten segments, and then comparing the estimates ability scores from the model built on the 33\% of the data, and from the estimates built on the held-out data. The difference between these two measures will provide an estimate of error for each of the model\'s predictive capability. This will then provide a metric for the selection of the best model, which can then be applied to the experimental data. 

<<hom2scales, echo=FALSE, results=hide>>=
randitems.paste <- paste(rand, c(1:36), sep="")
randitems2 <- hom2[,randitems.paste]
maasitems.paste <- paste(maas, c(1:15), sep="")
maasitems2 <- hom2[,maasitems.paste]
lotritems.paste <- paste(lotr, c(1,3,4,7,9,10), sep="")
lotritems2 <- hom2[,lotritems.paste]
@



One problem with this approach is that there were a huge number of non-responses to questions 13-16 on the RAND MOS, for unknown reasons.  This brings down the potential sample on this instrument to 281, which is not enough for the full analysis.  However, it is enough to test the factor solutions from the first sample, and as the RAND was not used in the experimental part of the research, this should be sufficient.




<<rand2semhom2, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "General and Emotional Health")
physfun <- paste(rand, c(3:12), sep="")
genemhealth <- paste(rand,c(1,14,17:36), sep="")
Rand2model2 <- mxModel(name="RAND2Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="General and Emotional Health", to=genemhealth),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                       mxData(observed=cov(na.omit(randitems2)), type="cov", numObs=281)
                      )
rand2fit2 <- mxRun(Rand2model2)
rand2summ2 <- summary(rand2fit2)
@

<<rand4semhom2, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "Social and Emotional Functioning", "General Health", "Physical Limitations")
physfun <- paste(rand, c(3:12), sep="")
genhealth <- paste(rand, c(1,33:36), sep="")
physlim <- paste(rand, c(13:16,21:22), sep="")
emsocfunc <- paste(rand, c(17:20, 23:32), sep="")
Rand4model2 <- mxModel(name="RAND4Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="Social and Emotional Functioning", to=emsocfunc),
                      mxPath(from="General Health", to=genhealth),
                      mxPath(from="Physical Limitations", to=physlim),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=281)
                      )
rand4fit2 <- mxRun(Rand4model2)
rand4summ2 <- summary(rand4fit2)
@ 
<<rand8semhom2, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "Social and Emotional Wellbeing", "General Health", "Emotional Role Limitations", "Role Limitations", "Fatigue", "Pain", "Energy")
physfun <- paste(rand, c(3:12), sep="")
socemwellbeing <- paste(rand, c(20,23:28, 30, 32), sep="")
genhealth <- paste(rand, c(1,33:36), sep="")
emrolelim <- paste(rand, c(17:20), sep="")
rolelim <- paste(rand, c(13:16), sep="")
energy <- paste(rand, c(29, 31), sep="")
pain <- paste(rand, c(21:22), sep="")
fatigue <- paste(rand, c(2, 23, 27), sep="")
Rand8model2 <- mxModel(name="RAND8Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="Social and Emotional Wellbeing", to=socemwellbeing),
                      mxPath(from="General Health", to=genhealth),
                      mxPath(from="Emotional Role Limitations", to=emrolelim),
                      mxPath(from="Role Limitations", to=rolelim),
                      mxPath(from="Energy", to=energy),
                      mxPath(from="Pain", to=pain),
                      mxPath(from="Fatigue", to=fatigue),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=281))
                      
rand8fit2 <- mxRun(Rand8model2)
rand8summ2 <- summary(rand8fit2)
@

<<rand13sem2, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning","Emotional Role Limitations","Role Limitations", "Positive Emotionality", "Physical Exertion", "Positive Health","Fatigue","Pain", "Emotional Problems", "Sickness", "Vigourous Activity","RAND20", "RAND24" )
physfun <- paste(rand, c(3:12), sep="")
emrolelim <- paste(rand, c(17:20), sep="")
rolelim <- paste(rand, c(13:16), sep="")
posemotions <- paste(rand, c(23,26,27,30), sep="")
physexertions <- paste(rand, c(3,4:5, 8), sep="")
poshealth <- paste(rand, c(1,34, 36), sep="")
fatigue <- paste(rand, c(29, 31), sep="")
pain <- paste(rand, c(21:22), sep="")
emproblems <- paste(rand, c(24, 25, 28, 30), sep="")
sickness <- paste(rand, 33, sep="")
vigactivity <- paste(rand, c(3,6), sep="")
rand20 <- "RANDQ20"
rand24 <- "RANDQ24"
Rand13model2 <- mxModel(name="RAND13Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                       mxPath(from="Emotional Role Limitations", to=emrolelim),
                       mxPath(from="Role Limitations", to=rolelim),
                       mxPath(from="Positive Emotionality", to=posemotions),
                      mxPath(from="Physical Exertion", to=physexertions),
                       mxPath(from="Positive Health", to=poshealth),
                       mxPath(from="Pain", to=pain),
                       mxPath(from="Fatigue", to=fatigue),
                       mxPath(from="Emotional Problems", to=emproblems),
                       mxPath(from="Vigourous Activity", to=vigactivity),
                      mxPath(from="Sickness", to=sickness),
                      mxPath(from="RANDQ20", to=rand20),
                       mxPath(from="RANDQ24", to=rand24),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=281)
                      )
rand13fit2 <- mxRun(Rand13model2)
rand13summ2 <- summary(rand13fit2)
@ 

<<randsemcomparehom2, echo=FALSE, results=tex>>=
randsemcomp.hom2 <- mxCompare(base=rand2fit2, comparison=c(rand4fit2, rand8fit2, rand13fit2))
print(xtable(randsemcomp,label="tab:randsemcomparehom2", caption="Model Comparison for MOS data using Models from Sample 1 on Sample 2" ))
@

As Table \ref{tab:randsemcomparehom2} shows, the eight factor model provided the best fit to this unseen data, similarly to the data on which the model was created. 

Next, the full cross-validation procedure was carried out on the MAAS and LOTR instruments.  To do this, the second sample was randomly split into three sets, and the models from the first sample were fit to this data, to examine which of them provided the best fit.

<<splitsample, echo=FALSE, results=hide>>=
set.seed(17)
maassamp <- sample(1:1109, 1109)
ms1 <- maassamp[1:370]
ms2 <- maassamp[371:740]
ms3 <- maassamp[741:length(maassamp)]
maasitems2a <- maasitems2[ms1,]
maasitems2b <- maasitems2[ms2,]
maasitems2c <- maasitems2[ms3,]
maasitems.nota <- maasitems2[c(ms2, ms3),]
maasitems.notb <- maasitems2[c(ms1, ms3),]
maasitems.notc <- maasitems2[c(ms1, ms2),]
lotrsamp <- sample(1:1109, 1109)
lr1 <- lotrsamp[1:370]
lr2 <- lotrsamp[371:740]
lr3 <- lotrsamp[741:length(lotrsamp)]
lotritems2a <- lotritems2[lr1,]
lotritems2b <- lotritems2[lr2,]
lotritems2c <- lotritems2[lr3,]
lotritems.nota <- lotritems2[c(lr2, lr3),]
lotritems.notb <- lotritems2[c(lr1, lr3),]
lotritems.notc <- lotritems2[c(lr1, lr2),]
randsamp <- sample(1:1109, 1109)
r1 <- randsamp[1:370]
r2 <- randsamp[371:740]
r3 <- randsamp[741:length(randsamp)]
randitems2a <- randitems[r1,]
randitems2b <- randitems[r2,]
randitems2c <- randitems[r3,]
randitems.nota <- randitems[c(r2, r3),]
randitems.notb <- randitems[c(r1, r3),]
randitems.notc <- randitems[c(r1, r2),]
@

<<maas7sem2, echo=FALSE, results=hide>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Automatic Behaviour", "Preoccupations", "Hurry",  "Lack of Present Focus", "Food Mindlessness", "Inattention", "Lack of Awareness")
automaticbehaviour <- paste(maas, c(9:12), sep="")
preoccupations <- "MAASQ13"
hurry <- paste(maas, c(7:8), sep="")
lackpresfocus <- paste(maas, c(2:3), sep="")
foodmindlessness <- "MAASQ15"
inattention <- "MAASQ6"
lackawareness <- paste(maas, c(1,4:5,9), sep="")
Maas7model2 <- mxModel(name="MAAS7Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Automatic Behaviour", to=automaticbehaviour),
                      mxPath(from="Preoccupations", to=preoccupations),
                      mxPath(from="Hurry", to=hurry),
                      mxPath(from="Lack of Present Focus", to=lackpresfocus),
                      mxPath(from="Lack of Awareness", to=lackawareness),
                           mxPath(from="Inattention", to=inattention),
                           mxPath(from="Lack of Present Focus", to=lackpresfocus),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2a)), type="cov", numObs=313)
                      )
maas7fit2 <- mxRun(Maas7model2)
maas7summ2 <- summary(maas7fit2)
@


<<maas1sem2, echo=FALSE, results=hide>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Unawareness")
unawareness <- paste(maas, c(1:15), sep="")
Maas1model2 <- mxModel(name="MAAS1Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Unawareness", to=unawareness),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems)), type="cov", numObs=313)
                      )
maas1fit2 <- mxRun(Maas1model2)
maas1summ2 <- summary(maas1fit2)
@




<<maassemcompare2, echo=FALSE, results=tex>>=
maascomp2 <- mxCompare(base=maas1fit2, comparison=maas7fit2)
maascomp.xtab2 <- xtable(maascomp2,label="tab:maassemcompare2", caption="Comparison of Sample One MAAS Factor Models on a subset of Sample Two Data (Split A)")
print(maascomp.xtab2)
@

Table \ref{tab:maassemcompare2} demonstrates that the MAAS 1 factor model provided the best fit to the subsample of data ($n=313$) used to test the model.


<<lotr1sem2, echo=FALSE, results=hide>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism")
optimism <- paste(lotr, c(1,3,4,7,9,10), sep="")
Lotr1model2 <- mxModel(name="LOTR1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2a)), type="cov", numObs=313)
                      )
lotr1fit2 <- mxRun(Lotr1model2)
lotr1summ2 <- summary(lotr1fit2)
@


<<lotr3sem, echo=FALSE, results=hide>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism", "Pessimism", "Pessimism2")
optimism <- paste(lotr, c(1,4,10), sep="")
pessimism <- paste(lotr, c(3,7), sep="")
pessimism2 <- paste(lotr, c(7,9), sep="")
Lotr3model2 <- mxModel(name="LOTR3Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from="Pessimism", to=pessimism),
                      mxPath(from="Pessimism2", to=pessimism2),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems)), type="cov", numObs=313)
                      )
lotr3fit2 <- mxRun(Lotr3model2)
lotr3summ2 <- summary(lotr3fit2)
@

<<lotrcompare2, echo=FALSE, results=hide>>=
lotrcomp2 <- mxCompare(base=lotr1fit2, comparison=lotr3fit2)
lotrcomp.xtab2 <- xtable(lotrcomp2,label="tab:lotrcompare2", caption="Comparison of SEM Results for Sample One LOTR Factor Models on a Subset of Sample Two (Split A)")
print(lotrcomp.xtab2)
@

Table \ref{tab:lotrcompare2}, shows that the one factor model provides the best fit to the subsample of data used to examine the model\'s performance on new data.

The next part of the analyses was examining the predictive ability of the IRT models developed on sample one.

\subsection{Confirmatory IRT Analyses}
\label{sec:conf-irt-analys}

The first scale to be examined was the RAND MOS Physical Functioning Scale. 

<<hom1physfuntest, echo=FALSE, results=hide>>=

hom1.grm.1pl.test <- testIRTModels(physfun.grm.1pl, rand2a.physfun, gpcmconstraint=NULL, grmconstraint=TRUE)
hom1.grm.2pl.test <- testIRTModels(physfun.grm.2pl, rand2a.physfun, gpcmconstraint=NULL, grmconstraint=FALSE)
hom1.grm.test.all <- rbind(hom1.grm.1pl.test, hom1.grm.2pl.test)
print(xtable(hom1.grm.test.all, label="tab:hom1physfungrmtest", caption="Performance of One and Two Parameter Graded Response Models from Sample One on a Subset of Sample Two (Split A)"))
@

As can be seen from Table \ref{tab:hom1physfungrmtest}, the one parameter model performed better on the unseen data. 


Next, the performance of the two emotional health GRM\'s on unseen data was assessed. 
<<hom1emhealthtest, echo=FALSE, results=hide>>=
emhealth2a <- randitems2a[,paste(rand, c(16:19,25,28,29,31), sep="")]
hom1.emhealth.grm.1pl.test <- testIRTModels(hom1.emhealth.grm.1pl, emhealth2a, gpcmconstraint=NULL, grmconstraint=TRUE)
hom1.emhealth.grm.2pl.test <- testIRTModels(hom1.emhealth.grm.2pl, emhealth2a, gpcmconstraint=NULL, grmconstraint=FALSE)
hom1.emhealth.grm.all <- rbind(hom1.emhealth.grm.1pl.test, hom1.emhealth.grm.2pl.test)
print(xtable(hom1.emhealth.grm.all, caption="Performance of One and Two Parameter Graded Response Models from Sample One on a subset of Data from Sample Two (Split A)", label="tab:hom1emhealthgrmtest"))
@ 

As can be seen from Table \ref{tab:hom1emhealthgrmtest}, the one parameter model performed best on the unseen data.


Next, we assess the performance of the two graded response models on the energy scale.

<<hom1energygrmtest, echo=FALSE, results=tex>>=
energy2a <- randitems2a[,paste(rand, c(1,20:23,26:27,30,34,36), sep="")]
hom1.energy.grm.1pl.test <- testIRTModels(hom1.energy.grm.1pl, energy2a, gpcmconstraint=NULL, grmconstraint=TRUE)
hom1.energy.grm.2pl.test <- testIRTModels(hom1.energy.grm.2pl, energy2a, gpcmconstraint=NULL, grmconstraint=FALSE)
hom1.energy.grm.all <- rbind(hom1.energy.grm.1pl.test, hom1.energy.grm.2pl.test)
print(xtable(hom1.energy.grm.all, caption="Performance of One and Two Parameter Graded Response Models from Sample One on Sample Two (Split A)", label="tab:hom1energygrmtest"))
@ 

As can be seen from Table \ref{tab:hom1energygrmtest}, the one parameter GRM provided the best fit to the unseen data. 

Next, we examine the performance of one and two parameter GRM's on the physical limitations scale from Sample One. 

<<hom1physlimtest, echo=FALSE, results=hide>>=
physlim2a <- randitems2a[,paste(rand, c(13:16), sep="")]
@ 


\part{Sample Two}

\begin{figure}
<<onlinemissingplot, echo=FALSE, fig=TRUE>>=
print(qplot(online.missing))
@
  \caption{Histogram of Missing Values, Online sample}
  \label{fig:onlinemissingplot}
\end{figure}
\begin{figure}
<<optplot2, echo=FALSE, fig=TRUE>>=
optplot2 <- ggplot(hom2, aes(x=generalhealth, y=optimism))+layer(geom="smooth", method="lm")
print(optplot2)
@  
  \caption{Scatterplot with Linear Regression Smooth of Relationship between General Health and Optimism}
  \label{fig:genhealthoptplot2}
\end{figure}


The relationship between optimism and health is even clearer in sample 2 (Figure \ref{fig:genhealthoptplot2}) (note the narrower confidence intervals and the steeper slope of the line).

\subsubsection{Regression Analyses}
\label{sec:regression-analyses}

Similiar regression analyses were carried out on the test sample from Study 2 as were carried out on the first sample.  The first action taken was to predict the coefficients in the new data set from the models developed on the first sample.

%This section needs to be done. 


\subsubsection{Psychometric Analyses}
\label{sec:psych-analys}

\subsubsubsection{Factor Analyses}
\label{sec:psych-analys-1}

The approach taken to the psychometric analysis of the second sample of data was as follows.  Firstly, factor models were built on the two remaining samples from this dataset (the first having been used to validate the results from Sample 1).  Next, a CFA was run on each of the other samples, such that if the model was developed on the b sample, it was tested on both the a and the c sample.  This provides a better measure of accuracy and replicability for each of the proposed factor structures.  Finally, the most successful model was back-tested on the data from sample 1. The model chosen by this procedure was then used to predict factor scores for each of the participants, and was also used to predict these scores for the experimental portion of the research.

\subsubsection{Split B}
\label{sec:split-b}



<<maas2bparallel, echo=FALSE, fig=TRUE, results=hide>>=
maas2b.parallel <- fa.parallel(na.omit(maasitems2b))
vss.maas.2b <- VSS(na.omit(maasitems2b))
@

The parallel analysis procedure suggests that this sample of the responses to the MAAS has five factors, while the MAP criterion suggests that it has only one.  Following our previous approach, each of these factor solutions will be examined and interpreted before a CFA is applied on the remainder of the dataset.

<<maas2b1, echo=FALSE, results=tex>>=
maas2b.fact1 <- fa(maasitems2b, 1, rotate="oblimin", fm="pa")
print(FactorXtab(maas2b.fact1,label="tab:maas2bfact1", caption="MAAS One Factor Solution, Sample 2B"))
@

The solution shown above in Table \ref{tab:maas2bfact1} shows adequate loadings of all the questions on a first factor which can be named mindfulness.  This solution explained 41\% of the variance, which is quite low for a factor solution.

<<maas2bfact2, echo=FALSE, results=tex>>=
maas2b.fact2 <- fa(maasitems2b, 2, rotate="oblimin", fm="pa")
print(FactorXtab(maas2b.fact2,label="tab:maas2bfact2", caption="MAAS Two Factor Solution, Sample 2B"))
@

As can be seen from Table \ref{tab:maas2bfact2}, a clear two factor solution emerges from the data.
PA1: MAAS4-MAAS12, MAAS14,MAAS15. This factor can probably best be termed lack of attention.
PA2: MAAS1-MAAS3, MAAS13. This second factor has loadings which relate to lack of awareness to the present, and as such can be termed lack of present awareness.


<<maas2bfact3, echo=FALSE, results=tex>>=
maas2b.fact3 <- fa(maasitems2b, 3, rotate="oblimin", fm="pa")
print(FactorXtab(maas2b.fact3,label="tab:tcq2bmaasfact3"))
@

This 3 factor solution explains 48\% of the variance, and the factors break down as follows:

PA1: "MAASQ4"  "MAASQ5"  "MAASQ7"  "MAASQ8"  "MAASQ9"  "MAASQ10".  All of these items relate to the lack of awareness of bodily/physical actitivites and so this factor can best be termed lack of bodily awareness.

PA3:"MAASQ11" "MAASQ12" "MAASQ13" "MAASQ14" "MAASQ15".  All of these questions appear to relate to being distracted, and so this factor could best be termed distractability.

PA2:"MAASQ1" "MAASQ2" "MAASQ3".  This factor is quite similiar to the second factor in the two factor solution, and can most usefully be termed lack of present focus.


<<maas2bfact4, echo=FALSE, results=tex>>=
maas2b.fact4 <- fa(maasitems2b, 4, rotate="oblimin", fm="pa")
print(FactorXtab(maas2b.fact4,label="tab:tcq2bmaasfact4"))
@
This four factor solution explains 51\% of the variance in the sample.
PA4: "MAASQ10" "MAASQ11" "MAASQ12" "MAASQ13" "MAASQ14" "MAASQ15".  Again, somewhat like PA3 in the three factor solution, all of these items relate to being distracted, and so the factor can best be termed distractability.

PA1:"MAASQ5"  "MAASQ6"  "MAASQ7"  "MAASQ8"  "MAASQ9"  "MAASQ10".  Again, like a previous factor, all of these items relate to lack of attention to physical symptoms, and so this factor can be called lack of physical awareness.

PA2: "MAASQ1" "MAASQ2" "MAASQ3".  This factor can best be termed as lack of present awareness.

PA3:"MAASQ4" "MAASQ5".  This factor is probably best described as lack of somatic awareness.

<<maas2bfact5, echo=FALSE, results=tex>>=
maas2b.fact5 <- fa(maasitems2b, 5, rotate="oblimin", fm="pa")
print(FactorXtab(maas2b.fact5,label="tab:tcq2bmaasfact5"))
@
This five factor solution explained 54\% of the variance in the sample.

PA1: "MAASQ5"  "MAASQ6"  "MAASQ7"  "MAASQ8"  "MAASQ9"  "MAASQ10".  This factor has come through in most of the previous solutions, and can again be termed distractability.

PA2: "MAASQ1" "MAASQ2" "MAASQ3".  Again, these items have clustered together previously, and this factor is again termed lack of present awareness.

PA3: "MAASQ4" "MAASQ5".  This factor is again termed lack of somatic awareness.

PA4: "MAASQ13" "MAASQ14".  This factor can best be termed as lack of attention.

PA5: "MAASQ10" "MAASQ11" "MAASQ12" "MAASQ14" "MAASQ15".  This factor again can be termed distractability.

Now, we examine the fit indices for the five solutions.

<<maas2bfitindices, echo=FALSE, results=tex>>=
maas2b.1fit <- FitIndices(maas2b.fact1)
maas2b.2fit <- FitIndices(maas2b.fact2)
maas2b.3fit <- FitIndices(maas2b.fact3)
maas2b.4fit <- FitIndices(maas2b.fact4)
maas2b.5fit <- FitIndices(maas2b.fact5)
maas2bfit <- as.data.frame(cbind(maas2b.1fit,maas2b.2fit,maas2b.3fit,maas2b.4fit,maas2b.5fit))
print(xtable(maas2bfit),label="tab:tcq2bmaassemcomp")
@

<<lotr2bparallel, echo=FALSE, fig=TRUE>>=
sink("tmp.txt")
lotr2b.parallel <- fa.parallel(na.omit(lotritems2b))
lotr2b.vss <- VSS(na.omit(lotritems2b))
sink(NULL)
@
Again, the parallel analysis criterion suggests two factors, while the MAP criterion suggests one, so both solutions will be examined and interpreted.

<<lotr2bfact1, echo=FALSE, results=tex>>=
lotr2b.fact1 <- fa(lotritems2b, 1, rotate="oblimin", fm="pa")
print(FactorXtab(lotr2b.fact1,label="tab:tcq2blotr1"))
@

<<lotr2bfact2, echo=FALSE, results=tex>>=
lotr2b.fact2 <- fa(lotritems2b, 2, rotate="oblimin", fm="pa")
print(FactorXtab(lotr2b.fact2,label="tab:tcq2blotr2"))
@

<<maas2bsemon2c, echo=FALSE, results=hide>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Unawareness")
unawareness <- paste(maas, c(1:15), sep="")
Maas1model2b <- mxModel(name="MAAS12b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Unawareness", to=unawareness),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2c)), type="cov", numObs=370)
                      )
maas1fit2b <- mxRun(Maas1model2b)
maas1summ2b <- summary(maas1fit2b)
@

<<maas2sem2b, echo=FALSE, results=hide>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Unawareness", "Distractability")
unawareness <- paste(maas, c(4:12, 14:15), sep="")
distractability <- paste(maas, c(1:3, 13), sep="")
Maas2model2b <- mxModel(name="MAAS22b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Unawareness", to=unawareness),
                        mxPath(from="Distractability", to=distractability),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2c)), type="cov", numObs=370)
                      )
maas2fit2b <- mxRun(Maas2model2b)
maas2summ2b <- summary(maas2fit2b)
@


<<maas3sem2b, echo=FALSE, results=hide>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Lack of Bodily Awareness", "Distractability", "Lack of present focus")
bodyunaware <- paste(maas, c(4,5,7:10), sep="")
distractability <- paste(maas, c(11:15), sep="")
lackpresfocus <- paste(maas, c(1:3), sep="")
Maas3model2b <- mxModel(name="MAAS32b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Lack of Bodily Awareness", to=bodyunaware),
                        mxPath(from="Distractability", to=distractability),
                        mxPath(from="Lack of present focus", to=lackpresfocus),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2c)), type="cov", numObs=370)
                      )
maas3fit2b <- mxRun(Maas3model2b)
maas3summ2b <- summary(maas3fit2b)
@

<<maas42b, echo=FALSE, results=hide>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Lack of Bodily Awareness", "Distractability", "Lack of present focus", "Lack of somatic awareness")
bodyunaware <- paste(maas, c(6:10), sep="")
distractability <- paste(maas, c(11:15), sep="")
lackpresfocus <- paste(maas, c(1:3), sep="")
lacksomaware <- paste(maas, c(4,5), sep="")
Maas4model2b <- mxModel(name="MAAS42b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Lack of Bodily Awareness", to=bodyunaware),
                        mxPath(from="Distractability", to=distractability),
                        mxPath(from="Lack of present focus", to=lackpresfocus),
                        mxPath(from="Lack of somatic awareness", to=lacksomaware),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2c)), type="cov", numObs=370)
                      )
maas4fit2b <- mxRun(Maas4model2b)
maas4summ2b <- summary(maas4fit2b)
@

<<maas5sem2b, echo=FALSE, results=hide>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Lack of Bodily Awareness", "Distractability", "Lack of present focus", "Lack of somatic awareness", "Lack of attention")
bodyunaware <- paste(maas, c(6:10), sep="")
distractability <- paste(maas, c(11:15), sep="")
lackpresfocus <- paste(maas, c(1:3), sep="")
lacksomaware <- paste(maas, c(4,5), sep="")
lackattention <- paste(maas, c(13,14), sep="")
Maas5model2b <- mxModel(name="MAAS52b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Lack of Bodily Awareness", to=bodyunaware),
                        mxPath(from="Distractability", to=distractability),
                        mxPath(from="Lack of present focus", to=lackpresfocus),
                        mxPath(from="Lack of somatic awareness", to=lacksomaware),
                        mxPath(from="Lack of attention", to=lackattention),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2c)), type="cov", numObs=370)
                      )
maas5fit2b <- mxRun(Maas5model2b)
maas5summ2b <- summary(maas4fit2b)
@

<<maas2bsemcompare, echo=FALSE, results=tex>>=
maas2b.semcomp <- mxCompare(base=maas1fit2b, comparison=c(maas2fit2b, maas3fit2b, maas4fit2b, maas5fit2b))
print(xtable(maas2b.semcomp,label="tab:maas2bsemcompare"))
@

As can be seen from Table \ref{tab:maas2bsemcompare}, the one factor solution again performs best, further increasing our confidence in its adequacy.  Next,  this procedure is repeated for the LOT-R.

<<lotr1sem2b, echo=FALSE, results=hide>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism")
optimism <- paste(lotr, c(1,3,4,7,9,10), sep="")
Lotr1model2b <- mxModel(name="LOTR1b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2c)), type="cov", numObs=370)
                      )
lotr1fit2b <- mxRun(Lotr1model2b)
lotr1summ2b <- summary(lotr1fit2b)
@


<<lotr2sem2b, echo=FALSE, results=hide>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism", "Pessimism")
optimism <- paste(lotr, c(1,4,10), sep="")
pessimism <- paste(lotr, c(3,7,9), sep="")
Lotr2model2b <- mxModel(name="LOTR2b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from="Pessimism", to=pessimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2c)), type="cov", numObs=370)
                      )
lotr2fit2b <- mxRun(Lotr2model2b)
lotr2summ2b <- summary(lotr2fit2b)
@

<<lotrcompare2, echo=FALSE, results=hide>>=
lotrcomp2b <- mxCompare(base=lotr1fit2b, comparison=lotr2fit2b)
lotrcomp.xtab2b <- xtable(lotrcomp2b,label="tab:tcq2blotrcomp")
print(lotrcomp.xtab2b)
@

\subsubsection{Item Response Theory Analyses}
\label{sec:item-response-theory-1}

\subsubsection{Split A}
\label{sec:split-a}

The next stage in the analyses was to examine the response structure of the items in all three splits of the second sample data. 

This process was begun with the RAND MOS.

The first step was to examine the breakdown of items within scales. 

<<rand2aaiasp, echo=FALSE, results=tex>>=
rand2a.aisp <- aisp(na.omit(randitems2a))
rand2a.aisp <- as.data.frame(rand2a.aisp)
print(xtable(rand2a.aisp, caption="Item Selection Procedure Results for RAND MOS, Split A", label="tab:rand2aaisp"))
@ 

Table \ref{tab:rand2aaisp} shows the breakdown of items and their assignment to particular scales. 
The first scale consists of items 3 to 12 and is the physical functioning scale and so retains that name. 

The second scale consists of the role limitations scales and the negatively worded questions from the emotional well being and energy/faatigue scales, and can probably be best termed as negative health. 

The third scale consists of the general health, pain and positively worded questions for emotional well being and energy/fatigue scales, and can probably best be termed as overall health. 

Two items (2 and 24) do not load on any scale, while 33 and 35 load on a seperate scale. This scale (4 in the table) was not analysed, as two items is not enough to make a useful scale. 

<<rand2ascales, echo=FALSE, results=hide>>=
rand2a.physfun <- randitems2a[,paste(rand, 3:12, sep="")]
rand2a.neghealth <- randitems2a[,paste(rand, c(13:19, 25,28, 29,31,32), sep="")]
rand2a.ovhealth <- randitems2a[,paste(rand, c(1, 22,23, 26,27,34,36), sep="")]
@ 

Firstly, the item ordering and monotonicity assumptions were checked for the physical functioning scale. 



<<rand2aphysfuncheck, echo=FALSE, results=hide>>=
physfun2a.iio <- check.iio(na.omit(rand2a.physfun))
physfun2a.mono <- check.monotonicity(na.omit(rand2a.physfun))
@ 

There were no violations of either the item ordering or monotonicity assumptions for the physical functioning scale in this split, so IRT modelling can proceed.

The first step was to fit three partial credit models to this scale.

<<physfun2apcm, echo=FALSE,results=hide >>=
physfun2a.pcm.rasch <- gpcm(rand2a.physfun, constraint="rasch")
physfun2a.pcm.1PL <- gpcm(rand2a.physfun, constraint="1PL")
physfun2a.pcm.gpcm <- gpcm(rand2a.physfun, constraint="gpcm")
@ 

All three partial credit models had non increasing parameter estimates, and thus are not considered further here. 

Next, one and two parameter Graded Response Models were fit to this scale. 

<<physfun2agrm, echo=FALSE, results=hide>>=
physfun2a.grm.1pl <- grm(rand2a.physfun, constrained=TRUE)
physfun2a.grm.2pl <- grm(rand2a.physfun, constrained=FALSE)
@ 

<<physfun2agrm1plprint, echo=FALSE, results=hide>>=
print(xtable(coef2mat(coef(physfun2a.grm.1pl)), caption="Coefficient Estimates for One Parameter Graded Response Model Physical Functioning Scale, Split A", label="tab:rand2aphysfungrm1pl"))
@ 

Table \ref{tab:rand2aphysfungrm1pl} shows the coefficient estimates for this scale. It can be seen that the discrimination parameter is quite high, while the ability estimates are quite low. This is presumably because many of these items are intended more for a clinical sample than the non-clincial sample used in this research. 

<<physfun2agrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(physfun2a.grm.2pl)), caption="Coefficient Estimates for Two Parameter Graded Response Model for Physical Functioning Scale, Split A", label="tab:physfun2agrm2pl"))
@ 

Table \ref{tab:physfun2agrm2pl} shows the coefficient estimates for the two parameter model. It can  be seen that while the ability estimates have remained quite low, the discrimination parameters have altered significantly. Q7 and 11 have the two highest discrimination parameters, which makes sense as they ask respectively regarding difficulties in climbing one flight of stairs or walking one block. Again, the ability estimates for each threshold are quite low. 

The next step is to examine the performance of each of these models on unseen data. 

<<rand2agrmtest, echo=FALSE, results=tex>>=
physfun.nota <- randitems.nota[,paste(rand, 3:12, sep="")]
physfun2a.grm.1pl.test <- testIRTModels(physfun2a.grm.1pl, physfun.nota, gpcmconstraint=NULL, grmconstraint=TRUE)
physfun2a.grm.2pl.test <- testIRTModels(physfun2a.grm.2pl, physfun.nota, gpcmconstraint=NULL, grmconstraint=FALSE)
physfun2a.grm.all <- rbind(physfun2a.grm.1pl.test, physfun2a.grm.2pl.test)
print(xtable(physfun2a.grm.all, caption="Performance of Physical Functioning One and Two Parameter Graded Response Models (Split A) on unseen data", label="tab:physfun2agrmtest"))
@ 


As can be seen from Table \ref{tab:physfun2agrmtest}, the one paramter model provided the best fit to the unseen data. 


Next, the negative health scale is checked for violations of the item ordering and monotonicity assumptions. 

<<neghealth2acheck, echo=FALSE, results=hide>>=
neghealth2a.iio <- check.iio(na.omit(rand2a.neghealth))
neghealth2a.mono <- check.monotonicity(na.omit(rand2a.neghealth))
@

There were no violations of either the item ordering assumption or the monotonicity assumption for this scale. 

The next step was to fit three partial credit models to this scale. 

<<neghealth2apcm, echo=FALSE, results=hide>>=
neghealth2a.pcm.rasch <- gpcm(rand2a.neghealth, constraint="rasch")
neghealth2a.pcm.1PL <- gpcm(rand2a.neghealth, constraint="1PL")
neghealth2a.pcm.gpcm <- gpcm(rand2a.neghealth, constraint="gpcm")
@ 

<<neghealth2apcmraschprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(neghealth2a.pcm.rasch)), caption="Coefficient Estimates for Rasch Partial Credit Model, Negative Health Scale, Split A", label="tab:neghealth2apcmrasch"))
@ 

Table \ref{tab:neghealth2apcmrasch} shows the coefficient estimates for the RAsch partial credit model. It can be seen that there is an extremely wide spread of abillity estimates, the physical limitations items tend to have quite low thresholds, while the items drawn from the emotional well being and energy/fatigue scales have a much broader spread. For instance, item 32 has a top ability threshold of 6.11, which is extremely high. The item asks about health problems interfering with the social life of the respondent, and given that the sample was predominantly young and students, may be the reason for the low probability of endorsement of the items (thus leading to the high estimated ability threshold). 

<<neghealth2apcm1pl, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(neghealth2a.pcm.1PL)), caption="Coefficient Estimates for Negative health Scale, One Parameter Partial Credit Model, Split A", label="tab:neghealth2apcm1pl"))
@ 

Table \ref{tab:neghealth2apcm1pl} shows the coefficient estimates for the one parameter partial credit model. The overall estimated discrimination parameter is quite low, while the same pattern of ability esitmates as the rasch model remains. 

<<neghealth2apcmgpcm, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(neghealth2a.pcm.1PL)), caption="Coefficient Estimates for Negative Health Scale, Two Parameter Partial Credit Model, Split A", label="tab:neghealth2apgm2pl"))
@ 

Table \ref{tab:neghealth2apcm2pl} shows the estimated thresholds and discrimination parameters for the two parameter partial credit model. Interestingly enough, the estimated discrimination parameters have actually decreased for the majority of the scale, which is unexpected. The ordering of the ability estimates remains the same as in previous models. 

Next, the performance of these three partial credit models was assessed on unseen data. 

<<neghealth2apcmtest, echo=FALSE, results=tex>>=
neghealth.nota <- randitems.nota[,paste(rand, c(13:19, 25,28, 29,31,32), sep="")]
neghealth2a.pcm.rasch.test <- testIRTModels(neghealth2a.pcm.rasch, neghealth.nota, gpcmconstraint="rasch", grmconstraint=NULL)
neghealth2a.pcm.1PL.test <- testIRTModels(neghealth2a.pcm.1PL, neghealth.nota, gpcmconstraint="1PL", grmconstraint=NULL)
neghealth2a.pcm.gpcm.test <- testIRTModels(neghealth2a.pcm.gpcm, neghealth.nota, gpcmconstraint="gpcm", grmconstraint=NULL)
neghealth2a.pcm.test.all <- rbind(neghealth2a.pcm.rasch.test, neghealth2a.pcm.1PL.test, neghealth2a.pcm.gpcm.test)
print(xtable(neghealth2a.pcm.test.all, caption="Performance of Negative Health Partial Credit Models (Split A) on unseen data (Splits B and C)", label="tab:neghealth2apcmtest"))
@ 

Table \ref{tab:neghealth2apcmtest} shows that the one parameter model provided a better fit to the unseen data. 

Next, one and two parameter Graded Response Models were fit to the negative health scale. 

<<neghealth2agrm, echo=FALSE, results=hide>>=
neghealth2a.grm.1pl <- grm(rand2a.neghealth, constrained=TRUE)
neghealth2a.grm.2pl <- grm(rand2a.neghealth, constrained=FALSE)
@ 

<<neghealth2agrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(neghealth2a.grm.1pl)), caption="Coefficient Estimates for Negative Health One Parameter Graded Response Model, Split A"))
@ 

\subsubsection{Split B}
\label{sec:split-b-1}



<<randcheckassumptions, echo=FALSE, results=hide>>=
rand.scales <- aisp(na.omit(randitems2b))
@

<<randscales, echo=FALSE, results=tex>>=
print(xtable(rand.scales,caption="Automatic Item Selection Procedure, RAND MOS, Split B",label="tab:randscales2b"))
@
As can be seen from Table \ref{tab:randscales}, the item selection procedure suggests that there are four scales in the RAND MOS.  In addition, one item does not load on any scale.  This is RANDQ2, which is not surprising given that it is not supposed to score on any scale.  The analysis of the RAND scales continues now, but using the four scales suggested by the item selection procedure.

The scales can be termed as follows:

Scale 1: RANDQ3-RANDQ12: This maps exactly to the physical functioning scale.

Scale 2: RANDQ17-RANDQ19,24-25,28-32,35. This scale incorporates the Role Limitations, Social Functioning, Emotional Well Being and Energy scales, and can best be termed as Negative Health.

Scale 3: RANDQ1, RANDQ20, RANDQ23, RANDQ26, RANDQ27, RANDQ30, RANDQ34, RANDQ36: This scale maps to the general health and positively worded questions from the energy/fatigue and emotional health scales, and so can best be termed Positive Health.  

Scale 4: RAND21-RAND22. This maps to the  pain scale, and can probably best be termed as pain. This scale will not be analysed further as two items is not enough to model the response structure properly. 


<<randirtscales, echo=FALSE, results=hide>>=
irtphysfun2b <- randitems2b[,paste(rand, c(3:12), sep="")]
irtneghealth2b <- randitems2b[,paste(rand, c(17:19, 24:25,28,29,31,35 ), sep="")]
irtposhealth2b <- randitems2b[,paste(rand, c(1, 20, 23, 26,27,30, 34,36), sep="")]
@

<<randphysfun, echo=FALSE, results=hide>>=
irtphys2b.item.ord <- check.iio(na.omit(irtphysfun2b))
irtphys2b.monotonicity <- check.monotonicity(na.omit(irtphysfun2b))
@

The check for invariant item ordering shows that all items  meet the Invariant Item Ordering  assumption. The check on the monotonicity assumption shows that all of the items meet this assumption. 


<<randphysfuncitemord, echo=FALSE, results=tex>>=
print(xtable(irtphys2b.item.ord[["violations"]],label="tab:physfunc2bitemord", caption="Item Ordering Check for RAND Physical Functioning Scale, Split B"))
@

As can be seen from Table \ref{tab:physfunc2bitemord}, there were no violations of item ordering for this sample.  This demonstrates that there are no violations of monotonicity for this subscale, and suggests that item response theory modelling can proceed, for this scale at least.

<<randneghealth, echo=FALSE, results=hide>>=
irtneghealth2b.item.ord <- check.iio(na.omit(irtneghealth2b))
irtemhealth2b.monotonicity <- check.monotonicity(na.omit(irtneghealth2b))
@

<<randneghealthItemord, echo=FALSE, results=tex>>=
print(xtable(irtneghealth2b.item.ord[["violations"]],label="tab:tcq2randneghealthitemord", caption="Item Ordering Assumption Check for RAND Negative Health Scale, Split B"))
@

As can be seen from Table \ref{tab:randneghealthitemord},quite a number of items failed the item ordering assumption (RAND29, RAND35) and so were removed from the scale before further analysis. 

<<irtneghealth, echo=FALSE, results=tex>>=
irtneghealth2b.s <- irtneghealth2b[,paste(rand, c(25,28,24,29,31,17,19,18), sep="")]
neghealth.item.ord2 <- check.iio(na.omit(irtneghealth2b.s))
neghealth.mono.2 <- check.monotonicity(na.omit(irtneghealth2b.s))
print(xtable(neghealth.item.ord2[["violations"]], caption="IIO for reduced negative health scale", label="tab:randneghealth2biio"))
@

As shown in Table \ref{tab:randneghealth2iio}, the removal of items from the emotional health scale appears to have removed the problems with invariant item ordering. The reduced scale had no issues with non-monotonic item responses. 



<<randposhealth, echo=FALSE, results=hide>>=
irtposhealth2b.item.ord <- check.iio(na.omit(irtposhealth2b))
irtposhealth2b.monotonicity <- check.monotonicity(na.omit(irtposhealth2b))
@

<<poshealthitemord2b, echo=FALSE, results=tex>>=
print(xtable(irtposhealth2b.item.ord[["violations"]], caption="Item Ordering Checks for RAND MOS Positive Health Scale, Split B", label="tab:rand2bposhealthitemord"))
@ 
As can be seen from Table \ref{tab:rand2bposhealthitemord}, all items met the item ordering assumption. 
Additionally, the smaller scale showed no violations of the monotonicity assumption. 



Having checked the assumptions for all three scales, the next step was to fit partial credit and graded response models to each of the scales, and to examine their fit on unseen data. 


<<physfunrasch, echo=FALSE, results=hide>>=
physfun2b.gpcm.rasch <- gpcm(na.exclude(irtphysfun2b), constraint="rasch")
physfun2b.gpcm.1pl <- gpcm(na.exclude(irtphysfun2b), constraint="1PL")
physfun2b.gpcm.gpcm <- gpcm(na.exclude(irtphysfun2b), constraint="gpcm")
@

Three partial credit models were fit (rasch, one parameter and two parameter). However, all of them had non-monotonically increasing ability estimates, and so are not reported further here. 

The next step in the modelling process was to fit a one and two parameter GRM. 

<<physfun2bgrm, echo=FALSE, results=hide>>=
physfun2b.grm.1pl <- grm(na.omit(irtphysfun2b), constrained=TRUE)
physfun2b.grm.2pl <- grm(na.omit(irtphysfun2b), constrained=FALSE)
@ 

<<physfun2bgrm1plprint, echo=FALSE, results=hide>>=
print(xtable(coef(physfun2b.grm.1pl), caption="One Parameter Graded Response Model for Physical Functioning IRT Scale, Split B", label="tab:physfun2bgrm1pl"))
@ 

As can be seen from Table \ref{tab:physfun2bgrm1pl}, there were no violations of monotonicity for the one parameter GRM. Note that the discrimination parameter is extremely high, which is not [particularly surprising given the use of a non-clinical sample. 

<<physfun2bgrm2plprint, echo=FALSE, results=hide>>=
print(xtable(coef(physfun2b.grm.2pl), caption="Two Parameter Graded Response Model for Physical Functioning IRT Scale, Split B", label="tab:physfun2bgrm2pl"))
@ 

It can be seen from Table \ref{tab:physfun2bgrm2pl} that the average ability estimates have risen while discrimination parameters have dropped for most of the items. The next step is to examine the performance of each of these models on unseen data (i.e. that from Splits A and C). 

<<physfun2bgrmtest, echo=FALSE, results=tex>>=
physfun.notb <- randitems.notb[,paste(rand, c(3:12), sep="")]
physfun2b.grm.1pl.test <- testIRTModels(physfun2b.grm.1pl, physfun.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
physfun2b.grm.2pl.test <- testIRTModels(physfun2b.grm.2pl, physfun.notb, gpcmconstraint=NULL, grmconstraint=FALSE)
physfun2b.grm.test.all <- rbind(physfun2b.grm.1pl.test, physfun2b.grm.2pl.test)
print(xtable(physfun2b.grm.test.all, caption="Performance of One and Two Parameter GRM's on Unseen Data (Splits A and C)", label="tab:physfun2btest"))
@ 

As can be seen from Table \ref{tab:physfun2btest}, the one parameter model provided a better fit to the unseen data. 

The next sub-scale to be examined is Negative Health, described above.  This process begins, as above, with the fitting of a simple one parameter model, in this case, the partial credit model.

<<neghealthpcm, echo=FALSE, results=hide>>=
neghealth2b.pcm.rasch <- gpcm(irtneghealth2b, constraint="rasch")
neghealth2b.pcm.1PL <- gpcm(irtneghealth2b, constraint="1PL")
neghealth2b.pcm.gpcm <- gpcm(irtneghealth2b, constraint="gpcm")
@ 

Three partial credit models were fitted, but all had non-monotonically increasing parameter estimates, and so are not reported further here. 

Next, one and two parameter Graded Response Models were fitted. 

<<neghealthgrm, echo=FALSE, results=hide>>=
neghealth2b.grm.1pl <- grm(irtneghealth2b, constrained=TRUE)
neghealth2b.grm.2pl <- grm(irtneghealth2b, constrained=FALSE)
@ 

<<negghealth2bgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(neghealth2b.grm.1pl)), caption="Coefficients for One Parameter Graded Response Model Negative Health Scale, Split B", label="tab:neghealth2bgrm1pl"))
@ 


As shown in Table \ref{tab:neghealth2bgrm1pl}, there were no problemw ith non-monotonically increasing parameter estimates for this model. Of interests is the relatively low discrimination parameter, which suggests that these items were more applicable to the sample. Also of interest is that the emotional role limitations questions (17,18,19) did not have any responses that were not no, suggesting that this scale was not particularly useful for this sample. 

<<neghealth2bgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(neghealth2b.grm.2pl)), caption="Coefficients for Negative Health Scale, Two Parameter Graded Response Model, Split B", label="tab:neghealth2bgrm2pl"))
@ 


As can be seen from Table \ref{tab:neghealth2bgrm2pl}, the discrimination parameter has lowered (in line with previous models) while the estimated abilities have widened. Note that question 25 and 28 appear to have been the source of the high discrimination parameter in the one parameter model. 

Next, the fit of each of these models on unseen data was examined. 

<<neghealth2bgrmtest, echo=FALSE, results=tex>>=
neghealth.notb <- randitems.notb[,paste(rand, c(17:19, 24:25,28,29,31,35 ), sep="")] 
neghealth2b.grm.1pl.test <- testIRTModels(neghealth2b.grm.1pl, neghealth.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
neghealth2b.grm.2pl.test <- testIRTModels(neghealth2b.grm.2pl, neghealth.notb, gpcmconstraint=NULL, grmconstraint=FALSE)
neghealth2b.grm.test.all <- rbind(neghealth2b.grm.1pl.test, neghealth2b.grm.2pl.test)
print(xtable(neghealth2b.grm.test.all, caption="Performance of Negative health Split B Graded Response Models on Unseen Data", label="tab:neghealth2bgrmtest"))
@ 

As can be seen from Table \ref{tab:neghealth2bgrmtest}, the one parameter model performed slightly better on the unseen data, though there is not much in the difference. 

Finally, for this split, the positive health scale was examined using partial credirt and graded response models. 

<<irtposhealthpcm, echo=FALSE, results=hide>>=
irtposhealth2b.pcm.rasch <- gpcm(irtposhealth2b, constraint="rasch")
irtposhealth2b.pcm.1PL <- gpcm(irtposhealth2b, constraint="1PL")
irtposhealth2b.pcm.gpcm <- gpcm(irtposhealth2b, constraint="gpcm")
@ 

Three partial credit models were fit to the data, but all had problems with the parameter estimates and are not reported further here. 

<<poshealth2bgrm, echo=FALSE, results=hide>>=
poshealth2b.grm.1pl <- grm(irtposhealth2b, constrained=TRUE)
poshealth2b.grm.2pl <- grm(irtposhealth2b, constrained=FALSE)
@ 

<<poshealth2bgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(poshealth2b.grm.1pl)), caption="Coefficients for One Parameter Graded Response Model on Positive Health Scale, Split B", label="tab:poshealth2bgrm1pl"))
@ 


As can be seen from Table \ref{tab:poshealth2bgrm1pl}, there were no problems with the parameter estimates for this model. The questions marked as most difficult (1, 30 and 23) relate to emotional well being and the general health scale. The discrimination parameter is relatively low in comparison to other scales, again probably due to the non-clinical sample used in this study. 

<<poshealth2bgrm2plprint, echo=FALSE, results=hide>>=
print(xtable(coef2mat(coef(poshealth2b.grm.2pl)), caption="Coefficients for Two Parameter Graded Response Model on Positive Health Scale, Split B", label="tab:poshealth2bgrm2pl"))
@ 

Again, there were no problems with parameter estimates for this scale (Table \ref{tab:poshealth2bgrm2pl}). For the majority of items the ability estimates have increased while the discrimination parameter(s) have lowered. 

Finally, we assess the fit of each of these models on unseen data. 

<<poshealth2bgrmtest, echo=FALSE, results=tex>>=
poshealth.notb <- randitems.notb[,paste(rand, c(1, 20, 23, 26,27,30, 34,36), sep="")]
poshealth2b.grm.1pl.test <- testIRTModels(poshealth2b.grm.1pl, poshealth.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
poshealth2b.grm.2pl.test <- testIRTModels(poshealth2b.grm.2pl, poshealth.notb, gpcmconstraint=NULL, grmconstraint=FALSE)
poshealth2b.grm.test.all <- rbind(poshealth2b.grm.1pl.test, poshealth2b.grm.2pl.test)
print(xtable(poshealth2b.grm.test.all, caption="Performance of One and Two Parameter Graded Response Models on Unseen Data (Splits A and C)", label="tab:poshealth2btest"))
@ 

As can be seen from Table \ref{tab:poshealth2btest}, the one parameter model provided a better fit to the unseen data. 

Next, we examine the Mindful Attention Awareness Scale, using the same methodologies. 

First, the assumptions underlying item response theory modelling are checked. 

<<maas2bcheck, echo=FALSE, results=hide>>=
maas2b.iio <- check.iio(na.omit(maasitems2b))
maas2b.mono <- check.monotonicity(na.omit(maasitems2b))
@ 

<<maas2bitemord, echo=FALSE, results=tex>>=
print(xtable(maas2b.iio[["violations"]], caption="Item Ordering Assumption Check, MAAS Split B", label="tab:maas2bitemord"))
maas2b.s <- maasitems2b[,paste(maas, c(1:4, 7:15), sep="")]
@ 

As can be seen from Table \ref{tab:maas2bitemord}, questions 5 and 6 failed the item ordering assumptions and so are removed from the scale before further analysis. The reduced scale had no violations of the monotonicity assumption. 

Firstly, three partial credit models are fit to the data. 

<<maas2bpcm, echo=FALSE, results=hide>>=
maas2b.pcm.rasch <- gpcm(maas2b.s, constraint="rasch")
maas2b.pcm.1PL <- gpcm(maas2b.s, constraint="1PL")
maas2b.pcm.gpcm <- gpcm(maas2b.s, constraint="gpcm")
@ 

All of these partial credit models had problems with non-monotonically increasing parameter estimates, and so are not reported further.

Next, one and two parameter graded response models were fit to the data. 

<<maas2bgrm, echo=FALSE, results=hide>>=
maas2b.grm.1pl <- grm(maas2b.s, constrained=TRUE)
maas2b.grm.2pl <- grm(maas2b.s, constrained=FALSE)
@ 


<<maas2bgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas2b.grm.1pl), caption="Coefficient Estimates for MAAS One Parameter Graded Response Model, Split B", label="tab:maas2bgrm1pl"))
@ 


It can be seen from Table \ref{tab:maas2bgrm1pl} that there were no problems with the parameter estimates for this model. 

<<maas2bgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas2b.grm.2pl), caption="Coefficient Estimates for MAAS Two Parameter Graded Response Model, Split B", label="tab:maas2bgrm2pl"))
@ 

It can be seen from Table \ref{tab:maas2bgrm2pl} that there were no problems with the coefficient estimates for this solution, and the estimates show the usual tradeoff between discrimination and ability estimate parameters. 

Finally, we examine the performance of these models on unseen data.

<<maas2bgrmtest, echo=FALSE, results=hide>>=
maas.irt.notb <- maasitems.notb[,paste(maas, c(1:4, 7:15), sep="")]
maas2b.grm.1pl.test <- testIRTModels(maas2b.grm.1pl, maas.irt.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
maas2b.grm.2pl.test <- testIRTModels(maas2b.grm.2pl, maas.irt.notb, gpcmconstraint=NULL, grmconstraint=FALSE)
maas2b.grm.test.all <- rbind(maas2b.grm.1pl.test, maas2b.grm.2pl.test)
print(xtable(maas2b.grm.test.all, caption="Performance of One and Two Parameter Graded Response Models for MAAS on Unseen Data", label="tab:maas2bgrmtest"))
@ 


As can be seen from Table \ref{tab:maas2bgrmtest}, the one parameter model provided the best fit to the unseen data. 

Next, we examine the LOT-R in this split. 

<<lotr2bcheck, echo=FALSE, results=hide>>=
lotr2b.iio <- check.iio(na.omit(lotritems2b))
lotr2b.mono <- check.monotonicity(na.omit(lotritems2b))
@ 

The LOT-R had no problems with either item ordering or monotonicity. 

Firstly, three partial credit models were fit to the data. 

<<lotr2bpcm, echo=FALSE, results=hide>>=
lotr2b.pcm.rasch <- gpcm(lotritems2b, constraint="rasch")
lotr2b.pcm.1PL <- gpcm(lotritems2b, constraint="1PL")
lotr2b.pcm.gpcm <- gpcm(lotritems2b, constraint="gpcm")
@ 

All of the partial credit models had non-monotonically increasing parameter estimates and so are not reported further here. 

Next, one and two parameter Graded Response Models were fit to the data. 

<<lotr2bgrm, echo=FALSE, results=hide>>=
lotr2b.grm.1pl <- grm(lotritems2b, constrained=TRUE)
lotr2b.grm.2pl <- grm(lotritems2b, constrained=FALSE)
@ 


<<lotr2bgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(lotr2b.grm.1pl), caption="Coefficient Esitmates for One Parameter Graded Response Model, LOT-R, Split B", label="tab:lotr2bgrm1pl"))
@ 

Table \ref{tab:lotr2bgrm1pl} clearly shows that there were no obvious problems with this model. The parameter estimates are relatively low in comparison with other scales, suggesting that these items were easier to endorse. 

<<lotr2bgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef(lotr2b.grm.2pl), caption="Coefficient Estimates for Two Parameter Graded Response Model, LOT-R, Split B", label="tab:lotr2bgrm2pl"))
@ 


It can be seen from Table \ref{tab:lotr2bgrm2pl} that the ability estimates have risen while the discrimination parameters have fallen for the majority of the items (except for 7). 


Finally, we assess the performance of each of these models on unseen data. 

<<lotr2bgrmtest, echo=FALSE, results=tex>>=
lotr2b.grm.1pl.test <- testIRTModels(lotr2b.grm.1pl, lotritems.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
lotr2b.grm.2pl.test <- testIRTModels(lotr2b.grm.2pl, lotritems.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
lotr2b.grm.test.all <- rbind(lotr2b.grm.1pl.test, lotr2b.grm.2pl.test)
print(xtable(lotr2b.grm.test.all, caption="Performance of One and Two Parameter Graded Response Models on Unseen Data (Splits A and C)", label="tab:lotr2bgrmtest"))
@ 

As can be seen from Table \ref{tab:lotr2bgrmtest}, the one parameter model performed best on the unseen data. 

\subsubsection{Split C}
\label{sec:split-c}

Firstly, the RAND MOS is examined to determine how many scales it consists of. 

<<rand2caisp, echo=FALSE, results=hide>>=
rand2c.aisp <- aisp(na.omit(randitems2c))
print(xtable(rand2c.aisp, caption="Item Selection Procedure for RAND MOS, Split C", label="tab:rand2caisp"))
@ 

As can be seen in Table \ref{tab:rand2caisp} again the RAND MOS divides into three scales. 

The first scale consists of items 3 through 13, and can be best termed as physical functioning (although it contains one item from the physical role limitations subscale). 

The second scale consists of most of the physical and role limitations scales and the negatively worded items from the emotional well being and social functioning scales and can again, as in the previous split, be termed as negative health. 

The third scale consists of the rest of the items, that is the general health, social functioning and emotional well being and energy/fatigue items which are positively worded, and can best be termed as positive health. 

<<rand2cscales, echo=FALSE, results=hide>>=
physfun2c <- randitems2c[,paste(rand, c(3:13), sep="")]
poshealth2c <- randitems2c[,paste(rand, c(1,20,22:23, 26:27, 30, 34, 36), sep="")]
neghealth2c <- randitems2c[,paste(rand, c(14:19, 24:25, 28:29, 31), sep="")]
@ 

Firstly, the assumptions underlying IRT modelling must be assessed before modelling can commence. 

<<physfun2ccheck, echo=FALSE, results=hide>>=
physfun2c.iio <- check.iio(na.omit(physfun2c))
physfun2c.mono <- check.monotonicity(na.omit(physfun2c))
@ 

There were no violations of either the item ordering assumption or the monotonicity assumptions for the physical functioning scale in this split. 

The preliminaries having been dealt with, the next step was to fit three partial credit models. 

<<physfun2cgpcm, echo=FALSE, results=hide>>=
physfun2c.gpcm.rasch <- gpcm(physfun2c, constraint="rasch")
physfun2c.gpcm.1pl <- gpcm(physfun2c, constraint="1PL")
physfun2c.gpcm.gpcm <- gpcm(physfun2c, constraint="gpcm")
@ 

All three partial credit models had problems with the ability estimates in that they were not montonotically increasing, and so are not discussed further here. 

Next, one and two parameter Graded Response Models were fit to the data.


<<physfun2cgrm, echo=FALSE, results=hide>>=
physfun2c.grm.1pl <- grm(physfun2c, constrained=TRUE)
physfun2c.grm.2pl <- grm(physfun2c, constrained=FALSE)
@ 

<<physfun2cgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(physfun2c.grm.1pl)), caption="Coefficient Estimates for One Parameter Graded Response Model, Physical Functioning Scale, Split C", label="tab:physfun2cgrm1pl"))
@ 

As can be seen from Table \ref{tab:physfun2cgrm1pl}, there are no obvious problems with this model. The rather low ability estimates are interesting in that they are all negative, but the discrimination parameter is quite high suggesting that there is much information in the choosing of one response over another. 

<<physfun2cgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(physfun2c.grm.2pl)), caption="Coefficient Estimates for Two Parameter Graded Response Model, Physical Functioning Scale, Split C", label="tab:physfun2cgrm2pl"))
@ 

As can be seen from Table \ref{tab:physfungrm2pl}, the discrimination parameters have lowered for the majority of items, except for 7 and 11 which refer to either walking up one flight of stairs or along one block (which presumably the non American sample here understood). 

The final step in the model building process is to assess each models\' performance on unseen data. 

<<physfun2cgrmtest, echo=FALSE, results=tex>>=
physfun.notc <- randitems.notc[,paste(rand, c(3:13), sep="")]
physfun2c.grm.1pl.test <- testIRTModels(physfun2c.grm.1pl, physfun.notc, gpcmconstraint=NULL, grmconstraint=TRUE)
physfun2c.grm.2pl.test <- testIRTModels(physfun2c.grm.2pl, physfun.notc, gpcmconstraint=NULL, grmconstraint=FALSE)
physfun2c.grm.test.all <- rbind(physfun2c.grm.1pl.test, physfun2c.grm.2pl.test)
print(xtable(physfun2c.grm.test.all, caption="Performance of One and Two Parameter Physical Functioning Graded Response Models on Unseen Data (Splits A and B", label="tab:physfun2ctest"))
@ 

As can be seen from Table \ref{tab:physfun2cgrmtest}, the one parameter model performed best on the unseen data. 

Next, the negative health scale was checked to ensure suitability for IRT modelling. 

<<neghealth2ccheck, echo=FALSE, results=hide>>=
neghealth2c.iio <- check.iio(na.omit(neghealth2c))
neghealth2c.mono <- check.monotonicity(na.omit(neghealth2c))
@ 

The negative health scale for this split showed no failures of the item ordering assumption or of the monotonicity assumption. 

Next, three partial credit models were fit to the data. 

<<neghealth2cgpcm, echo=FALSE, results=hide>>=
neghealth2c.gpcm.rasch <- gpcm(neghealth2c, constraint="rasch")
neghealth2c.gpcm.1PL <- gpcm(neghealth2c, constraint="1PL")
neghealth2c.gpcm.gpcm <- gpcm(neghealth2c, constraint="gpcm")
@ 

All three partial credit models suffered from non-monotonically increasing parameter estimate problems, and are not discussed here further. 

Next, one and two parameter Graded Response Models were fit to the data. 

<<neghealth2cgrm, echo=FALSE, results=hide>>=
neghealth2c.grm.1pl <- grm(neghealth2c, constrained=TRUE)
neghealth2c.grm.2pl <- grm(neghealth2c, constrained=FALSE)
@ 


<<neghealth2cgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(neghealth2c.grm.1pl)), caption="Coefficient Estimates for One Parameter Graded Response Model, Negative Health Scale, Split C", label="tab:neghealth2cgrm1pl"))
@ 

As can be seen from Table \ref{tab:neghealth2cgrm1pl}, there are no obvious problems with this model. The estimated discrimination parameter is quite low, suggesting that the items convey only the information in their location thresholds. 

<<neghealth2cgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(neghealth2c.grm.2pl)), caption="Coefficient Estimates for Two Parameter Graded Response Model, Negative Health Scale, Split C", label="tab:neghealth2cgrm2pl"))
@ 

It can be see from Table \ref{tab:neghealth2bgrm2pl}, that the ability estimates have lowered significantly, as have most of the discrimination parameters. 

Finally, the performance of each of these models was examined on unseen data. 

<<neghealth2cgrmtest, echo=FALSE, results=tex>>=
neghealth.notc <- randitems.notc[,paste(rand, c(14:19, 24:25, 28:29, 31), sep="")]
neghealth2c.grm.1pl.test <- testIRTModels(neghealth2c.grm.1pl, neghealth.notc, gpcmconstraint=NULL, grmconstraint=TRUE)
neghealth2c.grm.2pl.test <- testIRTModels(neghealth2c.grm.2pl, neghealth.notc, gpcmconstraint=NULL, grmconstraint=FALSE)
neghealth2c.grm.test.all <- rbind(neghealth2c.grm.1pl.test, neghealth2c.grm.1pl.test)
print(xtable(neghealth2c.grm.test.all, caption="Performance of One and Two Parameter Negative Health Graded Response Models on Unseen Data (Splits A and B)", label="tab:neghealth2cgrmtest"))
@ 

As can be seen from Table \ref{tab:neghealth2cgrmtest}, both models performed equivalently on the unseen data, therefore the simpler one parameter model is chosen. 

Next, the assumptions are checked for the positive health scale in this split. 

<<poshealth2ccheck, echo=FALSE, results=hide>>=
poshealth2c.iio <- check.iio(na.omit(poshealth2c))
poshealth2c.mono <- check.monotonicity(na.omit(poshealth2c))
@ 

There were no violations of the item ordering or monotonicity assumptions for this scale in this split.

Next, three partial credit models were fitted. 

<<poshealth2cgpcm, echo=FALSE, results=hide>>=
poshealth2c.gpcm.rasch <- gpcm(poshealth2c, constraint="rasch")
poshealth2c.gpcm.1PL <- gpcm(poshealth2c, constraint="1PL")
poshealth2c.gpcm.gpcm <- gpcm(poshealth2c, constraint="gpcm")
@ 

All three partial credit models has non-monotonic parameter estimates, and are not analysed further here. 


Next, one and two parameter Graded Response Models were fitted to this scale.

<<poshealth2cgrm, echo=FALSE, results=hide>>=
poshealth2c.grm.1pl <- grm(poshealth2c, constrained=TRUE)
poshealth2c.grm.2pl <- grm(poshealth2c, constrained=FALSE)
@ 

<<poshealth2cgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(poshealth2c.grm.1pl)), caption="Coefficient Estimates for Positive Health One Parameter Graded Response Model, Split C", label="tab:poshealth2cgrm1pl"))
@ 

The coefficient estimates for this model are shown in Table \ref{tab:poshealth2cgrm1pl}. It can be seen that the discrimination parameter is quite low, and the ability estimates (especially for question 1) are quite high. One would expect this to alter under a two parameter model. 

<<poshealth2cgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(poshealth2c.grm.2pl)), caption="Coefficient Estimates for Positive Health Two Parameter Graded Response Model, Split C", label="tab:poshealth2cgrm2pl"))
@ 

Table \ref{tab:poshealth2cgrm2pl} shows that the speculation regarding the shape of ability estimates and discrimination parameters was quite wrong, ad Q1 retains its extremely high ability estimates and Question 22 lowers its discrimination parameter significantly while gaining an extremely high estimate for ability. Q22 refers to pain, and this appears to be an extremely strong predictor of overall health, as the other two questions with as high abilities represent overall health questions.

Finally for this scale, its performance on unseen data is assessed. 

<<poshealth2cgrmtest, echo=FALSE, results=tex>>=
poshealth.notc <- randitems.notc[,paste(rand, c(1,20,22:23, 26:27, 30, 34, 36), sep="")]
poshealth2c.grm.1pl.test <- testIRTModels(poshealth2c.grm.1pl, poshealth.notc, gpcmconstraint=NULL, grmconstraint=TRUE)
poshealth2c.grm.2pl.test <- testIRTModels(poshealth2c.grm.2pl, poshealth.notc, gpcmconstraint=NULL, grmconstraint=FALSE)
poshealth2c.grm.test.all <- rbind(poshealth2c.grm.1pl.test, poshealth2c.grm.2pl.test)
print(xtable(poshealth2c.grm.test.all, caption="Performance of Positive Health One and Two Parameter Models (Split C) on unseen data (Splits A and B)", label="tab:poshealth2cgrmtest"))
@ 

As is shown in Table \ref{tab:poshealth2cgrmtest}, the one parameter model provided a batter fit to the unseen data. 

The next stage of the analysis is to examine the usefulness of IRT models for the Mindful Attention Awareness Scale in this split.

The first step is to check the item ordering and monotonicity assumptions. 

The automated item selection procedure suggests that items 6 and 15 do not fit the scale well enough to be considered, so these items are removed first. 

<<maas2ccheck, echo=FALSE, results=hide>>=
maas2c.iio <- check.iio(na.omit(maasitems2c))
maas2c.mono <- check.monotonicity(na.omit(maasitems2c))
maas2c.s <- maasitems2c[,paste(maas, c(1:4, 7:10, 12:14), sep="")]
@ 

Additionally, item 5 fails the item ordering check, and so is alsop removed. The reduced scale had no failures of the monotonicity assumption, so modelling continues with the reduced scale. 

Firstly, three partial credit models are fit to the data. 

<<maas2cgpcm, echo=FALSE, results=hide>>=
maas2c.gpcm.rasch <- gpcm(maas2c.s, constraint="rasch")
maas2c.gpcm.1PL <- gpcm(maas2c.s, constraint="1PL")
maas2c.gpcm.gpcm <- gpcm(maas2c.s, constraint="gpcm")
@ 

None of the partial credit models are analysed further, as they all had non-monotonically increasing parameter estimates. 

Next, one and two parameter Graded Response Models are fit to the scale. 

<<maas2cgrm, echo=FALSE, results=hide>>=
maas2c.grm.1pl <- grm(maas2c.s, constrained=TRUE)
maas2c.grm.2pl <- grm(maas2c.s, constrained=FALSE)
@ 

<<maas2cgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas2c.grm.1pl), caption="Coefficient Estimates for MAAS One Parameter Graded Response Model, Split C", label="tab:maas2cgrm1pl"))
@ 


As can be seen from Table \ref{tab:maas2cgrm1pl}, the coefficient estimates appear reasonable. The discrimination parameter is relatively low, suggesting that this scale is good for all levels of abilities, even though the highest estimated difficulty parameter is only 2.048, for Q13 which is ``I often find myself occupied with the future or the past'', which is a relatively concise summary of the entire construct of mindfulness. 

Next, a two parameter model is fit to this data. 

<<maas2cgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas2c.grm.2pl), caption="Coefficient Estimates for MAAS, Two Parameter Graded Response Model, Split C", label="tab:maas2cgrm2pl"))
@ 

The two parameter model (shown in Table \ref{tab:maas2cgrm2pl}), is not that much different from the one parameter model. Of interest is that Q13 remains the most difficult question, but its discrimination parameter has come down, suggesting that it behaves similarly for participants of all ability levels. Q8 has the highest discrimination parameter of all the items and is ``I rush through activities without being really attentive to them'' and it appears that this question is the best at discrimination between those higher and lower on the construct of mindfulness. 


The final step in the analysis of the MAAS scale is to test the performance of the models on unseen  data. 

<<maas2cgrmtest, echo=FALSE, results=hide>>=
maas.irt.notc <- maasitems.notc[,paste(maas, c(1:4, 7:10, 12:14), sep="")]
maas2c.grm.1pl.test <- testIRTModels(maas2c.grm.1pl, maas.irt.notc, gpcmconstraint=NULL, grmconstraint=TRUE)
maas2c.grm.2pl.test <- testIRTModels(maas2c.grm.2pl, maas.irt.notc, gpcmconstraint=NULL, grmconstraint=FALSE)
maas2c.grm.test.all <- rbind(maas2c.grm.1pl.test, maas2c.grm.2pl.test)
print(xtable(maas2c.grm.test.all, caption="Performance of MAAS One and Two Parameter Graded Response Models on Unseen Data (Splits A and B)", label="tab:maas2cgrmtest"))
@ 

As can be seen from Table \ref{tab:maas2cgrmtest}, the one parameter model provided a better fit to the unseen data (though neither model was particularly good). 


Next, the structure of the LOT-R in this split was examined. 

<<lotr2caisp, echo=FALSE, results=hide>>=
lotr2c.aisp <- aisp(na.omit(lotritems2c))
@ 

<<lotr2ccheck, echo=FALSE, results=hide>>=
lotr2c.iio <- check.iio(na.omit(lotritems2c))
lotr2c.mono <- check.monotonicity(na.omit(lotritems2c))
lotr2c.s <- lotritems2c[,paste(lotr, c(3,4,7,9,10), sep="")]
@ 

An examination of the item ordering assumption showed that Q1 did not fit this model, and so was removed from the scale. There were no failures of the monotonicity assumption, and thus the modelling could commence. 

<<lotr2cpcm, echo=FALSE, results=hide>>=
lotr2c.pcm.rasch <- gpcm(lotr2c.s, constraint="rasch")
lotr2c.pcm.1PL <- gpcm(lotr2c.s, constraint="1PL")
lotr2c.pcm.gpcm <- gpcm(lotr2c.s, constraint="gpcm")
@ 

All Partial Credit Models suffered from non-monotonically increasing parameter estimates, and so are not analysed further here. 

Next, one and two parameter Graded Response Models were fit to the scale.

<<lotr2cgrm, echo=FALSE, results=hide>>=
lotr2c.grm.1pl <- grm(lotr2c.s, constrained=TRUE)
lotr2c.grm.2pl <- grm(lotr2c.s, constrained=FALSE)
@ 

<<lotr2cgrm1plprint, echo=FALSE, results=hide>>=
print(xtable(coef(lotr2c.grm.1pl), caption="Coefficient Esitmates for LOT-R One Parameter Graded Response Model, Split C", label="tab:lotr2cgrm1pl"))
@ 

Table \ref{tab:lotr2cgrm1pl} shows the estimated difficulty parameters for the one parameter Graded Response Model. It can be seen that the discrimination parameter is quite high, and that the most difficult question is Q10 which is ``Overall, I expect more good things to happen to me than bad''. The ``easiest'' question is Q7, which is one of the negative.ly fphrased question, suggesting that the two of these questions would be enough to garner a rough estimate of ability from participants. 


<<lotr2cgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef(lotr2c.grm.2pl), caption="Coefficient Estimates for LOT-R, Two Parameter Graded Response Model", label="tab:lotr2cgrm2pl"))
@ 

Table \ref{tab:lotr2cgrm2pl} shows the estimates for the two parameter GRM. It can be seen that Q7 is not the most discriminating question, while still having the lowest ability estimates, suggesting that it is a very good question for seperating out optimism and pessimism. Q10 is still the most difficult, but not as discriminating as Q1 and Q10 (which refer to hopes around the future and indeed are very similar questions). 

Finally, the performance of these two models is tested against unseen data. 

<<lotr2cgrmtest, echo=FALSE, results=tex>>=
lotr.irt.notc <- lotritems.notc[,paste(lotr, c(3,4,7,9,10), sep="")]
lotr2c.grm.1pl.test <- testIRTModels(lotr2c.grm.1pl, lotr.irt.notc, gpcmconstraint=NULL, grmconstraint=TRUE)
lotr2c.grm.2pl.test <- testIRTModels(lotr2c.grm.2pl, lotr.irt.notc, gpcmconstraint=NULL, grmconstraint=FALSE)
lotr2c.grm.test.all <- rbind(lotr2c.grm.1pl.test, lotr2c.grm.2pl.test)
print(xtable(lotr2c.grm.test.all, caption="Performance of LOT-R Split C One and Two Parameter Graded Response Models on Unseen Data", label="tab:lotr2cgrmtest"))
@ 


As can be seen from Table \ref{tab:lotr2cgrmtest}, the one parameter model provides the best fit to the unseen data. 

\subsection{Discussion}

There are a number of interesting findings which have emerged from
the study.  The most striking is the large negative correlation between
optimism and self reported health.  The sample in this study is quite
large, so the result is unlikely to be a statistical fluke.  That being
said, the result is problematic to explain, given the large amount
of evidence of beneficial effects of optimism on health\cite{rasmussen2009optimism}.

That being said, there have been some findings, where higher optimism
has not has been associated with health outcomes.  There are two major
explanations for the curious and unexpected phenomenon,given the links
established by a recent meta-analysis \cite{rasmussen2009optimism}.
The first centers on the dimensionality of the optimism construct.
Many believe that these results are caused by optimism self report
measures being reflections of two interlinked constructs, optimism
and pessimism \cite{Herzberg2006} as established in a factor analytic
study in a sample of over 46000 participants.  Some authors claim that
the apparently contradictory results suggest that the pessimism part
of the construct is the driver of the effects on health, and that
the correlations between the two constructs decline with age.  This
viewpoint was partially supported by the recent meta-analysis which
found that pessimism had a larger effect on health, though the difference
between optimism and pessimism was not significant \cite{rasmussen2009optimism}.
The other viewpoint argues that the effects of optimism on health
are mediated by negative and positive affect, and that high levels
of negative affect can either negate or reverse the optimism-health
link \cite{Baker2007}.  The aforementioned Baker study found that
the optimism health link was entirely mediated by negative affect.
Nonetheless, the balance of the evidence suggests that optimism has
beneficial consequences for health and healthy behaviours.

An explanation for this finding might be that it was the result
of high levels of negative affect in the population.  However, this
variable was not measured, so such an explanation can be regarded
as speculative at best.  It is worth noting however, that the original
literature of the beneficial effects of optimism on health focused
on cellular immunity, which is obviously quite different from self
reported health.A problem with this explanation is that reports in the
literature indicate that the optimism-health link is larger when self
report methods are used \cite{rasmussen2009optimism}.  Age may also
have been a factor, as the regression weight for this vaiable was
negative, which suggests that the relationship may have been different
if this research had been carried out in a sample with a broader distribution
of ages.  We do not have a good explanation for this finding.
This issue could be resolved with a prospective study
measuring optimism and health at baseline, and having participants
report health problems and visits to medical professionals over the
course of a year.  Such a study could allow the casual chains of this
effect to be untangled.

Another interesting finding which arose from this research is the
impact of mindfulness scores on other health variables.  MAAS scores
correlated positively with all of the health sub-scales, very significantly
in the case of emotional well-being.  This may suggest that brief mindfulness
interventions may be of use for improving overall population health,
both physical and mental.  That being said, the researchers would like
to earnestly observe that the issues surrounding the mindfulness construct
itself and its relations with mindfulness meditation practice need
to be resolved before such strong conclusions can be drawn.

Another fascinating finding in this research is the strong negative correlation
between mindfulness and optimism.  Our study appears to have been the first to assess
these constructs using self-report measures, and this finding was not expected to occur.
MBSR programs have been found to increase optimism in a number of studies \cite{Carson2004},
but our results seem to show that mindfulness and optimism may be inversely related.

There are a number of reasons why this could be so.  Optimism is defined as generalised positive outcome expectancies about the future,
while mindfulness is defined as non-judgmental awareness of the content of thoughts.
It seems plausible that increased mindfulness could lead persons to become less optimistic,
 as their newfound awareness of their own thought patterns and behaviours makes them aware
that events have not always worked out well.  This increased awareness could temper future
assessments of the future, and decrease optimism as measured by the Life Orientation Test Revised.

This study also confirms the proposed one factor structure for the MAAS, in line with previous research.
This sample also appears to show that the LOT-R can be modelled without loss of information with
just one factor.  We also demonstrated a replicable and parsimonous 4 factor structure for the RAND MOS,
and our results cast further doubt on the notion that these factors are uncorrelated.

It is worth noting that in all cases, parallel analysis did not provide a good measure of
the best number of factors to retain.  For all three measures, the MAP criterion provided
a more accurate metric.  This may have resulted as parallel analysis procedures tend
to sample from a normal distribution, and this condition was not met for any of our
variables.  We would argue that the use of multiple decision criteria on a regular basis
in factor analytic research would help us to understand which method suits a particular
application best.

A major limitation of this study was the exclusive use of self report
measures, and a student sample.  That being said, the sample was large
and representative of the general student population.  Given that over
70\% of Irish people of this age group now attend college, it could
be argued that this sample is relatively representative of Irish young
people at large.  This, however, is somewhat speculative and further
research would need to investigate this proposition further.

In conclusion, this study points towards the importance of considering
psychosocial variables and their impact on health, and suggests that
further research is needed to examine how these psychological variables
are mediated by culture into differential biological outcomes.


<<writefileforexperiment, echo=FALSE, results=hide>>=
save.image("healthforthesis.rda")
@ 


%%% Local Variables:
%%% TeX-master: "PlaceboMeasurementByMultipleMethods"
% %%% End:
