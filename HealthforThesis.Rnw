
<<importdata, echo=FALSE, results=hide>>=
hom<-read.csv("HOM data for RFINAL.csv")
hom <- hom[,1:65]
write.csv(hom, "homfinal.csv")
hom1 <- hom[with(hom, CollectMeth=="Paper"),]
hom2 <- hom[with(hom, CollectMeth=="Online"),]
## setOptions("xtable.include.rownames"=FALSE)
## setOption("xtable.format.args", list=(big.mark=","))
@ 



\section{Introduction}
\label{sec:introduction}

In recent years, psychologists and researchers have become
more interested in studying the effects of positive psychological variables
including positive affect, mindfulness, optimism and  on psychological and physical 
health\cite{Miller2009,dockray2010positive,pressman2005does,Steptoe2006,steptoe2008positive,dockray2010positive}.
Along with positive emotions including happiness and gratitude, much of the clinical research examining 
positive psychology constructs has focused on the role of mindfulness
 and optimism in health and wellbeing\cite{Carver2010,grossman2008measuring} .
Mindfulness has tended to be researched as an outcome to meditation programmes, rather than as a trait or personality disposition.
 Futhermore, the roles of mindfulness and optimism as personality characteristics have rarely, if ever, 
 been researched alongside. In the next section, we briefly discuss the constructs of mindfulness and optimism, 
 outlining some of the recent empirical research based largely on systematic reviews,
  before introducing the present study which embraces these two psychological variables to examine 
  the extent to which each acts as a predictor of health in a student population.

\subsection{Mindfulness}

The construct of Mindfulness or 'attentional control' has been defined as: 'a mental ability which facilitates a direct and 
immediate perception of the present moment with non-judgemental awareness' \cite{kohls2009facets} (p. 2). Derived from the Buddhist 
contemplative traditions, mindfulness represents attention to the thought process, rather than to thought content \cite{brown2007addressing}. 
Jon Kabat-Zinn popularised the practice of mindfulness in the West \cite{Kabat-Zinn2003}, developing an eight-week 
long Mindfulness Based Stress Reduction (MBSR) programme which is now often used in clinical settings for patients with chronic illnesses. 

In a study of cancer patients mindfulness training was found to reduce stress and to improve quality of life \cite{Carlson2007}, 
while in a study of patients who were HIV positive, MBSR training was associated with improved natural killer cell activity 
\cite{Robinson2003}. Meta-analyses have suggested that MBSR programmes have an effect size of d=.5 for psychological variables 
such as quality of life and mental health, and d=.2 for physical outcome variables such as cortisol levels and immune function \cite{Grossman2004}.
Measures of the construct of mindfulness date from the early years of this century. Mindfulness has been operationalised 
into a number of different scales including the Mindful Attention Awareness Scale (MAAS) \cite{brown2003benefits} , 
the Kentucky Inventory of Mindfulness Skills \cite{Ruth2004} and the Five Facet Mindfulness Questionnaire (FFMQ), 
which was developed by the factor analysis of items from a number of different scales \cite{Ruth2006}. The most popular instrument is
 the Mindful Attention Awareness Scale (MAAS) \cite{brown2003benefits}.
Interestingly, while higher MAAS scores have been found to be associated with meditation experience in some 
studies \cite{brown2003benefits}, other research using student samples found no significant correlation between 
MAAS scores and experience with meditative practices (assessed by self report) \cite{MacKillop2007} . This finding which
 was also replicated by Thompson and Waltz \cite{thompson2007everyday}  who suggest that this may be due to the fact that 
mindfulness during meditation may be a state like construct, while mindfulness in everyday life may be a trait like construct. 
So while various rigorous reviews of clinical trials have found MBSR programmes to be effective for
 reducing stress \cite{Chiesa2009,praissman2008mindfulness} , it is still unclear whether individuals 
with higher levels of mindfulness are also psychologically healthier.
 MBSR programs have also been associated with higher levels of optimism
 \cite{Carson2004}, but mindfulness and optmism have not tended to be researched together. 
We identified only one study where mindfulness levels were correlated positively 
with self reported health \cite{Hansen2009}. Hansen, using a cross-sectional design, 
found that MAAS scores correlated with a five point one item measure of health,   
a very crude measure of health status. In a recent meta-analysis of 29 studies, 
Giluk explored the relationship between mindfulness, Big Five personality and affect \cite{giluk2009mindfulness}.
 She found that mindfulness was strongly correlated with neuroticism and negative affect,
 though its not obvious whether being mindful lowers neuroticism or 
 whether neuroticism interferes with mindfulness. 



\subsection{Optimism}
Optimism, which is defined as positive generalised outcome expectancies
about the future \cite{Carver2010} is perhaps the most well known of
the psychological influences on health.Recently reviewed in relation to its 
effect on physical health and wellbeing \cite{Carver2010}, optimism has been used as
a predictor for many years in the area of Psychoneuroimmunology (PNI)
and appears to be usually associated with better health outcomes than
is pessimism \cite{Baker2007} \cite{Conway2008}. Optimism has also
been associated with lower mortality risk in a large longitudinal
cross-sectional study of individuals at risk for cardio-vascular disease
(the Women's Health Initiative) \cite{Tindle2009}. A meta-analysis
has also confirmed this link between optimism and better coping styles,
as well as a strong negative relationship between optimism and negative
affect \cite{andersson1996benefits,Segerstrom2006}. Higher levels
of optimism have also been associated with quicker recovery from surgery,
insulin therapy and chemotherapy \cite{Allison2000}. A prospective
study looking at outcomes from a group of head and neck cancer patients
found that optimists consistently reported better health outcomes
than pessimists \cite{Allison2000}. A meta-analysis reported on correlations
between optimism and post-traumatic growth, which found a mean effect
size of 0.2, but the studies were of quite poor quality, so this finding
must be regarded as tentative \cite{Bostock2009}.

The question of the mechanism by which optimism manifests differences
in health is still unclear. Some researchers argue for a direct effect
of optimism on immune function ,  while others
argue that optimism exerts its protective effects through the effects
of persistent striving after health goals \cite{Segerstrom2003}.

In summation, optimism is well known as a predictor of health, both self reported and objectively assessed and mindfulness, while a much newer construct, has also been associated with health in a number of studies.
The aim of this study was to examine the relationshiop between mindfulness, optimism and subjective health in a sample of University students. We hypothesised that both optimism and mindfulness would be positively associated with health.


\subsection{Aims of the Research}

As discussed above, optimism is well known as a predictor of health, both self reported and objectively assessed and mindfulness, while a much newer construct, has also been associated with health in a number of studies.  In addition, optimism has been associated with the placebo response in some research, while mindfulness has been proposed as a mediator of the relationship between explicit and implicit measures.
The primary aim of this part of thesis was to develop and test psychometric models that could be used to predict scores according to IRT and factor analysis criteria in the experimental part of the research.

The major hypotheses of this part of the thesis were as follows:
\begin{itemize}
\item Optimism and mindfulness would be positively associated with health.

\item The RAND MOS would have 8 first order factors%% , and two second order factors, which would be correlated

\item The MAAS would have one factor

\item The LOT-R would have one factor.
\end{itemize}

The major types of analysis carried out are described in the Methodology chapter, as are the sample sizes and sampling methods. 

The response rate for Sample One (paper) was approximately
90\% of those asked, while the response rate for Sample Two (Online)
was 10\%.

\section{Methods}

The methods used for this part of the thesis were primarily psychometric. Cross-validation approaches (described in Chapter \ref{cha:methodology}) were applied to both samples to increase generalisability of the models to the experimental portion of the research. This section describes the measures used for this part of the study, followed by a description of the sample, and concludes with a description of the methods of analysis used in this study. 

\subsection{Measures for Health, Optimism and Mindfulness Research}

There were three measures used for this part of the research.
\begin{enumerate}
\item \textit{RAND-MOS}: The RAND Medical Outcomes Survey produced the most widely used instrument of HRQoL (Health Related Quality of Life) worldwide \cite{hays1993rand}. The instrument was later revised and the number of response categories standardised across scales and renamed the SF-36. The older version was used for this research, as they are extremely similar and the newer version is under copyright and expensive to use, even for non-commercial research. The RAND-MOS has 36 questions, and is divided into 8 sub-scales, General Health (GH),
Physical Functioning (PF), Role Limitations (RL), Emotional Role Limitations
(RLE), Pain (PN), Energy (EN), Emotional Well Being (EMWB) and Social
Functioning (SF). All sub-scales showed acceptable reliability
(>.7) in this and in other studies over the years\cite{Lam2007, Ferreira2000}.
The instrument has 8 first order factors and two higher order factors
\cite{Hann2008}. The scale involves dichotomous, trichotomous and
five and six point scales for various items, so all questions are
transformed to a 100 point scale before analysis, where higher scores
represent better functioning.
\item The \textit{Mindful Attention Awareness Scale (MAAS)}
\cite{brown2003benefits} is a 15 item scale which is scored on
a six point scale from ``almost always'' to ``almost never''. The scale usesquestions which measure mindlessness. The summary score is produced
from the mean of all indvidual scores. The scale has shown adequate
psychometric validity in many samples, with alpha ranging from 0.7
to 0.9\cite{brown2003benefits,Ruth2006}.
\item \textit{Life Orientation Test, Revised:} The Life Orientation Test Revised (LOT-R) was developed and revised
by Scheier and Carver \cite{Scheier1994}, and consists of 10 items.
Three of the items load on pessimism, three on optimism and four are
distractor items. The LOT-R has shown excellent psychometric validity,
and is very commonly used as a measure of optimism/pessimism. The
scale is scored on a 5 point scale,
and the sum of all items after items 3,7 and 9 are reverse coded are taken to produce the overall score.
\end{enumerate}



\subsection{Sampling for Health, Optimism and Mindfulness Data}

The first 392 participants completed the forms by hand between August and October 2009. The participants were sampled pseudo-randomly from all of the public areas (coffeeshops, restuarants etc) of the campus.

Following this pen and paper approach to sampling, the survey was sent to a random selection of students via email on December 12th 2009, and data was collected and analysed from this point until the 24th of December 2009. Differences between the samples and the possible effects of these on the results obtained are discussed below. %% In addition, due to the unexpected results of the analysis, a third sample was collected in Summer 2011. 

\subsection{Analysis for Health, Optimism and Mindfulness Data}
% Analysis was carried out seperately on the two samples, to allow for development of factor analysis and IRT models on the first sample and validation on the second. In addition, it could not be assumed that two samples collected in different ways would be comparable. 

Firstly, the proportion of missing data was identified, and multiple imputation was employed to combat this problem in cases where there were substantial amounts of missing data. This was only necessary in the case of the sample collected by online methods. 


The data was checked for errors in entry or recording using summary functions and plots. Following this, the question responses were recoded according to the instructions.

Following this, the summary scores were calculated. Next, summary statistics and characteristics of the data were reported. 
Following this, a correlation matrix for the data was calculated and analysed.

% After these preliminary procedures, the data-set was analysed to assess if there were any differences in the samples which could be attributed to collection method (pen and paper versus on-line). As there were no major differences between the two samples, further analyses were carried out on the two samples as a whole.

Next, simple reliability analyses (Cronbach's $\alpha$) were carried out on the scales themselves. Following this, parallel analysis, the MAP criterion and the scree plot were used to estimate the number of factors which could be extracted from the data. After this, factor solutions were extracted using principal axis methods, with maximum likelihood estimation used if these failed to converge. Primarily, direct oblimin methods of rotations were utilised, but promax rotations were also applied to ensure that the proposed structure was not overly sensitive to the methods of rotation.

After the various factor structures were obtained, they were plotted and analysed for interpretability. Communalities and uniquenesses were assessed to ensure that there was no over or under factoring in the solutions. Communalities were then graphed against the number of factors extracted and the methods of extraction to provide a simple graphical guide to the usefulness of each solution.

Following this procedure of extraction and interpretation, Structural Equation Modelling was applied to each of the proposed factor solutions. The optimal factor solution was chosen using the AIC of each fitted model, along with the RMSEA of the proposed solutions.

The successful SEM models from the first sample were then tested on a subset of the second sample, to determine their performance on new data. 

Following the development and testing of SEM models, each of these was tested on the validation set and factor scores were created for each of the measures.

Following the investigation of structure with the methods of classical test theory, the scales were analysed using Rasch models and item response theory. Firstly, Mokken analyses  were run, in order to check the assumptions of monotonicity, local independence and to assess how many sub-scales the analysis should be carried out on. 

Following this, a Rasch model was fitted to the data, and person and item parameters estimated from the data. Item and person fit statistics were also calculated, and as this model did not fit any of the three scales, a two parameter model was fit to the data. Again, item and person estimates were obtained and the relevant fit statistics calculated. 


After this, linear regressions were run to examine the differential effects of each of the correlated variables. Stepwise selection on the training set was carried out, along with lasso and ridge regression  methods. Within each split, each regression model used ten-fold cross-validation to choose the optimal penalty criterion. 

The performance of all methods was then assessed on held-out data. In the case of Sample one, some of sample 2 was used as a heldout data set. For Sample two, the entire dataset was split into three splits, and the cross validation procedure carried out for each. The splits were kept quite large (approximately 300 non missing observations) to allow for psychometric models to be fit to each split seperately, and to be able to compare the performance of simple mean/sum scores against the factor scores and ability estimates derived from the psychometric modelling procedures. 

The approach taken to the psychometric analysis of the second sample of data was as follows.  Firstly, factor models were built on the two remaining samples from this dataset (the first having been used to validate the results from Sample 1).  Next, a CFA was run on each of the other samples, such that if the model was developed on the b sample, it was tested on both the a and the c sample.  This provides a better measure of accuracy and replicability for each of the proposed factor structures.  Finally, the most successful model was back-tested on the data from sample 1. The model chosen by this procedure was then used to predict factor scores for each of the participants, and was also used to predict these scores for the experimental portion of the research.

\subsection{Crossvalidation Approaches}
\label{sec:crossv-appr}

\subsubsection{Crossvalidation for factor selection}
\label{sec:crossv-fact-select}

 As discussed in the Methodology, there are two main approaches here, either item-subject CV (known as Gabriels method) or the typical ten fold validation common in  machine learning (Wolds method). Both of these approaches were examined for the RAND items, and the results are shown below. In the gabriel approach, a leave two of item and subject out was used, while ten fold cross-validation was used for the Wold method. One problem with this approach is that there were a huge number of non-responses to questions 13-16 on the RAND MOS, for unknown reasons.  This brings down the potential sample on this instrument to 281, which is not enough for the full analysis.  However, it is enough to test the factor solutions from the first sample, and as the RAND was not used in the experimental part of the research, this should be sufficient.

\subsubsection{CrossValidation for Psychometric Models}
\label{sec:crossv-psych-models}

 A solution that was proposed to this problem was cross-validation.  Typical cross-validation holds back some data in order to test the models developed on the rest.  This hold-out sample is typically of the order of 10\%.  However, for factor analyses, around 300 observations are typically needed for accurate estimation of parameters.  Therefore, models were developed on the first sample, and then will be fitted to a subset of the second sample.  This procedure will then be repeated with the data not used for testing in the second sample and the first sample.  This will allow the issues of overfitting to be avoided, and will allow for the most promising models to be applied to the experimental data, which by itself would not be sufficient to engage in any useful psychometric analyses.


The testing will be carried out using a few different methods.  Firstly, the predict scores method will be used for all factor analytic solutions.  Secondly,  CFA models will be fitted to the new data, allowing for comparision of their effectiveness on unseen data.  

A different procedure will be followed for the IRT models, using  three fold cross validation used to build an IRT model on each of the ten segments, and then comparing the estimates ability scores from the model built on the 33\% of the data, and from the estimates built on the held-out data. The difference between these two measures will provide an estimate of error for each of the model\'s predictive capability. This will then provide a metric for the selection of the best model, which can then be applied to the experimental data. 


\section{Results}


\part{Sample One}

\section{Missing Data Analysis}

The first step in the analysis was the assessment of how much data was missing.

<<loadpackages, echo=FALSE, results=hide>>=
require(ggplot2)
require(psych)
require(xtable)
require(OpenMx)
require(car)
require(arm)
require(caret)
require(nFactors)
require(bcv)
require(glmnet)
@

<<sourcefunc, echo=FALSE, results=hide>>=
source("func.R")
@




<<scorescalestransform, echo=FALSE, results=hide>>=
grep.rand <- grep("^RANDQ", x=names(hom))
randitems <- hom[,grep.rand]
rand <- "RANDQ"
randset1 <-paste(rand, c(1, 2, 20, 22, 34, 36), sep="")
randset1 <- hom[,randset1]
randrecode1 <- RecodeMany(randset1, vars=c("RANDQ1", "RANDQ2","RANDQ20", "RANDQ34","RANDQ36"), Recodings=("1=100;2=75;3=50;4=25;5=0"))
randset2 <- paste(rand, c(3:12), sep="")
randset2 <- hom[,randset2]
randrecode2 <- RecodeMany(randset2, vars=c("RANDQ3", "RANDQ4","RANDQ5","RANDQ6","RANDQ7","RANDQ8","RANDQ9", "RANDQ10","RANDQ11","RANDQ12"),Recodings="1=0;2=50;3=100")
randset3 <- paste(rand, c(13:19), sep="")
randset3 <- hom[,randset3]
randrecode3 <- RecodeMany(randset3, vars=c("RANDQ13", "RANDQ14", "RANDQ15", "RANDQ16","RANDQ17","RANDQ18", "RANDQ19"), Recodings="1=0;2=100")
randset4 <- paste(rand, c(21,23,26,27,30), sep="")
randset4 <- hom[,randset4]
randrecode4 <- RecodeMany(randset4, vars=c("RANDQ21", "RANDQ23", "RANDQ26","RANDQ27","RANDQ30"), Recodings="1=100;2=80;3=60;4=40;5=20;6=0")
randset5 <- paste(rand, c(24,25,28,29,31), sep="")
randset5 <- hom[,randset5]
randrecode5 <- RecodeMany(randset5, vars=c("RANDQ24", "RANDQ25", "RANDQ28", "RANDQ29", "RANDQ31"), Recodings="1=0;2=20;3=40;4=60;5=80;6=100")
randset6 <- paste(rand, c(32,33,35), sep="")
randset6 <- hom[,randset6]
randrecode6 <- RecodeMany(randset6,vars=c("RANDQ32","RANDQ33","RANDQ35"), Recodings="1=0;2=25;3=50;4=75;5=100")
randrecoding <- ls(pattern="randrecode")
randrecoding.df <- as.data.frame(lapply(randrecoding, function (x) get(x)))
randsortpaste <- paste(rand, c(1:36), sep="")
randitems.unscored <- hom[,grep.rand]
randitems.scored <- randrecoding.df[,randsortpaste]
hom[,grep.rand] <- randitems.scored
hom <- createSumScores(hom) 
hom1 <- hom[hom$CollectMeth=="Paper",]
hom2 <- hom[hom$CollectMeth=="Online",]
@




<<missingdata, echo=FALSE, results=hide>>=
paper.missing <- sapply(hom1, function (x) sum(is.na(x)))
online.missing <- sapply(hom2, function (x) sum(is.na(x)))
@
\begin{figure}
<<label=papermissingplot, echo=FALSE, fig=TRUE>>=
ggplot(as.data.frame(paper.missing),aes(x=paper.missing, y=..density..))+geom_density()
@
  \caption{Density Plot of Missing values in paper sample. x Axis is the proportion of responses with that  number of missing observations in each variable }
  \label{fig:papermissingplot}
\end{figure}

\begin{figure}
<<onlinemissingplot, echo=FALSE, fig=TRUE>>=
  print(ggplot(as.data.frame(online.missing), aes(x=online.missing))+geom_density())
@
  \caption{Histogram of Missing Values, Online sample}
  \label{fig:onlinemissingplot}
\end{figure}




As can be seen from Figure \ref{fig:papermissingplot}, there are quite low levels of missing data for the sample collected by paper.


However, the situation is very different with the second sample, as can be seen from Figure \ref{fig:onlinemissingplot}, where there are a number of items which have between six and eight hundred missing values.  This was investigated further, as this amount of missing values in a small set of the data may cause problems in the course of the analysis. 

<<missingvaluestable, echo=FALSE, results=tex>>=
missing.many <- online.missing[online.missing>300]
missing.many.df <- as.data.frame(missing.many)
names(missing.many.df) <- "Number of Missing Observations"
missing.many.xtab <- xtable(missing.many.df, label="tab:missingmanytable", caption="Number of Missing Observations for RAND MOS items with greater than 10 percent missingness")
print(missing.many.xtab)
@

It can be seen from Table \ref{tab:missingmanytable} that the problems with missing data are concentrated in four consecutive questions, RAND questions 13 through 16. These questions all load on the Role Limitations subscale, explaining why this subscale shows up with lots of missing data.  The consecutive nature of the data suggests that the reason for this may be that participants believed that it was only necessary to answer one of the questions, though that does not explain why so many people did not answer the first question.

%% Next, we will impute the missing data using a multiple imputation procedure (as discussed in the Methodology).

<<impute, eval=FALSE, echo=FALSE, results=hide>>=
require(mice)
hom.imp <- mice(hom[,2:length(hom)], m=10) #remove first column as it causes problems with the imputation
hom.comp1 <- complete(hom.imp, 1)
hom.comp2 <- complete(hom.imp, 2)
hom.comp3 <- complete(hom.imp, 3)
hom.comp4 <- complete(hom.imp, 4)
hom.comp5 <- complete(hom.imp, 5)
hom.comp6 <- complete(hom.imp, 6)
hom.comp7 <- complete(hom.imp, 7)
hom.comp8 <- complete(hom.imp, 8)
hom.comp9 <- complete(hom.imp, 9)
hom.comp10 <- complete(hom.imp, 10)
homlist <- list(hom.comp1, hom.comp2, hom.comp3, hom.comp4, hom.comp5, hom.comp6, hom.comp7, hom.comp8, hom.comp9, hom.comp10)
for (i in 1:length(homlist)) {
  homi <- homlist[[i]]
  write.csv(homlist[[i]], file=paste("homcomp", i, ".csv", sep=""))
}
  
@ 

<<readcomphom, echo=FALSE, results=hide>>=
homfiles <- list.files(pattern="homcomp")
homlist <- lapply(homfiles, read.csv)
@ 

\section{Descriptive Statistics}
% \section{Methods}



To begin the analysis, frequencies, means and ranges were calculated
for all the variables of interest.  The results of this analysis can
be seen in Table \ref{tab:sumstatscales} , below.

<<sumstats, echo=FALSE,results=tex>>=
hom.tot <- hom1[,66:75]
tot.sum <- summary(hom.tot)
tot.xtab <- xtable(tot.sum, label="tab:sumstatscales", caption="Summary Statistics for Health Scales, Mindfulness and Optimism Measures")
print(tot.xtab, scalebox=0.5, include.rownames=FALSE)
@



In Table \ref{tab:democollect} the breakdown of the demographics
of the sample by Collection Method is shown.

<<demostats, echo=FALSE, results=tex>>=
hom.demo <- hom1[,2:8]
hom.demo.xtab <- xtable(summary(hom.demo), label="tab:democollect", caption="Demographic Statistics for Sample One (paper sample)")
print(hom.demo.xtab, scalebox=0.6, include.rownames=FALSE)
@


Mindfulness levels were quite high, while optimism levels were
at the half way point of the scale. Health levels were quite high for all of the subscales, which makes sense given the non-clinical sample involved in this research. 



\section{Inferential Statistics}

Following these preliminary analyses, the main hypotheses can now
be addressed.



\begin{figure}
<<pairsplot, echo=FALSE, fig=TRUE>>=
pairs.panels(na.omit(hom.tot))
@
\caption{Pairs plot for Scale totals of Health, Optimism and Mindfulness Data.  Top triangle has correlations scaled by their size, bottom triangle has scatterplots with locally weighted regression lines, diagonal has histograms with density estimation.  GH=Gen Health, PF=Physical Funct, RL=Role Lim, RLE=Emotional Role Lim, EmWB=Emotional Well Being}
\label{fig:pairsplot}
\end{figure}

<<corrmatrix, echo=FALSE, results=tex>>=
hom.tot.cor <- cor(hom.tot, use="pairwise.complete.obs", method="spearman")
hom.tot.cor <- as.data.frame(hom.tot.cor)
names(hom.tot.cor) <- c("Physical Functioning", "Role Limitations", "Emotional Role Limitations", "Energy Fatigue", "Social Functioning", "Pain", "General Health", "Mindfulness", "Optimism")
hom.tot.cor.xtab <- xtable(hom.tot.cor, label="tab:scalecorr", caption="Correlations Between Scales GH=Gen Health, PF=Physical Funct, RL=Role Lim, RLE=Emotional Role Lim, EmWB=Emotional Well Being.All relationships significant at p<0.01")
print(hom.tot.cor.xtab, scalebox=0.4)
@



As can be seen from Table \ref{tab:scalecorr}, the optimism hypothesis
was not supported.  Contrary to predictions, optimism was negatively
correlated with health.
Possible explanations are examined in the Discussion section.  In fact, optimism correlated negatively with all of the other totals, suggesting that something strange happened in the sample.

\begin{figure}
<<optplot1, echo=FALSE, fig=TRUE>>=
optplot1 <- ggplot(hom1, aes(x=generalhealth, y=optimism))+layer(geom="smooth", method="lm")
print(optplot1)
@  
  \caption{Plot of General Health against optimism with a linear regression smooth line. Dark areas on plot represent the confidence intervals (95\%)}
  \label{fig:optplot1}
\end{figure}


The figure above \ref{fig:optplot1}, shows a linear regression of optimism against general health in the sample collected by paper.  It can be clearly seen that the relationship is negative, an unexpected and surprising occurence.  %a fact which is borne out in the examination of Figure \ref{fig:optplot2} below.




\begin{figure}
<<optplotgend, echo=FALSE, fig=TRUE>>=
optplotgend <- ggplot(na.omit(hom1), aes(x=generalhealth, y=optimism))+layer(geom="smooth", method="lm")+facet_grid(.~Gender)
print(optplotgend)
@  
  \caption{Plot of General Health against Optimism stratified by Gender using a linear regression smooth. Dark edges represent errors of estimation}
  \label{fig:optplotgend}
\end{figure}


It can be seen from Figure \ref{fig:optplotgend} that participants of both genders showed the relationship in the same direction, with participants reporting greater health reporting less optimism.  Below, the effect of college course is examined, in case this could be skewing results.

\begin{figure}
<<optplotcollege, echo=FALSE, fig=TRUE>>=
optplotcoll <- ggplot(hom1, aes(x=generalhealth, y=optimism,colour=College))+layer(geom="smooth", method="lm")## +facet_grid(~.College)
print(optplotcoll)
@  
  \caption{Plot of General Health against Optimism stratified by Faculty of respondents. Dark areas of plot represent errors of estimates. }
  \label{fig:optplotcollege}
\end{figure}



Again, we see that the result is a general trend across all subgroups divided by college, suggesting that it is the result of a general pattern across the sample rather than being driven by some small number of abberant observations.


\begin{figure}
<<optmaasplot, echo=FALSE, fig=TRUE>>=
optplot.maas<- ggplot(na.omit(hom1), aes(x=generalhealth, y=optimism, colour=mindfulness, size=mindfulness))+geom_point(method="lm")
print(optplot.maas)
@  
  \caption{Scatterplot of General Health against Optimism, stratified by Mindfulness }
  \label{fig:optplotmaas}
\end{figure}


It can be seen from Figure \ref{fig:optplotmaas} that higher levels of mindfulness are associated with higher levels of Health and also with lower levels of Optimism.

\begin{figure}
<<lotrageplot, echo=FALSE, fig=TRUE>>=
lotrage <- ggplot(na.omit(hom1), aes(x=Age, y=optimism, ))+layer(geom="smooth",method="lm")
print(lotrage)
@  
  \caption{Regression Line of Optimism against Age (linear regression smooth). Dark areas on plot surrounding line represent confidence intervals}
  \label{fig:lotrageplot}
\end{figure}


Above, in Figure \ref{fig:lotrageplot} it can be seen that Optimism levels decreased as a function of age, but this finding should be taken with caution as the majority of partiticipants in this study were between 18 and 25, and those who were not are not likely to be typical of the general population (as they are all students).

Below, we examine if the relationship between mindfulness and health can account for the unexpected effects of Optimism.
\begin{figure}
<<healthmaassamp, echo=FALSE, fig=TRUE>>=
healthmaas.samp <- ggplot(na.omit(hom), aes(x=generalhealth, y=mindfulness, colour=CollectMeth))+layer(geom="smooth", method="lm")
print(healthmaas.samp)
@  
  \caption{General Health against Mindfulness, Stratified by Method of Collection, linear regression smooth, dark areas on plots represent confidence intervals}
  \label{fig:healthmaasmethplot}
\end{figure}


It can be seen from Figure \ref{fig:healthmaasmethplot} that the relationship between health and mindfulness was slightly stronger in the paper sample, but not significantly so.
\begin{figure}
<<healthmaasgend, echo=FALSE, fig=TRUE>>=
healthmaas.gend <- ggplot(na.omit(hom1), aes(x=generalhealth, y=mindfulness, colour=Gender))+layer(geom="smooth", method="lm")
print(healthmaas.gend)
@  
  \caption{General Health against Mindfulness Stratified by Gender using a linear regression smooth. Dark areas represent errors of estimation}
  \label{fig:healthmaasgend}
\end{figure}


Again, from Figure \ref{fig:healthmaasgend} it can be seen that Gender did not appear to have a substantial effect on mindfulness totals, although it is interesting to note that the range of health scores reported was much greater in the female participants.
\begin{figure}
<<healthmaas, echo=FALSE, fig=TRUE>>=
healthmaas1 <- ggplot(na.omit(hom1), aes(x=generalhealth, y=mindfulness, colour=College))+layer(geom="smooth", method="lm")
print(healthmaas1)
@  
  \caption{General Health against Mindfulness stratified by College using a linear regression smooth}
  \label{fig:healthmaascoll}
\end{figure}


As can be seen from Figure \ref{fig:healthmaascoll} the relationship between general health and mindfulness levels is positive, and constant across the different groups of students.

\begin{figure}
<<maasage, echo=FALSE, fig=TRUE>>=
maas.age <- ggplot(na.omit(hom1), aes(x=Age, y=mindfulness, ))+layer(geom="smooth", method="lm")
print(maas.age)
@  
  \caption{Age against Mindfulness Scores using a linear regression smooth}
  \label{fig:maasage}
\end{figure}


MAAS scores were associated with greater health as expected, as can be seen from Table \ref{tab:scalecorr}.

\section{Regression Analyses}

Given the correlation matrix reported above in Table \ref{tab:scalecorr}, regression analyses were run on the three major variables (General Health, Mindfulness and Optimism) to determine which other variables were involved in the effect.

\subsection{Optimism}

%  A maximal model approach was
% taken for this regression.  The fit was carried out using lasso regression (as described in the methodology) and the $\lambda$ parameter was selected by using ten-fold cross-validation.

<<cvsetup, echo=FALSE, results=hide>>=
hom1.full <- na.omit(hom1)
hom1.partition <- with(hom1.full, createDataPartition(optimism, list=FALSE, p=0.8))
hom1.train.opt <- hom1.full[hom1.partition,]
hom1.test.opt <- hom1.full[-hom1.partition,]
mytrain <- trainControl(method="cv")
@ 

<<opttrain, echo=FALSE, results=hide>>=
testpred <- hom1.test.opt[,c("Age", "pain", "mindfulness", "socialfunctioning", "rolelim", "rolelimem", "emwellbeing", "physfun", "energyfat", "generalhealth")]
trainopt <- with(hom1.train.opt, optimism)
trainpred <- hom1.train.opt[,c(4, 66:74)]
testopt <- with(hom1.test.opt, optimism)
@ 

<<optridge, echo=FALSE, results=tex>>=
opt.ridge <- penalisedRegression(x=trainpred, y=trainopt, newy=testopt, traindata=hom1.train.opt, testdata=testpred, alpha=0, nfolds=10, type="coefficients")
opt.ridge.pred <- penalisedRegression(x=trainpred, y=trainopt, newy=testopt, traindata=hom1.train.opt, testdata=testpred, alpha=0, nfolds=10, type="response")
ridge.xtab <- xtable(as.matrix(opt.ridge), caption="Coefficients for Ridge Regression of Optimism on other variables", label="tab:hom1optridge")
digits(ridge.xtab) <- 4
print(ridge.xtab)
@

Ridge regression penalises the coefficients by pushing them towards zero in the process of searching for the best model. However, it will not remove variables from the model, regardless of how small their coefficients get. It can be seen from Table \ref{tab:hom1optridge} that all of the coefficients are quite small, and mostly negative (which is the same as was seen in the OLS fit). Note that the coefficients shown are from the test data set, not used in the construction of the model. From this model, it would appear that mindfulness and emotional well being are the only useful predictors of optimism. 
%% \begin{figure}
%% <<optridgeplot, echo=FALSE, fig=TRUE>>=
%% opt.ridge.melt <- melt(opt.ridge.pred)
%% opt.ridge.pred.plot <- ggplot(opt.ridge.melt, aes(x=value, group=variable, colour=variable))+geom_density()

%% @ 
  
%%   \caption{Density Plot of Predicted versus Observed Values, Optimism Ridge Regression Model}
%%   \label{fig:optridgelassoplot}
%% \end{figure}

As can be seen from Figure \ref{fig:optridgelassoplot} the predictive ability of the ridge regression model was quite poor, as the model appeared to systematically underpredict the values for optimism. 

Next, a lasso regression model was fitted to the data. The lasso includes a penalty on the coefficients, similarly to ridge, but in contrast to ridge regression, lasso will remove predictors from the model.

<<optlasso, echo=FALSE, results=tex>>=
opt.lasso <- penalisedRegression(x=trainpred, y=trainopt, traindata=hom1.train.opt, testdata=hom1.test.opt, alpha=1, nfolds=10, type="coefficients")
opt.lasso.pred <- penalisedRegression(x=trainpred, y=trainopt, newy=testopt, traindata=hom1.train.opt, testdata=as.matrix(testpred), alpha=1, nfolds=10, type="response")
opt.lasso.xtab <- xtable(as.matrix(opt.lasso), caption="Coefficients for Lasso Regression on Optimism scores. Note that coefficients are for the fit of the chosen model on new data", label="tab:hom1optlasso")
digits(opt.lasso.xtab) <- 4
print(opt.lasso.xtab)
lasso.opt.pred <- penalisedRegression(x=trainpred, y=trainopt, traindata=hom1.train.opt, testdata=as.matrix(testpred), alpha=1, nfolds=10, type="link")
@ 

Table \ref{tab:hom1optlasso} shows that the lasso procedure has eliminated most of the predictors from the model. Only general health, energy fatigue and emotional well-being remain following the selection procedure. Note that mindfulness appears to be related to optimism under this model, but not when an OLS model is fitted with stepwise selection (see Table \ref{tab:hom1optstep}. However, these two models are not directly comparable as the OLS model was fitted to the same datset repeatedly, while the lasso and ridge models were fitted to new data. 


%% \begin{figure}
## <<optlassopredplot, echo=FALSE, results=hide>>=
## opt.lasso.pred.m <- melt(opt.lasso.pred)
## names(opt.lasso.pred.m)[2] <- "Optimism"
## opt.lasso.pred.plot <- ggplot(opt.lasso.pred.m, aes(x=value, group=variable, colour=variable))+geom_density()
%% @   
%%   \caption{Predicted versus Observed Values, Lasso Regression, Optimism, Sample One}
%%   \label{fig:optlassopredplot}
%% \end{figure}

<<optstep, echo=FALSE, results=hide>>=
opt.reg.first <- lm(optimism~(generalhealth+Age+College+pain+mindfulness+socialfunctioning+rolelim+rolelimem+emwellbeing+physfun+energyfat), data=hom1.train.opt)
opt.reg.simple <- lm(optimism~1, data=hom1.train.opt)
opt.step <- stepAIC(opt.reg.first, direction="both", k=2)
opt.step.pred.resp <- predict(opt.step, newdata=hom1.test.opt, type="response")
opt.step.pred.coef <- predict(opt.step, newdata=hom1.test.opt, type="terms")
step.pred.obs <- data.frame(pred=opt.step.pred.resp, obs=hom1.test.opt[,"optimism"])

@ 

<<optstepprint, echo=FALSE, results=tex>>=
print(xtable(summary(opt.step), caption="Coefficients for Stepwise Selected Regression Model (using AIC(k=3)), forward and backward selection from a full model", label="tab:hom1optstep"))
@ 

As Table \ref{tab:hom1optstep} shows, the stepwise selected models kept three predictors. General health and Age are retained, despite their lack of signifiance, suggesting that while they have an impact, they are moderated by the effect of emotional well being ($ p \le 0.0001$). 

<<optsteppred, echo=FALSE, results=tex>>=
opt.step.test <- lm(optimism~generalhealth+Age+emwellbeing, data=hom1.test.opt)
print(xtable(summary(opt.step), captio="Coefficients for Stepwise Selected Model on Test Data", caption="tab:hom1stepopttest"))
@ 

As shown in Table \ref{tab:hom1stepopttest} the model performs much more poorly on unseen data. However, the correlation between predicted values and optimism is 0.5, which is better than either the lasso or ridge performance on unseen data. 

<<optloess, echo=FALSE, results=hide, eval=FALSE>>=
opt.loess <- loess(optimism~generalhealth*mindfulness*emwellbeing, data=hom1.train.opt)
## opt.loess.fit <- tuneLoess(optimism~generalhealth*mindfulness*emwellbeing, data=hom1.train.opt, tuneLength=10, newdata=hom1.test.opt)
opt.loess.pred <- predict(opt.loess, newdata=hom1.test.opt, type="response")
## opt.loess.pred.obs <- data.frame(pred=opt.loess.pred, obs=hom1.test.opt)
## train.gam.loess <- train(optimism~generalhealth*mindfulness*emwellbeing, data=hom1.train.opt, method="gamLoess", tuneLength=10)
@ 

<<optreglm, echo=FALSE, results=hide>>=
opt.reg.first <- lm(optimism~(generalhealth+Age+College+pain+mindfulness+socialfunctioning+rolelim+rolelimem+emwellbeing+physfun+energyfat), data=hom1.train.opt)
opt.reg.sum <- summary(opt.reg.first)
opt.reg.xtab <- xtable(opt.reg.sum, label="tab:optregfirst", caption="Maximal Model for Regression on Optimism")
print(opt.reg.xtab, table.placement="ht")
lm.pred.opt <- predict(opt.reg.first, hom1.test.opt, type="response")
@


%% In table \ref{tab:optregfirst} the summary of the regression results can be seen, the model is significant, with an $F$ value of \Sexpr{round(opt.reg.sum[["fstatistic"]],3)}  and an $R^2$ of \Sexpr{round(opt.reg.sum[["adj.r.squared"]],3)}.  


%% Below, in Table \ref{tab:optregfinal} can be seen the final model,
%% including all significant predictor variables.

<<optregfinal, echo=FALSE, results=hide>>=
opt.part <- createMultiFolds(na.omit(hom1$optimism), k=10)
optfolds <- TrainTestSets(opt.part, hom1)
opt.reg.final <- lm(optimism~generalhealth+mindfulness+emwellbeing+Age, data=hom1)
opt.reg.fin.stand <- standardize(opt.reg.final)
opt.fin.sum <- summary(opt.reg.final)
opt.fin.xtab <- xtable(opt.fin.sum, label="tab:optregfinal", caption="Final Regression Model for Optimism")
print(opt.fin.xtab , table.placement="ht")
@


%% The model was significant, F(15, 528) = \Sexpr{round(opt.fin.sum[["fstatistic"]], 3)}, and the adjusted $R^2$ was equal to \Sexpr{round(opt.fin.sum[["adj.r.squared"]],3)}
%% The model seems to show that the only important predictors of Optimism in the dataset are General Health, Mindfulness, Emotional Well Being and Age.  This corroborates the plots that were shown above for these variables. The strongest predictor was emotional well being, with age and general health being of similiar magnitude. 

%% The residuals were homoscedastic and normally distributed, meeting the assumptions of the model.

As has been shown above, all three kinds of regression models agreed that emotional well being and Age were important, and general health tended not to be removed from any of the models, though it did not have a particularly large coefficient. Surprisingly enough, stepwise selection performed better on unseen data than either lasso or ridge regression, suggesting that overfitting did not occur for this variable. 




\subsection{Mindfulness Regressions}

A similiar procedure as described above for optimism was employed
in the midfulness regressions.  The results are shown below.


<<mindsetup, echo=FALSE, results=hide>>=
mind.ind <- with(hom1.full, createDataPartition(mindfulness, p=0.8, times=1, list=FALSE))
hom1.mind.train <- hom1.full[mind.ind, ]
hom1.mind.test <- hom1.full[-mind.ind,]
mind.train <- with(hom1.mind.train, mindfulness)
mind.pred.train <- hom1.mind.train[,c(4,67:73,75)]
mind.test <- with(hom1.mind.test, mindfulness)
mind.pred.test <- hom1.mind.test[,c(4,67:73,75)]
@ 


<<mindlasso, echo=FALSE, results=tex>>=
mind.lasso <- penalisedRegression(x=mind.pred.train, y=mind.train, traindata=mind.pred.train, testdata=as.matrix(mind.pred.test), alpha=1, type="coefficients")
print(xtable(as.matrix(mind.lasso), caption="Coefficients on Unseen Data, Mindfulness Regression (lasso penalisation)", label="tab:hom1mindlasso"))
mind.lasso.pred <- penalisedRegression(x=mind.pred.train, y=mind.train, traindata=mind.pred.train, testdata=as.matrix(mind.pred.test), newy=mind.test, alpha=1, type="response")
@ 

As shown in Table \ref{tab:hom1mindlasso}, the largest coefficient on mindfulness was optimism, and in line with the correlations seen above, it is negative. Energy Fatigue and Emotional Well being appear to be important in this particular model. 

Next, a ridge regression model was fit to this data. 

<<mindridge, echo=FALSE, results=tex>>=
mind.ridge <- penalisedRegression(x=mind.pred.train, y=mind.train, traindata=mind.pred.train, testdata=as.matrix(mind.pred.test), alpha=0, type="coefficients")
print(xtable(as.matrix(mind.ridge), caption="Coefficients on Unseen Data, Mindfulness Regression (ridge penalisation)", label="tab:hom1mindridge"))
mind.ridge.pred <- penalisedRegression(x=mind.pred.train, y=mind.train, traindata=mind.pred.train, testdata=as.matrix(mind.pred.test), newy=mind.test, alpha=0, type="response")
@ 

Table \ref{tab:hom1mindridge} shows the coefficients for the ridge regression, and they are almost identical to those of the lasso (at reasonable levels of precision). Next, a stepwise selection model procedure was employed on the dataset. 

<<maasregfirst, echo=FALSE, results=hide>>=
maas.reg.first <- lm(mindfulness~ generalhealth + optimism + pain+ socialfunctioning+physfun+rolelim+rolelimem+emwellbeing+energyfat+Age, data=hom1.mind.train)
mind.step <- stepAIC(maas.reg.first, data=hom1.mind.train, direction="both", k=3)
mind.step.test <- lm(mindfulness~rolelim+rolelimem+emwellbeing+energyfat, data=hom1.mind.test)

mind.step.pred <- predict(mind.step, hom1.mind.test, type="response")
mind.step.pred.obs <- data.frame(pred=mind.step.pred, obs=hom1.mind.test[["optimism"]])
@

<<mindstepprint, echo=false, results=tex>>=
print(xtable(summary(mind.step.test), caption="Coefficients for Stepwise Selected Regression on Mindfulness", label="tab:hom1mindsteptest"))
@ 

As can be seen from Table \ref{tab:hom1mindsteptest}, the stepwise selection process retained role limitation, emotional role limitations, emotional well being and energy fatigue. In contrast to the lasso and ridge fits, the model selected using this method did not include optimism. When the correlations between predicted and observed values were measured, all three model fits had correlations with the observed variables of approximately 0.44. However, in the case of the stepwise model, this correlation was negative. This unexpected result would seem to imply that this model is not particularly useful, and so either the lasso or ridge would appear to be the best model in terms of this data. 




\subsection{Health Regressions}

<<healthsetup, echo=FALSE, results=hide>>=
hom1.health.train.ind <- with(hom1.full, createDataPartition(generalhealth, p=0.75, list=FALSE))
hom1.health.train <- hom1.full[hom1.health.train.ind,]
hom1.health.test <- hom1.full[-hom1.health.train.ind,]
healthpred.train <- hom1.health.train[,c(4, 66:72, 74:75)]
health.train <- with(hom1.health.train, generalhealth)
healthpred.test <- hom1.health.test[,c(4, 66:72, 74:75)]
health.test <- with(hom1.health.test, generalhealth)
@ 


<<healthregfirst, echo=FALSE, results=hide>>=
health.mod1<-lm(generalhealth~mindfulness+physfun+optimism+energyfat+emwellbeing+Age+pain+Status+socialfunctioning+rolelim+rolelimem, na.action="na.omit", data=hom1.health.train)
health.step <- stepAIC(health.mod1, direction="both", k=3)
health.step.test <- lm(generalhealth~optimism+energyfat+rolelim, data=hom1.health.test)

@

<<healthregfirstprint, echo=FALSE, results=tex>>=
print(xtable(summary(health.step.test), caption="Coefficients for General Health Regression Model with predictors chosen by Stepwise Selection on unseen data", label="tab:hom1healthsteptest"))
@ 
As can be seen from Table \ref{tab:hom1healthsteptest}, optimism, role limitations and energy fatigue were retained as predictors in the model. However, only energy fatigue was a significant predictor in the test sample, though the coefficient for the effect of optimism was in line with the correlations and previous analyses carried out above. 

Next, a lasso regression model was fit to the same dataset. 


<<healthlasso, echo=FALSE, results=tex>>=
health.lasso <- penalisedRegression(x=as.matrix(healthpred.train), y=health.train, traindata=as.matrix(healthpred.train), testdata=healthpred.test, newy=health.test, alpha=1, type="coefficients")
health.lasso.pred <- penalisedRegression(x=as.matrix(healthpred.train), y=health.train, traindata=as.matrix(healthpred.train), testdata=healthpred.test, newy=health.test, alpha=1, type="response")
print(xtable(as.matrix(health.lasso), caption="Coefficients for Lasso Regression on General Health on unseen data", label="tab:hom1healthlasso"))
@ 

As can be seen from Table \ref{tab:hom1healthlasso}, optimism appears to be the best predictor in this case, along with energy fatigue and role limitations, supporting the set of predictors chosen by stepwise selection above. 

<<healthridge, echo=FALSE, results=tex>>=
health.ridge <- penalisedRegression(x=as.matrix(healthpred.train), y=health.train, traindata=as.matrix(healthpred.train), testdata=healthpred.test, newy=health.test, alpha=0, type="coefficients")
health.ridge.pred <- penalisedRegression(x=as.matrix(healthpred.train), y=health.train, traindata=as.matrix(healthpred.train), testdata=healthpred.test, newy=health.test, alpha=0, type="response")
print(xtable(as.matrix(health.ridge), caption="Coefficients for Ridge Regression on General Health on unseen data", label="tab:hom1healthridge"))
@ 

As can be seen from Table \ref{tab:hom1healthridge}, thee model for ridge regression is quite similiar, save that many of the coefficients are not shrunk nearly as much using this methodolody. Nonetheless, the overall magnitudes and relationships between the sizes of coefficients are quite similiar. 

\section{Psychometric Analyses}

\subsection{Number of Factors to retain}

<<scaleitems, echo=FALSE, results=hide>>=
## rand.grep <- grep("^RAND", x=names(hom1))
## randitems <- hom1[,rand.grep]
maas.grep <- grep("^MAASQ", x=names(hom1))
maasitems <- hom1[,maas.grep]
lotr.grep <- grep("^LOTRQ", x=names(hom1))
lotritems <- hom1[,lotr.grep]
@
<<retainfactors, echo=FALSE, results=hide>>=
sink("tmp.txt")
rand.nscree <- nScree(x=cor(na.omit(randitems.scored), use="pairwise.complete.obs"), model="factors")
sink(NULL)
@

<<retainfactorsprint, echo=FALSE, results=tex>>=
print(xtable(summary(rand.nscree), label="tab:randretain", caption="Comparison of Criteria to retain factors"))
@ 

As shown in Table \ref{tab:randretain}, the optimal co-ordinates criteria and the acceleration curve suggest that there are two factors, while the parallel analysis criterion and the kaiser criterion suggest eight factors. These latter two measures may be picking up on the higher order factor structure of the items, as the RAND is typically modelled as having two higher order factors (physical and mental health).

Another approach which can be applied to select the number of factors is a cross-validation method.

<<randfactorcv, echo=FALSE, results=hide>>=
rand.fact.gabriel <- cv.svd.gabriel(na.omit(randitems.scored), krow=2, kcol=2, maxrank=18)
rand.fact.wold <- cv.svd.wold(na.omit(randitems.scored), k=10, maxrank=12)
@ 

<<randcvwold, echo=FALSE, results=tex>>=
print(xtable(Svdcv(rand.fact.wold, label="tab:randfactwold", caption="Estimate of number of factors to retain based on 10-fold Cross Validation")))
@ 

It can be seen from Table \ref{tab:randfactwold} that the Wold method of cross validation suggests that four factors should be retained for further analysis. This seems in line with the results of the other criteria.



<<randcvgabriel, echo=FALSE, results=tex>>=
print(Svdcv(rand.fact.gabriel, label="tab:randfactgabriel", caption="Estimate of number of factors to retain based on leave out 2 item/subject Cross-validation"))
@

It can be seen from Table \ref{tab:randfactgabriel} that the Gabriel method of cross-validation suggests that there are thirteen factors which underlie this scale. This seems relatively implausible, but it will be tested. 

To summarise, two, four, eight and thirteen factor structures will be examined for the RAND MOS and their performance assessed on unseen data (from sample 2) to determine which of these provides the best fit. 

Next, the various metrics for determining the number of factors were applied to the Mindfulness scale. 

<<retainmaas, echo=FALSE, results=hide>>=
sink("tmp.txt")
maas.nscree <- nScree(x=cor(na.omit(maasitems), use="pairwise.complete.obs"), model="factors")
sink(NULL)
@ 


<<printretainmaas, echo=FALSE, results=tex>>=
print(xtable(summary(maas.nscree), label="tab:maasretain", caption="Comparison of Criteria to retain factors, MAAS"))
@ 

Looking at the eigenvalues, it is clear that one factor explains the majority of the variance. However, both the Kaiser criterion and parallel analysis suggest that three factors should be retained, while the VSS and the Minimum Average Partial criterion procedures suggest a one factor solutions.

<<maasfactorcv, echo=FALSE, results=tex>>=
maasitems <- as.data.frame(lapply(maasitems, as.numeric))
maas.fact.gabriel <- cv.svd.gabriel(na.omit(maasitems), krow=2, kcol=2, maxrank=7)
maas.fact.wold <- cv.svd.wold(na.omit(maasitems), k=10, maxrank=7)
@


<<maascvwold, echo=FALSE, results=tex>>=
print(Svdcv(maas.fact.wold, label="tab:maasfactwold", caption="Estimate of number of factors to retain based on 10-fold Cross Validation, MAAS Sample One"))
@ 

The Gabriel method of cross-validation, (shown in Table \ref{tab:maasfactwold}), agrees with the other criteria in suggesting that one factor should be retained. 


<<maascvgabriel, echo=FALSE, results=tex>>=
print(Svdcv(maas.fact.gabriel, label="tab:maasfactgabriel", caption="Estimate of number of factors to retain based on leave out 2 item/subject Cross-validation, MAAS Sample One"))
@ 

It can be seen from Table \ref{tab:maasfactgabriel} that the Wold method of cross-validation suggests that there are seven factors which underlie this scale. This seems relatively implausible, but it will be tested. 

Therefore, one, three and seven factor solutions will be examined and their performance assessed on unseen data. 


<<retainlotr, echo=FALSE, results=hide>>=
lotr.nscree <- nScree(x=cor(na.omit(lotritems), use="pairwise.complete.obs"), model="factors")
@ 

<<lotrprint, echo=FALSE, results=tex>>=
print(xtable(summary(lotr.nscree), label="tab:lotrretain", caption="Comparison of Criteria to retain factors, LOTR, Sample One"))

@ 

As shown in Table \ref{tab:lotrretain},all of the methods for determining the correct number of factors to retain suggest that a one factor solution fits the data matrix best. 

<<lotrfactorcv, echo=FALSE, results=hide>>=
lotritems <- as.data.frame(lapply(lotritems, as.numeric))
lotr.fact.gabriel <- cv.svd.gabriel(na.omit(lotritems), krow=3, kcol=3, maxrank=3)
lotr.fact.wold <- cv.svd.wold(na.omit(lotritems), k=10, maxrank=3)
@


<<lotrcvwold, echo=FALSE, results=tex>>=
print(Svdcv(lotr.fact.wold, label="tab:lotrfactwold", caption="Estimate of number of factors to retain based on 10-fold Cross Validation"))
@ 

The Wold method of cross-validation, (shown in Table \ref{tab:lotrfactwold}), agrees with the other criteria in suggesting that one factor should be retained. 


<<lotrcvgabriel, echo=FALSE, results=tex>>=
print(Svdcv(lotr.fact.gabriel, label="tab:lotrfactgabriel", caption="Estimate of number of factors to retain based on leave out 2 item/subject Cross-validation"))
@ 

It can be seen from Table \ref{tab:lotrfactgabriel} that the Gabriel method of cross-validation suggests that there are three factors which underlie this scale. This seems relatively implausible, but it will be tested. 

Therefore, one and three factor solutions will be assessed for the LOT-R, and their performance evaluated on unseen data. 

<<mapall, echo=FALSE, results=hide>>=
MAP.rand<-VSS(na.omit(randitems.scored), n=12, rotate="oblimin", fm="ml", plot=FALSE )
MAP.maas<-VSS(na.omit(maasitems),rotate="oblimin", fm="ml", plot=FALSE )
MAP.lotr<-VSS(na.omit(lotritems),rotate="promax", fm="gls", plot=FALSE )
@

%As can be seen from Figure ~\ref{fig:rand1}, the 8 factor structure was replicated.  However, the MAP criterion suggests a four factor solution, so both of these proposed solutions were examined and tested.




\subsection{RAND MOS}

Two, four, eight and thirteen factor solutions were extracted and interpreted from the RAND MOS items. 

<<rand2fact, echo=FALSE, results=tex>>=
rand.fact.2<-fa(na.omit(randitems.scored), 2,fm="ml", rotate="promax")
print(FactorXtab(rand.fact.2, label="tab:rand2fact", caption="Factor Loadings, RAND MOS Two Factor Solution, Sample One"))
@

PA1: "RANDQ1",  "RANDQ14", "RANDQ16" ,"RANDQ17" ,"RANDQ18" ,"RANDQ19" ,"RANDQ20" ,"RANDQ21" ,"RANDQ22" ,"RANDQ23" ,"RANDQ24" ,"RANDQ25" ,"RANDQ26" ,"RANDQ27"
,"RANDQ28" ,"RANDQ29" ,"RANDQ30" ,"RANDQ31" ,"RANDQ32" ,"RANDQ33" ,"RANDQ34"
,"RANDQ35" ,"RANDQ36". Essentially this factor appears to contain all of the scales except for Physical Functioning, which loads on Factor 2. We can best term this factor as General and Emotional Health.

PA2:"RANDQ3"  "RANDQ4"  "RANDQ5"  "RANDQ6"  "RANDQ7"  "RANDQ8"  "RANDQ9"  "RANDQ10" "RANDQ11" "RANDQ12". This factor maps exactly to the Physical Functioning Scale, and so retains that name.



The non-normed fit index was equal to \Sexpr{round(rand.fact.2[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(rand.fact.2[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(rand.fact.2[["RMSEA"]][2],3 )} to \Sexpr{round(rand.fact.2[["RMSEA"]][3],3 )}.

This factor solution does not appear to be useful, as it has extremely low fit indices (NNFI=0.69), and the breakdown of the factors is rather strange. If the factors had broken down in terms of Physical and Mental Health, then this would have made more sense. The factor loadings were invariant under a number of rotations (varimax, oblimin and promax), so it appears to be a real (if less than interpretable) factor structure. 

<<rand2corr, echo=FALSE, results=tex>>=
print(xtable(rand.fact.2$r.scores, label="tab:tcq1rand4corr", caption="Factor Correlations, RAND MOS Two Factor Solution, Sample One"))
@

It can be seen from Table \ref{tab:tcq1rand4corr} that the factor correlations were quite low for this solution, at 0.17. This suggests that an orthogonal rotation might be more appropriate, but attempting this did not change any of the loadings. 



\paragraph{Rand MOS 4 Factor Solution}

<<rand4fact, echo=FALSE, results=tex>>=
rand.fact.4<-factor.pa(na.omit(randitems.scored), 4, rotate="oblimin")
print(FactorXtab(rand.fact.4, label="tab:rand4fact", caption="Four Factor Solution, RAND MOS, Sample One (Oblimin Rotation)"))
@

The loadings on the four factor solution broke down as follows. 
PA2: "RANDQ3" , "RANDQ4" , "RANDQ5" , "RANDQ6" , "RANDQ7" , "RANDQ8" , "RANDQ9" , "RANDQ10" ,"RANDQ11" ,"RANDQ12". This factor maps exactly to the Physical Functioning scale, and so retains that name. 

PA1: "RANDQ17" ,"RANDQ18" ,"RANDQ19" ,"RANDQ20" ,"RANDQ23" ,"RANDQ24" ,"RANDQ25" ,"RANDQ26" ,"RANDQ27" ,"RANDQ28" ,"RANDQ29" ,"RANDQ30" ,"RANDQ31" ,"RANDQ32". This factor maps to the emotional role limitations, social functioning and emotional well being scales, and so can probably best be termed as Social and Emotional Functioning. 

PA3: "RANDQ13" ,"RANDQ14" ,"RANDQ15" ,"RANDQ16" ,"RANDQ21" ,"RANDQ22". This factor maps to the role limitations and pain sub-scales, and so can probably best be termed as physical limitations. 

PA4: "RANDQ1" , "RANDQ3" , "RANDQ21" ,"RANDQ27" ,"RANDQ33" ,"RANDQ34" ,"RANDQ35" ,"RANDQ36". This scale maps to the General Health scale, with one item from the Physical Functioning and one item from Energy/Fatigue (27). However, both these items have higher loadings on other factors, and so this factor can probably best be termed as General Health. 



The non-normed fit index was equal to \Sexpr{round(rand.fact.4[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(rand.fact.4[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(rand.fact.4[["RMSEA"]][2],3 )} to \Sexpr{round(rand.fact.4[["RMSEA"]][3],3 )}.

The fit indices are somewhat better for this solution than for the two factor solution, though the NNFI is still quite low. The RMSEA is somewhat too high for comfort, also. 

<<rand4corr, echo=FALSE, results=tex>>=
print(xtable(rand.fact.4[["r.scores"]], label="tab:tcq1rand4corr", caption="Factor Correlations, RAND MOS Four Factor Solution, Sample One"))
@

It can be seen from Table \ref{tab:tcq1rand4corr} that the factor correlations are quite low in this solution also, though they are all high enough to retain the oblique rotations.




\paragraph{RAND MOS 8 Factor Solution}

<<rand8fact, echo=FALSE, results=tex>>=
rand.fact.8<-factor.pa(na.omit(randitems.scored), 8, rotate="oblimin")
print(FactorXtab(rand.fact.8, label="tab:tcq1rand8fact", caption="factor Loadings Eight Factor Solution, RAND MOS, Sample One"))
@

PA2: "RANDQ3" , "RANDQ4" , "RANDQ5" , "RANDQ6" , "RANDQ7" , "RANDQ8" , "RANDQ9" , "RANDQ10" ,"RANDQ11" ,"RANDQ12". Again, the first factor extracted maps exactly to the Physical Functioning scale, and retains that name. 

PA1: "RANDQ20" ,"RANDQ23" ,"RANDQ24" ,"RANDQ25" ,"RANDQ26" ,"RANDQ27" ,"RANDQ28" ,"RANDQ30" ,"RANDQ32". This scale maps to the Social Functioning, and the Emotional Well Being Scale. There are some items taken from the energy faitigue scale, and as these are the positively worded items, this scale can probably best be termed as Social and Emotional Well Being. 

PA4: "RANDQ1" , "RANDQ33" ,"RANDQ34" ,"RANDQ36". These items map exactly to the General Health scale. Item 35 also loads on this scale, and as its loading was 0.29 while the cutoff was 0.30, it can be best characterised as part of that scale. Therefore, this factor can be best termed as General Health.

PA7: "RANDQ17" ,"RANDQ18" ,"RANDQ19" ,"RANDQ20". This scale maps to the Emotional Role Limitations and one item (20) from the Social Functioning scale (which asks about social events that have been missed due to health problems) and so this can probably best be termed as Emotional Role Limitations.

PA3: "RANDQ13" ,"RANDQ14" ,"RANDQ15" ,"RANDQ16". This scale maps exactly to the Role Limitations sub-scale, and so retains that name. 

PA5: "RANDQ29" ,"RANDQ31". These items are the negative items from the Energy/Fatigue scale, and so this factor can probably best be termed as Fatigue. 

PA6: "RANDQ21" ,"RANDQ22". These items map exactly to the Pain scale, and so retain that name. 

PA8: "RANDQ2" , "RANDQ23" ,"RANDQ27". The loadings on this factor are all below .4, which argues against its unproblematic interpretation. In addition, Q2 loads on this factor, when it is not typically associated with any factor. The other two items are the positively worded items from the  Energy/Fatigue scales, and so this factor can best be termed Energy. 



The non-normed fit index was equal to \Sexpr{round(rand.fact.8[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(rand.fact.8[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(rand.fact.8[["RMSEA"]][2],3 )} to \Sexpr{round(rand.fact.8[["RMSEA"]][3],3 )}.

This factor structure definitely makes sense, and the fit indices are relatively acceptable, although the RMSEA is a little higher than would be wanted. In addition, the items map quite well to the subscales, which further reinforces our confidence in this solution.


<<rand8corr, echo=FALSE, results=tex>>=
print(xtable(rand.fact.8[["r.scores"]], label="tab:hom1rand8corr", caption="Factor Correlations, Eight Factor Solution RAND MOS, Sample One"))
@

The factor correlations are moderate (shown in Table \ref{tab:hom1rand8corr}) and in line with expectations. The correlations are definitely too high to use an othrogonal rotation. 



\paragraph{Rand MOS 13 factor Solution}

<<rand13fact, echo=FALSE, results=tex>>=
rand.fact.13<-factor.pa(na.omit(randitems.scored), 13, rotate="oblimin")
print(FactorXtab(rand.fact.13,label="tab:tcq1rand13fact", caption="Factor Loadings, Thirteen Factor Solution RAND MOS, Sample One (oblimin rotation)"), scalebox=0.8)
@

PA2: "RANDQ6" , "RANDQ7" , "RANDQ9" , "RANDQ10" ,"RANDQ11" ,"RANDQ12". This appears to be the majority of the Physical Functioning scale, and therefore retains that name.


PA8: "RANDQ17" ,"RANDQ18" ,"RANDQ19" ,"RANDQ20". This consists of the three Emotional Role Limitations items, and a low loading on the negatively worded social functioning scale. It can therefore be named Emotional Role Limitations.

PA3: "RANDQ13" ,"RANDQ14" ,"RANDQ15" ,"RANDQ16". This factor maps exactly to the Role Limitations scale, and thus retains that name. 

PA7: "RANDQ23" ,"RANDQ26" ,"RANDQ27" ,"RANDQ30". These items map to the positive questions of Emotional Well Being and Energy/Fatigue, and therefore can best be termed as Positive Emotionality. 

PA11: "RANDQ3" ,"RANDQ4" ,"RANDQ5" ,"RANDQ8". These items are all from the Physical Functioning scale, and appear to all relate to relatively heavy exertions. This factor can therefore be termed Physical Exertion.

PA4: "RANDQ1" , "RANDQ34" ,"RANDQ36". These items all relate to health and are all framed in a positive way. Therefore this factor can be termed Positive Health.

PA5:"RANDQ29" ,"RANDQ31". These are the negatively framed items from the Energy/Fatigue scale, and can be probably best be termed Fatigue. 

PA6: "RANDQ21" ,"RANDQ22". These items map exactly to the Pain Scale, and thus retain that name. 

PA1: "RANDQ24" ,"RANDQ25" ,"RANDQ28" ,"RANDQ30". These items mostly relate to the emotional well being scale, and are almost all negatively framed. It can probably best be termed Emotional Problems.


PA10: "RANDQ3" ,"RANDQ6". These items relate to vigourous activities and climbing stairs. It can probably best be termed as Vigourous Activity.

PA9: "RANDQ33". 

PA12: "RANDQ20"

PA13: "RANDQ24"

The last three factors have only one item loading on them, and three is normally regarded as the minimum for a factor to replicate.   %cite Tabachnick & Fidell here.
Therefore we can stop the interpretation here, as this factor does not seem to add much to our understanding of the scale. 

<<rand13corr, echo=FALSE, results=tex>>=
print(xtable(rand.fact.13[["r.scores"]],label="tab:hom1rand13corr", caption="Factor Correlations, Thirteen Factor RAND MOS Solution, Sample One"), scalebox=0.6)
@
As Table \ref{tab:hom1rand13corr} shows, the factors are inter-correlated, but not in any coherent fashion. 



The non-normed fit index was equal to \Sexpr{round(rand.fact.4[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(rand.fact.4[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(rand.fact.4[["RMSEA"]][2],3 )} to \Sexpr{round(rand.fact.4[["RMSEA"]][3],3 )}. 

Although the factor structure from this solution looks unlikely to be useful (given that there are three factors consisting only of one item each), the fit indices indicate that this is a better solution than any of the other proposed structures. That being said, this structure is unlikely to replicate, due to presumed overfitting. 


\paragraph{CFA for RAND MOS}


<<rand2sem, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "General and Emotional Health")
physfun <- paste(rand, c(3:12), sep="")
genemhealth <- paste(rand,c(1,14,17:36), sep="")
Rand2model <- mxModel(name="RAND2Samp1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="General and Emotional Health", to=genemhealth),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=364)
                      )
rand2fit <- mxRun(Rand2model)
rand2summ <- summary(rand2fit)
@

<<rand4sem, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "Social and Emotional Functioning", "General Health", "Physical Limitations")
physfun <- paste(rand, c(3:12), sep="")
genhealth <- paste(rand, c(1,33:36), sep="")
physlim <- paste(rand, c(13:16,21:22), sep="")
emsocfunc <- paste(rand, c(17:20, 23:32), sep="")
Rand4model <- mxModel(name="RAND4Samp1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="Social and Emotional Functioning", to=emsocfunc),
                      mxPath(from="General Health", to=genhealth),
                      mxPath(from="Physical Limitations", to=physlim),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=364)
                      )
rand4fit <- mxRun(Rand4model)
rand4summ <- summary(rand4fit)
@

<<rand8sem, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "Social and Emotional Wellbeing", "General Health", "Emotional Role Limitations", "Role Limitations", "Fatigue", "Pain", "Energy")
physfun <- paste(rand, c(3:12), sep="")
socemwellbeing <- paste(rand, c(20,23:28, 30, 32), sep="")
genhealth <- paste(rand, c(1,33:36), sep="")
emrolelim <- paste(rand, c(17:20), sep="")
rolelim <- paste(rand, c(13:16), sep="")
energy <- paste(rand, c(29, 31), sep="")
pain <- paste(rand, c(21:22), sep="")
fatigue <- paste(rand, c(2, 23, 27), sep="")
Rand8model <- mxModel(name="RAND8Samp1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="Social and Emotional Wellbeing", to=socemwellbeing),
                      mxPath(from="General Health", to=genhealth),
                      mxPath(from="Emotional Role Limitations", to=emrolelim),
                      mxPath(from="Role Limitations", to=rolelim),
                      mxPath(from="Energy", to=energy),
                      mxPath(from="Pain", to=pain),
                      mxPath(from="Fatigue", to=fatigue),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=364)
                      )
rand8fit <- mxRun(Rand8model)
rand8summ <- summary(rand8fit)
@

<<rand13sem, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning","Emotional Role Limitations","Role Limitations", "Positive Emotionality", "Physical Exertion", "Positive Health","Fatigue","Pain", "Emotional Problems", "Sickness", "Vigorous Activity","RAND20", "RAND24" )
physfun <- paste(rand, c(3:12), sep="")
emrolelim <- paste(rand, c(17:20), sep="")
rolelim <- paste(rand, c(13:16), sep="")
posemotions <- paste(rand, c(23,26,27,30), sep="")
physexertions <- paste(rand, c(3,4:5, 8), sep="")
poshealth <- paste(rand, c(1,34, 36), sep="")
fatigue <- paste(rand, c(29, 31), sep="")
pain <- paste(rand, c(21:22), sep="")
emproblems <- paste(rand, c(24, 25, 28, 30), sep="")
sickness <- paste(rand, 33, sep="")
vigactivity <- paste(rand, c(3,6), sep="")
rand20 <- "RANDQ20"
rand24 <- "RANDQ24"
Rand13model <- mxModel(name="RAND13Samp1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                       mxPath(from="Emotional Role Limitations", to=emrolelim),
                       mxPath(from="Role Limitations", to=rolelim),
                       mxPath(from="Positive Emotionality", to=posemotions),
                      mxPath(from="Physical Exertion", to=physexertions),
                       mxPath(from="Positive Health", to=poshealth),
                       mxPath(from="Pain", to=pain),
                       mxPath(from="Fatigue", to=fatigue),
                       mxPath(from="Emotional Problems", to=emproblems),
                       mxPath(from="Vigorous Activity", to=vigactivity),
                      mxPath(from="Sickness", to=sickness),
                      mxPath(from="RANDQ20", to=rand20),
                       mxPath(from="RANDQ24", to=rand24),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=364)
                      )
rand13fit <- mxRun(Rand13model)
rand13summ <- summary(rand13fit)
@ 

<<randsemcompare, echo=FALSE, results=tex>>=
randsemcomp <- mxCompare(base=rand2fit, comparison=c(rand4fit, rand8fit, rand13fit), all=TRUE)
print(xtable(randsemcomp,label="tab:randsemcompare", caption="SEM Comparison for RAND MOS Factor Solutions, Sample One"), scalebox=0.8, table.placement="ht")
@

As can be seen from \ref{tab:randsemcompare}, the 8 factor solution appears to fit better (lower AIC), so on the basis of this analysis, this is the solution which should be retained.

\section{Mindfulness Attention Awareness Scale}

For the MAAS, parallel analysis, MAP, VSS, Kaisers rule and the Wold method of cross-validation suggested a one factor solution, while the Gabriel method of cross-validation suggested a seven factor solution.  Therefore, one and seven factor solutions were extracted and the results interpreted, as shown below.

\subsection{MAAS One Factor Solution}
\label{sec:maas-one-factor}



<<maas1fact, echo=FALSE, results=tex>>=
maas.fact.1<-factor.pa(na.omit(maasitems), 1, rotate="oblimin")
print(FactorXtab(maas.fact.1,label="tab:hom1maas1fact", caption="Factor Loadings, One Factor Solution, MAAS, Sample One"))
@

The results of the one factor solution for the MAAS are shown in Table \ref{tab:hom1maas1fact}. 

The non-normed fit index was equal to \Sexpr{round(maas.fact.1[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(maas.fact.1[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(maas.fact.1[["RMSEA"]][2],3 )} to \Sexpr{round(maas.fact.1[["RMSEA"]][3],3)}.



\subsection{MAAS Seven Factor Solution}
\label{sec:maas-seven-factor}

The next factor solution to be examined was the seven factor solution suggested by the Gabriel method of cross-validation.


<<maas7fact, echo=FALSE, results=tex>>=
maas.fact.7<-factor.pa(na.omit(maasitems), 7, rotate="oblimin")
print(FactorXtab(maas.fact.7,label="tab:tcq1maas7fact", caption="Factor Loadings, Seven Factor Solution, MAAS, Sample One"))
@



PA1: "MAASQ9",  "MAASQ10", "MAASQ11", "MAASQ12".These items all relate to automatic behaviour, and this factor can possibly best be termed as Automatic Behaviour.

PA3:"MAASQ13". This factor is unlikely to be useful, as it consists only of one item. The item relates to preoccupations, and so is given that name. 


PA7: "MAASQ7", "MAASQ8". These items both relate to rushing through activities, and can probably best be termed as Hurry. 

PA2: "MAASQ2", "MAASQ3". These items can probably best be termed as lack of present focus.

PA4: "MAASQ15". This item relates to food, and so this factor can best be termed as Food Mindlessness. 

PA5: "MAASQ6". This item can best be termed as Inattention.

PA6:"MAASQ1", "MAASQ4", "MAASQ5", "MAASQ9". These items can probably best be termed as lack of awareness. 

The non-normed fit index was equal to \Sexpr{round(maas.fact.7[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(maas.fact.7[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(maas.fact.7[["RMSEA"]][2],3 )} to \Sexpr{round(maas.fact.7[["RMSEA"]][3],3)}.




\subsection{CFA for MAAS}
\label{sec:cfa-maas}

<<maas7sem, echo=FALSE, results=hide>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Automatic Behaviour", "Preoccupations", "Hurry",  "Lack of Present Focus", "Food Mindlessness", "Inattention", "Lack of Awareness")
automaticbehaviour <- paste(maas, c(9:12), sep="")
preoccupations <- "MAASQ13"
hurry <- paste(maas, c(7:8), sep="")
lackpresfocus <- paste(maas, c(2:3), sep="")
foodmindlessness <- "MAASQ15"
inattention <- "MAASQ6"
lackawareness <- paste(maas, c(1,4:5,9), sep="")
Maas7model <- mxModel(name="MAAS7",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Automatic Behaviour", to=automaticbehaviour),
                      mxPath(from="Preoccupations", to=preoccupations),
                      mxPath(from="Hurry", to=hurry),
                      mxPath(from="Lack of Present Focus", to=lackpresfocus),
                      mxPath(from="Lack of Awareness", to=lackawareness),
                           mxPath(from="Inattention", to=inattention),
                           mxPath(from="Lack of Present Focus", to=lackpresfocus),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems)), type="cov", numObs=364)
                      )
maas7fit <- mxRun(Maas7model)
maas7summ <- summary(maas7fit)
@




<<maas1fit, echo=FALSE, results=hide>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Unawareness")
unawareness <- paste(maas, c(1:15), sep="")
Maas1model <- mxModel(name="MAAS1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Unawareness", to=unawareness),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems)), type="cov", numObs=364)
                      )
maas1fit <- mxRun(Maas1model)
maas1summ <- summary(maas1fit)
@

<<maassemcompare, echo=FALSE, results=tex>>=
maascomp <- mxCompare(base=maas1fit, comparison= maas7fit)
maascomp.xtab <- xtable(maascomp,label="tab:maassemcomp")
print(maascomp.xtab)
                      
@

Factor solutions for one and seven factors were extracted, and the results were subjected to CFA.
The results of the CFA are shown below in Table \ref{tab:maassemcomp}.

From Table \ref{tab:maassemcomp} it can be seen that the best model is the one factor model, which is in line with previous research.
The factor structure is not reported here as all factors loaded on the first factor.  This factor explained 35\% of the variance
in the sample, which is low.  Possible explanations for this are discussed below. 
\subsection{Life Orientation Test, Revised}

Parallel Analysis indicated that two factors should be extracted, while the MAP criterion suggested one.  Therefore, both one and two factor solutions were extracted from the matrix and their results examined for adequacy and interpretability.

\subsection{LOTR One Factor Solution}
\label{sec:lotr-one-factor}


<<lotr1fact, echo=FALSE, results=tex>>=
lotr.fact.1<-factor.pa(na.omit(lotritems), 1, rotate="oblimin")
print(FactorXtab(lotr.fact.1,label="tab:hom1lotr1fact", caption="Factor Loadings, One Factor Solution, LOT-R, Sample One"))
@

Table \ref{tab:hom1lotr1fact} shows the loadings for the one factor solution. The communalities are relatively high, except for question one which is a sign that perhaps this solution is not optimal. 


The non-normed fit index was equal to \Sexpr{round(lotr.fact.1[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(lotr.fact.1[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(lotr.fact.1[["RMSEA"]][2],3)} to \Sexpr{round(lotr.fact.1[["RMSEA"]][3],3 )}.  This solution does not seem optimal, as the RMSEA is well outside the recommended bounds, and the NNFI is quite low.  %% In addition, the communalities are not very high, with over half the variance being left out of the solution.




\subsection{LOTR Three Factor Solution}
\label{sec:lotr-three-factor}

<<lotr3fact, echo=FALSE, results=tex>>=
lotr.fact.3<-factor.pa(na.omit(lotritems), 3, rotate="oblimin")
print(FactorXtab(lotr.fact.3,label="tab:tcq1lotr3fact", caption="Factor Loadings, Three Factor Solution, LOT-R, Sample One"))
@

PA2:"LOTRQ1",  "LOTRQ4",  "LOTRQ10". These items are all the positively framed items, and so this factor can best be termed as Optimism. 

PA1:"LOTRQ3", "LOTRQ7". This factor, and the next, consist of the pessimism items, and so can best be termed Pessimism. 

PA3: "LOTRQ7", "LOTRQ9". This factor can best be termed Pessimism (Reversed). 





\subsection{CFA for LOTR}
\label{sec:cfa-lotr}

<<lotr1sem, echo=FALSE, results=hide>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism")
optimism <- paste(lotr, c(1,3,4,7,9,10), sep="")
Lotr1model <- mxModel(name="LOTR1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems)), type="cov", numObs=364)
                      )
lotr1fit <- mxRun(Lotr1model)
lotr1summ <- summary(lotr1fit)
@


<<lotr3sem, echo=FALSE, results=hide>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism", "Pessimism", "Pessimism2")
optimism <- paste(lotr, c(1,4,10), sep="")
pessimism <- paste(lotr, c(3,7), sep="")
pessimism2 <- paste(lotr, c(7,9), sep="")
Lotr3model <- mxModel(name="LOTR3",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from="Pessimism", to=pessimism),
                      mxPath(from="Pessimism2", to=pessimism2),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems)), type="cov", numObs=364)
                      )
lotr3fit <- mxRun(Lotr3model)
lotr3summ <- summary(lotr3fit)
@

<<lotrcompare, echo=FALSE, results=tex>>=
lotrcomp <- mxCompare(base=lotr1fit, comparison=lotr3fit)
lotrcomp.xtab <- xtable(lotrcomp,label="tab:semlotrcomp", caption="Comparison of CFA for the LOT-R")
print(lotrcomp.xtab)
@

As can be seen from Table \ref{tab:semlotrcomp}, the one factor solution provided the best fit to the data.  Therefore, this solution will be tested on the second sample.

 \section{Item Response Theory Analyses}
\label{sec:item-response-theory}

In addition to the classical test theory analyses carried out, each scale was also subjected to item response theory analyses.  The first step in this process was to use Mokken scaling to test the assumptions required for item response theory modelling (as described in the methodology section).

<<loadirtpackages, echo=FALSE, results=hide>>=
require(mokken)
require(eRm)
require(ltm)
@

<<randcheckassumptions, echo=FALSE, results=hide>>=
rand.scales <- aisp(na.omit(randitems))
@

<<randscales, echo=FALSE, results=tex>>=
print(xtable(rand.scales,label="tab:randscales", caption="Item Selection Procedure Results, RAND MOS Sample One"))
@
As can be seen from Table \ref{tab:randscales}, the item selection procedure suggests that there are five scales in the RAND MOS.  


The scales can be termed as follows:

Scale 1: RANDQ3-RANDQ12: This maps exactly to the physical functioning scale.

Scale 2: RANDQ16-RANDQ20,24,25,28,29,31,32. This scale incorporates the Role Limitations, Social Functioning, Emotional Well Being and Energy scales, and can best be termed as Mental/Emotional Health.

Scale 3: RAND1, RAND20-23,26,27,30,34,36. This scale incorporates the pain scale, items from social functioning, emotional well being and general health. It can probably best be termed as Energy.

Scale 4: RAND13-RAND16. This maps to the role limitations scale, and can probably best be termed as physical limitations.

Scale 5:  RANDQ33, RANDQ35. This maps to the negatively phrased items from the General Health scale, and can best be termed Sickness. However, this scale will not be analysed as there are not enough items to allow for the analytic procedure to work.

<<randirtscales, echo=FALSE, results=hide>>=
irtphysfun <- randitems[,paste(rand, c(3:12), sep="")]
irtemhealth <- randitems[,paste(rand, c(16:20,24,25,28,29,31,32), sep="")]
irtenergy <-  randitems[,paste(rand, c(1,20:23,26:27,30,34,36), sep="")]
irtphyslim <- randitems[,paste(rand, c(13:16), sep="")]
irtsickness <- randitems[,paste(rand, c(33,35), sep="")]
@

\subsection{IRT Physical Functioning Scale}
\label{sec:irt-phys-funct}

Firstly, the physical functioning scale suggested by the item selection procedure was checked using non-parametric IRT methods before being modelled using successively more complex parametric IRT models. 

<<randphysfun, echo=FALSE, results=hide>>=
irtphys.item.ord <- check.iio(na.omit(irtphysfun))
irtphys.monotonicity <- check.monotonicity(na.omit(irtphysfun))
@


<<randphysfuncitemord, echo=FALSE, results=tex, eval=FALSE>>=
print(xtable(irtphys.item.ord[["violations"]],label="tab:randphysfuncitemord", caption="IRT Physical Functioning Item Ordering Results"))
@

There were no violations of item ordering for this sample.  This demonstrates that there are no violations of monotonicity for this subscale, and suggests that item response theory modelling can proceed, for this scale at least. Two items failed the item ordering assumption (RAND20, RAND32) and so were removed from the scale before further analysis. Interestingly enough, these items represent the Social Functioning scale, which may suggest that this scale is not valid from an IRT point of view.  The removal of  these two items from the emotional health scale appears to have removed the problems with invariant item ordering. The reduced scale had no issues with non-monotonic item responses. 

The IIO and monotonicity assumptions were checked for the physical limitations scale, and there were no violations. 

<<physfunrasch, echo=FALSE, results=hide>>=
physfun.gpcm.rasch <- gpcm(na.exclude(irtphysfun), constraint="rasch")
physfun.gpcm.1pl <- gpcm(na.exclude(irtphysfun), constraint="1PL")
physfun.gpcm.gpcm <- gpcm(na.exclude(irtphysfun), constraint="gpcm")
@

<<randemhealth, echo=FALSE, results=hide>>=
irtemhealth.item.ord <- check.iio(na.omit(irtemhealth))
irtemhealth.monotonicity <- check.monotonicity(na.omit(irtemhealth))
@



<<randphyslim, echo=FALSE, results=hide>>=
irtphyslim.item.ord <- check.iio(na.omit(irtphyslim))
irtphyslim.monotonicity <- check.monotonicity(na.omit(irtphyslim))
@

<<physfunraschprint, echo=FALSE, results=tex>>=
print(xtable(coef(physfun.gpcm.rasch),label="tab:physfunrasch", caption="Coefficient Estimates, Rasch Partial Credit Model, Sample One"))
@

As can be seen from Table \ref{tab:physfunrasch}, the Rasch model was not a good fit to these items, as RANDQ7 and RANDQ12 showed non increasing item difficulty estimates. 

<<physfunraschprint, echo=FALSE, results=tex>>=
print(xtable(coef(physfun.gpcm.1pl),label="tab:randphysfun1pl", caption="Coefficient Estimates, Rand MOS One Parameter Partial Credit Model, Sample One"))
@ 

As shown in Table \ref{tab:randphysfun1pl}, the one parameter model also proved a poor fit to the data, as RANDQ7 and RANDQ12 still show problems with the ordering of abilities. 

<<physfungpcmprint, echo=FALSE, results=tex>>=
  print(xtable(coef(physfun.gpcm.gpcm),label="tab:randphysfungpcm", caption="Coefficient Estimates for RAND Physical Functioning IRT Scale, Two Parameter Partial Credit Model, Sample One"))
@ 

Table \ref{tab:randphysfungpcm} shows that RANDQ12 is still a problematic item for the two parameter model, though RANDQ7 does not show the problems seen earlier. The next step in the modelling process was to fit a one and two parameter GRM.  



<<physfungrm1, echo=FALSE, results=hide>>=
physfun.grm.1pl <- grm(irtphysfun, constrained=TRUE)
physfun.grm.2pl <- grm(irtphysfun, constrained=FALSE)
@ 

<<physfungrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(physfun.grm.1pl), label="tab:physfungrm1", caption="Coefficient Estimates for One Parameter Graded Response Model, IRT Physical Functioning Scale"))
@ 

As can be seen from Table \ref{tab:physfungrm1}, there appear to be no problems with the fit of this model as all of the coefficients are monotonically increasing. Next, a two parameter GRM was fit to this scale. 


<<physfungrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(physfun.grm.2pl), label="tab:randphysfungrm2pl", caption="Coefficient Estimates for One Parameter Graded Response Model, IRT Physical Functioning Scale"))
@ 


From Table \ref{tab:randphysfungrm2pl}, it can be seen that the allowing the discrimination parameter to vary freely makes a large difference to the coefficient estimates. Note that Q7 appears to be the most discriminating question, with a slope of 5.64. The next process was to assess if the two parameter model provided a significant improvement in fit over and above the one parameter model.

The results of this ANOVA showed that the two parameter model was a much better fit to the data ($p \le 0.001$). 

<<grmanova, echo=FALSE, results=hide>>=
grmanova <- anova(physfun.grm.1pl, physfun.grm.2pl)
@ 

However, a better predictor of the usefulness of a model is to examine its fit on unseen data, and this was done in Section \ref{sec:predictions}. 


\subsection{IRT Energy Scale}
\label{sec:irt-energy-scale}



<<randenergycheck, echo=FALSE, results=hide>>=
irtenergy.item.ord <- check.iio(na.omit(irtenergy))
irtenergy.monotonicity <- check.monotonicity(na.omit(irtenergy))
irtenergy2 <- randitems[,paste(rand, c(1,20,22,26,30,34), sep="")]
@

<<randenergyprint, echo=FALSE, results=hide>>=
print(xtable(irtenergy.item.ord[["violations"]],label="tab:randenergyitemord", caption="IRT RAND Energy Scale, Invariant Item Ordering Results"))
@

There were two violations of the item ordering assumption for the Energy scale, RAND21 and RAND23, and these items were removed before further analyses.

The next scale to be examined is the  Energy scale.  The first analysis undertaken was to fit a partial credit rasch model to the scale.

<<genhealthPCM, echo=FALSE, results=tex>>=
irtenergy.pcm.rasch <- gpcm(na.omit(irtenergy), constraint="rasch")
print(xtable(coef2mat(coef(irtenergy.pcm.rasch)), label="tab:hom1energpcmrasch", caption="Coefficient Estimates for IRT Energy Scale, Rasch Partial Credit Model"))
@


It can be seen from Table \ref{tab:hom1energpcmrasch} that with the exception of Q36, all the items seem appropriately fitting. With Q36, there is no ability level where a response category of four is more probable than any of the other items, which is a failure either of the model or the scale. 


<<genhealthpcm1pl, echo=FALSE, results=tex>>=
irtenergy.pcm.1pl <- gpcm(na.omit(irtenergy), constraint="1PL")
print(xtable(coef2mat(coef(irtenergy.pcm.1pl)), label="tab:hom1energpcm1pl", caption="Coefficients for IRT Energy Scale, One parameter Partial Credit Model"))
@

Table \ref{tab:hom1energpcm1pl} shows the coefficient estimates for the one parameter Partial Credit Model fitted to the energy RAND MOS scale. Suprisingly, the overall discrimination parameter has decreased. The most difficult question is 21, which asks about bodily pain in the past six months. The difficulty of this question would seem to be a function of the sampling population (i.e. students) here, rather than a function of the item's properties. Again, Q36 has the problem with the third response category, as does Q22, suggesting that the population is split bimodally on this question, with either high or low responses being more probable than a response in the middle. 


<<genhealthpcm2pl, echo=FALSE, results=tex>>=
irtenergy.pcm.2pl <- gpcm(na.omit(irtenergy), constraint="gpcm")
print(xtable(coef2mat(coef(irtenergy.pcm.2pl)), label="tab:hom1energypcm2pl", caption="Coefficients of IRT Energy Scale PCM, Two Parameter Model"))
@


Table \ref{tab:hom1energpcm2pl} shows the coefficient estimates for the two parameter PCM on this scale. Again, items 36 and 22 have issues with category 3 estimates, and the discrimination parameters are all quite low with over half the scale having discrimination parameters of less than one, meaning that they discriminate less well between high and low ability respondents than would questions meeting the assumptions of a Rasch model. 

<<genhealthpcmcompare, echo=FALSE, results=tex>>=
anova.rasch.1pl <- anova.gpcm(irtenergy.pcm.rasch, irtenergy.pcm.1pl)
anova.1pl.2pl <- anova.gpcm(irtenergy.pcm.1pl, irtenergy.pcm.2pl)
anova.rasch.2pl <- anova.gpcm(irtenergy.pcm.rasch, irtenergy.pcm.2pl)
@

An anova conducted on the three models indicated that the 2 parameter model fit the data best ($p \le 0.001$), subject to the caveats above.


Next, one and two parameter Graded Response Models were fit to the energy scale. 

<<hom1irtenergygrm, echo=FALSE, results=hide>>=
hom1.energy.grm.1pl <- grm(irtenergy, constrained=TRUE)
hom1.energy.grm.2pl <- grm(irtenergy, constrained=FALSE)
@ 


<<hom1energygrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(hom1.energy.grm.1pl)), label="tab:hom1energygrm1pl", caption="Coefficient Estimates for One Parameter Graded Response Model on IRT Energy Scale, RAND MOS, Sample One"))
@ 

Table \ref{tab:hom1energygrm1pl} shows the coefficient estimates for the one parameter model on the Energy scale. It can be seen that the Graded Response Model does not have the same issues with category 3 for items 22 and 36, which implies that this issue above was a property of the model rather than the scale. The discrimination parameter is estimated as much higher under this model also, as are the ability thresholds, though the rank ordering remains the same. 

<<hom1energygrm2plprint, echho=FALSE, results=hide>>=
print(xtable(coef2mat(coef(hom1.energy.grm.2pl)), label="tab:hom1energygrm2pl", caption="Coefficient Estimates for Two Parameter Graded Response Model, IRT Energy Scale, RAND MOS, Sample One"))
@ 

Table \ref{tab:hom1energygrm2pl} shows the coefficient estimates for the two parameter Graded Response Model. It can be seen that the discrimination parameters are very different from those of a similar Partial Credit Model (see Table \ref{tab:hom1energygpcm2pl}). Additionally, the ability estimates are much higher on average, with Q22 having the highest ability threshold. 



\subsection{IRT Emotional Health Scale}



<<randEmhealthItemord, echo=FALSE, results=tex, eval=FALSE>>=
print(xtable(irtemhealth.item.ord[["violations"]],label="tab:randemhealthitemord", caption="IRT Emotional Health Scale Item Ordering Results"))
@

<<newIrtEmHealth, echo=FALSE, results=hide>>=
irtemhealth2 <- randitems[,paste(rand, c(16:19,25,28,29,31), sep="")]
emhealth.item.ord2 <- check.iio(na.omit(irtemhealth2))
emhealth.mono.2 <- check.monotonicity(na.omit(irtemhealth2))
print(xtable(emhealth.item.ord2[["violations"]], caption="IIO for reduced emotional health scale", label="tab:randemhealth2iio"))
@

<<emhealthPCM, echo=FALSE, results=hide>>=
emhealth.pcm.rasch <- gpcm(irtemhealth2, constraint="rasch")
emhealth.pcm.1PL <- gpcm(na.omit(irtemhealth2), constraint="1PL")
emhealth.pcm.gpcm <- gpcm(na.omit(irtemhealth2), constraint="gpcm")
@


<<emhealthRasch, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(emhealth.pcm.rasch)),label="tab:hom1emhealthrasch", caption="Coefficient Estimates for Emotional Health IRT Scale, Rasch Partial Credit Model, Sample One"))
@



As can be seen from Table \ref{tab:hom1emhealthrasch}, the rasch model does not provide a good fit for this data, as shown by the numerous failures of the monotonicity assumption.  Therefore, the next step was to fit a more flexible model, the coefficients of which are shown in Table \ref{tab:hom1emhealth1pl}.  This model kept the constant discrimination parameter, but allowed it to be estimated from the data.

<<emhealth1pl, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(emhealth.pcm.1PL)),label="tab:hom1emhealth1pl", caption="Coefficient Estimates for One Parameter Partial Credit Model, Emotional Health Scale, RAND MOS, Sample One"))
@

<<emhealth2pl, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(emhealth.pcm.gpcm)),label="tab:hom1emhealth2pl", caption="Coefficient Estimates for One Parameter Partial Credit Model, Emotional Health Scale, RAND MOS, Sample One"))
@ 

In Table \ref{tab:hom1emhealth2pl} can be seen the results of estimating a true two parameter model for this dataset, where the discrimination parameter was estimated seperately for each item.

A likelihood test was carried out between these three models, and the results showed that the true two parameter model provided a much better fit to the data ($p \le 0.001$).  This is not particularly surprising given that it estimates twice as many parameters as the most parsimonious model, and the real test of these model\'s predictive abilities will come when we fit them to unseen data.

Next, one and two parameter Graded Response Models were fit to the data. 

<<hom1emhealthgrm, echo=FALSE, results=hide>>=
hom1.emhealth.grm.1pl <- grm(irtemhealth2, constrained=TRUE)
hom1.emhealth.grm.2pl <- grm(irtemhealth2, constrained=FALSE)
@ 


<<hom1emhealthgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(hom1.emhealth.grm.1pl)), label="tab:hom1memhealthgrm1pl", caption="Coefficient Estimates for One Parameter Graded Response Model, IRT Emotional Health Scale, Sample One"))
@ 

Table \ref{tab:hom1emhealthgrm1pl} shows the coefficient estimates for the one parameter Graded Response Model fitted to the Energy Scale of the RAND MOS. It can be seen that the emotional role limitations questions (16-19) are estimated at quite low ability thresholds, as the sample was non-clinical, and that the questions have a relatively high discrimination parameter, but relatively low ability estimates (as most of the rest of the questions come from the emotional health and energy/fatigue scales). 

Next, a two parameter model was fitted to this scale. 

<<hom1emhealthgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(hom1.emhealth.grm.2pl)), label="tab:hom1emhealthgrm2pl", caption="Coefficient Estimates for Two Parameter Graded Response Model, Emotional Health RAND MOS Scale, Sample One"))
@ 
Table \ref{tab:hom1emhealthgrm2pl} shows the coefficient estimates for the two parameter Graded Response Model, in which all of the signs and ordering of the thresholds have reversed. 




\subsection{IRT Physical Limitations Scale}
\label{sec:irt-phys-limit}



Next, we assess the usefulness of one and two parameter GRM\'s for the physical limitations scale. 

<<physlimgrm, echo=FALSE, results=hide>>=
hom1.physlim.grm.1pl <- grm(irtphyslim, constrained=TRUE)
hom1.physlim.grm.2pl <- grm(irtphyslim, constrained=FALSE)
@ 

<<physlimgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(hom1.physlim.grm.1pl)), label="tab:hom1physlimgrm1pl", caption="Coefficient Estimates for One Parameter Graded Response Model, Physical Limitations Scale, Sample One"))
@ 

Table \ref{tab:hom1physlimgrm1pl} shows the coefficient estimates for the one parameter Graded Response Model. The discrimination parameter is quite high, suggesting that a positive response on these questions is informative as to a person's ability level. The ability thresholds are quite low, which makes sense given the non-clinical and young nature of the sample. 

Next, the two parameter graded response model was examined for this scale. 

<<hom1physlimgrm2pl, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(hom1.physlim.grm.2pl)), label="tab:hom1physlimgrm2pl", caption="Coefficient Estimates for Two Parameter Physical Limitations Graded Response Model, Sample One"))
@ 


Table \ref{tab:hom1physlimgrm2pl} shows the coefficient estimates for the two paramter GRM for the physical limitations scale. It can be seen that the discrimination parameters have lowered signigicantly for Q14 and Q16, while have raised for Q13 and Q15. 

\subsection{MAAS}
\label{sec:maas}



The next instrument examined was the MAAS.  Firstly, the instrument was examined using mokken analysis to check if it could be considered one scale, and whether or not there were violation of monotonicity.

<<maasassumptioncheck, echo=FALSE, results=tex>>=
maas.scales <- aisp(na.omit(maasitems))
print(xtable(as.matrix(maas.scales),label="tab:maasassumptioncheck", caption="Item Attribution to Scales, MAAS, Sample One"))
@

As can be seen from Table \ref{tab:maasassumptioncheck}, the mokken analysis suggests that two items should be dropped from the scale, items 2 and 6. This leaves a thirteen item scale for further analysis. There were no violations of the monotonicity assumption for the reduced scale. The item coefficients (ItemH) are quite low, many of them hang around 0.30, which is the minimum allowed. 

<<maasreduced, echo=FALSE, results=hide>>=
maas.irt <- paste(maas, c(1,3:5,7:15), sep="")
maas.irt <- maasitems[,maas.irt]
@

Next, item ordering was examined for this scale.

<<maasitemord, echo=FALSE, results=hide>>=
maas.iio <- check.iio(na.omit(maas.irt))
print(xtable(maas.iio[["violations"]],label="tab:maasitemord", caption="Invariant Item Ordering Check Results for MAAS"))
@


<<maasmonotonicity, echo=FALSE, results=hide>>=
maas.mono <- check.monotonicity(na.omit(maas.irt))
print(xtable(summary(maas.mono),label="tab:maasmono", caption="Monotonicity Check Results for MAAS"))
@




<<maasitemspcm, echo=FALSE, results=hide>>=
maas.pcm.rasch <- gpcm(na.omit(maas.irt), constraint="rasch")
maas.pcm.1pl <- gpcm(na.omit(maas.irt), constraint="1PL")
maas.pcm.2pl <- gpcm(na.omit(maas.irt), constraint="gpcm")
@

<<maasirtraschprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas.pcm.rasch), label="tab:maaspcmrasch", caption="MAAS Partial Credit Model Coefficients, Rasch"))
@ 

As can be seen from Table \ref{tab:maaspcmrasch}, the Rasch partial credit model is not a good fit to the data. There are numerous violations of the increasing ability scores assumption. Next, a one parameter model (with discrimination estimated from the data) was fitted to the maas items. 

<<maasirtraschprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas.pcm.1pl), label="tab:maaspcm1pl", caption="MAAS Partial Credit Model Coefficients, One Parameter Model"))
@ 

The estimated one parameter model is shown in Table \ref{tab:maaspcm1pl}, and it can clearly be seen that it suffers from the same problems as the rasch model fitted above. Finally, a two parameter partial credit model was fitted to these items. 

<<maasirtraschprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas.pcm.2pl), label="tab:maaspcm2pl", caption="MAAS Partial Credit Model Coefficients, Two Parameter Model"))
@ 

Table \ref{tab:maaspcm2pl} clearly shows that even the two parameter model does not provide a good fit to the data. The next stage of analysis for the Mindfulness scale was to fit one and two parameter Graded Response Models to the data. 

<<maasgrmfit, echo=FALSE, results=hide>>=
maas.grm.1pl <- grm(maas.irt, constrained=TRUE)
maas.grm.2pl <- grm(maas.irt, constrained=FALSE)
@

\begin{figure}
<<maasgrm1plplot, echo=FALSE, fig=TRUE>>=
maasp <- ggplotGRM(maas.grm.1pl)
print(maasp)
@   
  \caption{MAAS Graded Response Model (One Parameter) Ability Thresholds}
  \label{fig:maasgrm1plplot}
\end{figure}

As shown in Figure \ref{fig:maasgrm1plplot}, there were no obvious scaling violations for this scale. 

<<maasgrm1plprint, echo=FALSE, results=hide>>=
print(xtable(coef(maas.grm.1pl), caption="Coefficient Estimates for MAAS One Parameter Graded Response Model, Sample One", label="tab:maasgrm1pl"))
@ 

Table \ref{tab:maasgrm1pl} shows the estimated ability thresholds and discrimination parameter for the one parameter Graded Response Model on the MAAS. The discrimination parameter is moderate, as are the ability estimates, suggesting that this scale may not be suitable for respondents particularly high in mindfulness. 

Next, a two parameter Graded Response Model was examined for the same scale. 

\begin{figure}
<<maasgrm2plot, echo=FALSE, fig=TRUE>>=
maas2plp <- ggplotGRM(maas.grm.1pl)
print(maas2plp)
@   
  \caption{MAAS Graded Response Model (Two parameter) Item Ability Thresholds}
  \label{fig:maasgrm2plplot}
\end{figure}

<<maasgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas.grm.2pl), caption="Coefficient Estimates for MAAS Two Parameter Graded Response Model", label="tab:maasgrm2pl"))
@ 


Table \ref{tab:maasgrm2pl} shows the estimated coefficients for a two parameter Graded Response Model. IT can be seen that Q14 has the highest discriminatory power, and that Q11 has the highest ability threshold, while Q1 has the lowest. Q11 refers to listening to others while engaging in other tasks, and its ability estimates sugegst that it is a good question for pinpointing the abilities of respondents high on the construct of mindfulness. 

<<maasanovacomp, echo=FALSE, results=hide>>=
anova.rasch.maas.1pl <- anova.gpcm(maas.pcm.rasch, maas.pcm.1pl)
anova.1pl.maas.2pl <- anova.gpcm(maas.pcm.1pl, maas.pcm.2pl)
anova.rasch.maas.2pl <- anova.gpcm(maas.pcm.rasch, maas.pcm.2pl)
@

The three models were subjected to anova comparison and the 1 parameter model was significantly ($p \le 0.001$) better than the rasch model, and the 2 parameter model was significantly better than the 1 parameter model ($p \le 0.001$). However, the ultimate test is the ability of the model to predict out-of-sample data. 

\subsection{LOTR}
\label{sec:lotr}



The Life Orientation Test was the next instrument to be examined using the IRT approach.

Firstly, the scale analysis was conducted to determine which items fit best together.

<<lotrscales, echo=FALSE, results=tex>>=
lotr.scales <- aisp(na.omit(lotritems))
print(xtable(as.matrix(lotr.scales),label="tab:lotrscales"))
@

As can be seen from Table \ref{tab:lotrscales}, all of the items meet the assumptions of a unidimensional scale.  Next, the item orderings were examined.

<<lotritemord, echo=FALSE, results=hide>>=
lotr.iio <- check.iio(na.omit(lotritems))
print(xtable(lotr.iio[["violations"]],label="tab:lotritemord"))
@


Q1 needs to be removed from the scale in order to meet the assumptions of the model. There were no violations of monotonicity in the sample.

<<lotrmono, echo=FALSE, results=hide>>=
lotr.mono <- check.monotonicity(na.omit(lotritems))
print(xtable(summary(lotr.mono),label="tab:lotrmono"))
@


<<lotrreduced, echo=FALSE, results=hide>>=
lotr.paste <- paste(lotr, c(3,4,7,9,10), sep="")
lotr.irt <- lotritems[,lotr.paste]
@

<<lotrmodelsirt, echo=FALSE, results=hide>>=
lotr.pcm.rasch <- gpcm(na.omit(lotr.irt), constraint="rasch")
lotr.pcm.1pl <- gpcm(na.omit(lotr.irt), constraint="1PL")
lotr.pcm.2pl <- gpcm(na.omit(lotr.irt), constraint="gpcm")
@

<<lotrpcmraschprint, echo=FALSE, results=tex>>=
print(xtable(coef(lotr.pcm.rasch), label="tab:lotrpcmrasch", caption="Rasch Partial Credit Model for Life Orientation Test, Revised"))
@ 

As shown in Table \ref{tab:lotrpcmrasch}, the model does not provide a good fit to the data. There are numerous violations of the ordering assumptions of the model. Next, a one parameter model was fit to the data. 

<<lotrpcm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(lotr.pcm.1pl), label="tab:lotrpcm1pl", caption="One parameter Partial Credit Model for Life Orientation Test, Revised"))
@ 

As can be seen in Table \ref{tab:lotrpcm1pl}, there are again some violations of the ability ordering assumption (LOTR4, category 3). Next, a two parameter PCM was fitted to the LOTR items. 

<<lotrpcm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(lotr.pcm.2pl), label="tab:lotrpcmgpcm", caption="Two Parameter Partial Credit Model for Life Orientation Test, Revised"))
@ 

The coefficients in Table \ref{tab:lotrpcmgpcm} show that again, LOTR4 ensures that the model does not fit correctly. 

<<lotranovacomp, echo=FALSE, results=hide>>=
anova.rasch.lotr.1pl <- anova.gpcm(lotr.pcm.rasch, lotr.pcm.1pl)
anova.1pl.lotr.2pl <- anova.gpcm(lotr.pcm.1pl, lotr.pcm.2pl)
anova.rasch.lotr.2pl <- anova.gpcm(lotr.pcm.rasch, lotr.pcm.2pl)
@

The results of the model comparison showed that the rasch model was not significantly different from the one parameter model ($p=0.862$), but that the two parameter model provided a significantly better fit to the data ($p \le 0.001$), even with a penalty for the extra parameters.

However, the difference in likelihoods was extremely small between the Rasch and two parameter models (20.15), and the BIC suggested that the Rasch model was a better fit.  One issue for the BIC is that it presumes that a true model exists amongst the candidate models, which is almost certainly not the case in this (or indeed any other psychological research) case.

The next stage in the analysis of the LOTR was the fitting of one and two parameter Graded Response Models. 

<<lotrgrmfit, echo=FALSE, results=hide>>=
lotr.grm.1pl <- grm(lotr.irt, constrained=TRUE)
lotr.grm.2pl <- grm(lotr.irt, constrained=FALSE)
@ 
\begin{figure}
<<lotrgrm1plplot, echo=FALSE, fig=TRUE>>=
lotr1plgrm <- ggplotGRM(lotr.grm.1pl)
print(lotr1plgrm)
@   
  \caption{One Parameter Graded Response Model for LOTR Item Ability Thresholds}
  \label{fig:lotr1plgrm}
\end{figure}

Figure \ref{fig:lotr1plgrm} shows the estimated threshold points for each of the categories for each item. It can be seen that there are no violations of item ordering, and that LOTR4 is the hardest item to endorse, while LOTR3 appears to be the easiest. 

\begin{figure}
<<lotr2plgrmplot, echo=FALSE, fig=TRUE>>=
lotr2plgrm <- ggplotGRM(lotr.grm.2pl)
print(lotr2plgrm)
@   
  \caption{Two Parameter Graded Response Model Item Ability Threshold Plot}
  \label{fig:lotr2plgrm}
\end{figure}


Again, Figure \ref{fig:lotr2plgrm} shows no violations of the modelling assumptions for the two parameter GRM. The next step was to assess which of the models provided the best fit to the data using a likelihood ratio test.

<<anovagrmlotr, echo=FALSE, results=hide>>=
anova.grm.lotr <- anova(lotr.grm.1pl, lotr.grm.2pl)
@ 

The two parameter model was significantly better than the one parameter model (p=0.001), but the BIC suggested that the one parameter model provided a better overall fit to the data. 

<<lotr2plestimates, echo=FALSE, results=tex>>=
print(xtable(coef(lotr.pcm.2pl),label="tab:lotr2plestimates", caption="Coefficient Estimates for LOTR Two Paramter Graded Response Model"))
@

The estimates for the two parameter model can be seen in Table \ref{tab:lotr2plestimates}.

\section{Predictions}
\label{sec:predictions}

As discussed in the methodology , one of the problems with psychometric methods is the problem of overfitting. This problem was dealt with for this research by building the models on the first dataset, and then fitting them on a seperate sample \footnote{a process conventionally known as replication}. 

This section covers the following assessments of models developed on Sample One

\begin{enumerate}
\item Confirmatory Factor Analyses are carried out for the RAND MOS, the LOTR and the MAAS. 

\item The IRT models are assessed using a process of prediction of factor scores using the old model and a new one developed on the second sample. 

\item The regression models are refit to new data and the errors in prediction are examined. 
\end{enumerate}

<<hom2scales, echo=FALSE, results=hide>>=
randitems.paste <- paste(rand, c(1:36), sep="")
randitems2 <- hom2[,randitems.paste]
maasitems.paste <- paste(maas, c(1:15), sep="")
maasitems2 <- hom2[,maasitems.paste]
lotritems.paste <- paste(lotr, c(1,3,4,7,9,10), sep="")
lotritems2 <- hom2[,lotritems.paste]
@



\subsection{Confirmatory Factor Analyses}
\label{sec:conf-fact-analys}




\subsubsection{RAND MOS CFA on Sample Two (Split A)}
\label{sec:rand-mos-cfa}



<<rand2semhom2, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "General and Emotional Health")
physfun <- paste(rand, c(3:12), sep="")
genemhealth <- paste(rand,c(1,14,17:36), sep="")
Rand2model2 <- mxModel(name="RAND2Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="General and Emotional Health", to=genemhealth),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                       mxData(observed=cov(na.omit(randitems2)), type="cov", numObs=281)
                      )
rand2fit2 <- mxRun(Rand2model2)
rand2summ2 <- summary(rand2fit2)
@

<<rand4semhom2, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "Social and Emotional Functioning", "General Health", "Physical Limitations")
physfun <- paste(rand, c(3:12), sep="")
genhealth <- paste(rand, c(1,33:36), sep="")
physlim <- paste(rand, c(13:16,21:22), sep="")
emsocfunc <- paste(rand, c(17:20, 23:32), sep="")
Rand4model2 <- mxModel(name="RAND4Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="Social and Emotional Functioning", to=emsocfunc),
                      mxPath(from="General Health", to=genhealth),
                      mxPath(from="Physical Limitations", to=physlim),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=281)
                      )
rand4fit2 <- mxRun(Rand4model2)
rand4summ2 <- summary(rand4fit2)
@ 
<<rand8semhom2, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "Social and Emotional Wellbeing", "General Health", "Emotional Role Limitations", "Role Limitations", "Fatigue", "Pain", "Energy")
physfun <- paste(rand, c(3:12), sep="")
socemwellbeing <- paste(rand, c(20,23:28, 30, 32), sep="")
genhealth <- paste(rand, c(1,33:36), sep="")
emrolelim <- paste(rand, c(17:20), sep="")
rolelim <- paste(rand, c(13:16), sep="")
energy <- paste(rand, c(29, 31), sep="")
pain <- paste(rand, c(21:22), sep="")
fatigue <- paste(rand, c(2, 23, 27), sep="")
Rand8model2 <- mxModel(name="RAND8Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="Social and Emotional Wellbeing", to=socemwellbeing),
                      mxPath(from="General Health", to=genhealth),
                      mxPath(from="Emotional Role Limitations", to=emrolelim),
                      mxPath(from="Role Limitations", to=rolelim),
                      mxPath(from="Energy", to=energy),
                      mxPath(from="Pain", to=pain),
                      mxPath(from="Fatigue", to=fatigue),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=281))
                      
rand8fit2 <- mxRun(Rand8model2)
rand8summ2 <- summary(rand8fit2)
@

<<rand13sem2, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning","Emotional Role Limitations","Role Limitations", "Positive Emotionality", "Physical Exertion", "Positive Health","Fatigue","Pain", "Emotional Problems", "Sickness", "Vigourous Activity","RAND20", "RAND24" )
physfun <- paste(rand, c(3:12), sep="")
emrolelim <- paste(rand, c(17:20), sep="")
rolelim <- paste(rand, c(13:16), sep="")
posemotions <- paste(rand, c(23,26,27,30), sep="")
physexertions <- paste(rand, c(3,4:5, 8), sep="")
poshealth <- paste(rand, c(1,34, 36), sep="")
fatigue <- paste(rand, c(29, 31), sep="")
pain <- paste(rand, c(21:22), sep="")
emproblems <- paste(rand, c(24, 25, 28, 30), sep="")
sickness <- paste(rand, 33, sep="")
vigactivity <- paste(rand, c(3,6), sep="")
rand20 <- "RANDQ20"
rand24 <- "RANDQ24"
Rand13model2 <- mxModel(name="RAND13Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                       mxPath(from="Emotional Role Limitations", to=emrolelim),
                       mxPath(from="Role Limitations", to=rolelim),
                       mxPath(from="Positive Emotionality", to=posemotions),
                      mxPath(from="Physical Exertion", to=physexertions),
                       mxPath(from="Positive Health", to=poshealth),
                       mxPath(from="Pain", to=pain),
                       mxPath(from="Fatigue", to=fatigue),
                       mxPath(from="Emotional Problems", to=emproblems),
                       mxPath(from="Vigourous Activity", to=vigactivity),
                      mxPath(from="Sickness", to=sickness),
                      mxPath(from="RANDQ20", to=rand20),
                       mxPath(from="RANDQ24", to=rand24),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=281)
                      )
rand13fit2 <- mxRun(Rand13model2)
rand13summ2 <- summary(rand13fit2)
@ 

<<randsemcomparehom2, echo=FALSE, results=tex>>=
randsemcomp.hom2 <- mxCompare(base=rand2fit2, comparison=c(rand4fit2, rand8fit2, rand13fit2))
print(xtable(randsemcomp,label="tab:randsemcomparehom2", caption="Model Comparison for RAND MOS data using Models from Sample 1 on Sample 2" ), scalebox=0.9)
@

As Table \ref{tab:randsemcomparehom2} shows, the eight factor model provided the best fit to this unseen data, similarly to the data on which the model was created. 

\subsubsection{MAAS CFA on Sample Two (Split A)}
\label{sec:maas-cfa-sample}



<<splitsample, echo=FALSE, results=hide>>=
set.seed(17)
maassamp <- sample(1:1109, 1109)
ms1 <- maassamp[1:370]
ms2 <- maassamp[371:740]
ms3 <- maassamp[741:length(maassamp)]
maasitems2a <- maasitems2[ms1,]
maasitems2b <- maasitems2[ms2,]
maasitems2c <- maasitems2[ms3,]
maasitems.nota <- maasitems2[c(ms2, ms3),]
maasitems.notb <- maasitems2[c(ms1, ms3),]
maasitems.notc <- maasitems2[c(ms1, ms2),]
lotrsamp <- sample(1:1109, 1109)
lr1 <- lotrsamp[1:370]
lr2 <- lotrsamp[371:740]
lr3 <- lotrsamp[741:length(lotrsamp)]
lotritems2a <- lotritems2[lr1,]
lotritems2b <- lotritems2[lr2,]
lotritems2c <- lotritems2[lr3,]
lotritems.nota <- lotritems2[c(lr2, lr3),]
lotritems.notb <- lotritems2[c(lr1, lr3),]
lotritems.notc <- lotritems2[c(lr1, lr2),]
randsamp <- sample(1:1109, 1109)
r1 <- randsamp[1:370]
r2 <- randsamp[371:740]
r3 <- randsamp[741:length(randsamp)]
randitems2a <- randitems[r1,]
randitems2b <- randitems[r2,]
randitems2c <- randitems[r3,]
randitems.nota <- randitems[c(r2, r3),]
randitems.notb <- randitems[c(r1, r3),]
randitems.notc <- randitems[c(r1, r2),]
scales <- hom2[,c(4,66:75)]
scales.samp <- sample(1:1109, 1109)
s1.samp <- scales.samp[1:370]
s2.samp <- scales.samp[371:740]
s3.samp <- scales.samp[741:length(scales.samp)]
scales2a <- scales[s1.samp,]
scales2b <- scales[s2.samp,]
scales2c <- scales[s3.samp,]
@

<<maas7sem2, echo=FALSE, results=hide>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Automatic Behaviour", "Preoccupations", "Hurry",  "Lack of Present Focus", "Food Mindlessness", "Inattention", "Lack of Awareness")
automaticbehaviour <- paste(maas, c(9:12), sep="")
preoccupations <- "MAASQ13"
hurry <- paste(maas, c(7:8), sep="")
lackpresfocus <- paste(maas, c(2:3), sep="")
foodmindlessness <- "MAASQ15"
inattention <- "MAASQ6"
lackawareness <- paste(maas, c(1,4:5,9), sep="")
Maas7model2 <- mxModel(name="MAAS7Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Automatic Behaviour", to=automaticbehaviour),
                      mxPath(from="Preoccupations", to=preoccupations),
                      mxPath(from="Hurry", to=hurry),
                      mxPath(from="Lack of Present Focus", to=lackpresfocus),
                      mxPath(from="Lack of Awareness", to=lackawareness),
                           mxPath(from="Inattention", to=inattention),
                           mxPath(from="Lack of Present Focus", to=lackpresfocus),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2a)), type="cov", numObs=313)
                      )
maas7fit2 <- mxRun(Maas7model2)
maas7summ2 <- summary(maas7fit2)
@


<<maas1sem2, echo=FALSE, results=hide>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Unawareness")
unawareness <- paste(maas, c(1:15), sep="")
Maas1model2 <- mxModel(name="MAAS1Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Unawareness", to=unawareness),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems)), type="cov", numObs=313)
                      )
maas1fit2 <- mxRun(Maas1model2)
maas1summ2 <- summary(maas1fit2)
@




<<maassemcompare2, echo=FALSE, results=tex>>=
maascomp2 <- mxCompare(base=maas1fit2, comparison=maas7fit2)
maascomp.xtab2 <- xtable(maascomp2,label="tab:maassemcompare2", caption="Comparison of Sample One MAAS Factor Models on a subset of Sample Two Data (Split A)")
print(maascomp.xtab2)
@

Table \ref{tab:maassemcompare2} demonstrates that the MAAS 1 factor model provided the best fit to the subsample of data ($n=313$) used to test the model.

\subsubsection{LOTR CFA on Sample Two (Split A)}
\label{sec:lotr-cfa-sample}



<<lotr1sem2, echo=FALSE, results=hide>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism")
optimism <- paste(lotr, c(1,3,4,7,9,10), sep="")
Lotr1model2 <- mxModel(name="LOTR1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2a)), type="cov", numObs=313)
                      )
lotr1fit2 <- mxRun(Lotr1model2)
lotr1summ2 <- summary(lotr1fit2)
@


<<lotr3sem, echo=FALSE, results=hide>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism", "Pessimism", "Pessimism2")
optimism <- paste(lotr, c(1,4,10), sep="")
pessimism <- paste(lotr, c(3,7), sep="")
pessimism2 <- paste(lotr, c(7,9), sep="")
Lotr3model2 <- mxModel(name="LOTR3Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from="Pessimism", to=pessimism),
                      mxPath(from="Pessimism2", to=pessimism2),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems)), type="cov", numObs=313)
                      )
lotr3fit2 <- mxRun(Lotr3model2)
lotr3summ2 <- summary(lotr3fit2)
@

<<lotrcompare2, echo=FALSE, results=tex>>=
lotrcomp2 <- mxCompare(base=lotr1fit2, comparison=lotr3fit2)
lotrcomp.xtab2 <- xtable(lotrcomp2,label="tab:lotrcompare2", caption="Comparison of SEM Results for Sample One LOTR Factor Models on a Subset of Sample Two (Split A)")
print(lotrcomp.xtab2)
@

Table \ref{tab:lotrcompare2}, shows that the one factor model provides the best fit to the subsample of data used to examine the models performance on new data.

The next part of the analyses was examining the predictive ability of the IRT models developed on sample one.

\subsection{Confirmatory IRT Analyses}
\label{sec:conf-irt-analys}

\subsubsection{Physical Functioning Scale}
\label{sec:phys-funct-scale}



The first scale to be examined was the RAND MOS Physical Functioning Scale. 

<<hom1physfuntest, echo=FALSE, results=tex>>=
rand2a.physfun <- randitems2a[,paste(rand, 3:12, sep="")]
hom1.grm.1pl.test <- testIRTModels(physfun.grm.1pl, rand2a.physfun, gpcmconstraint=NULL, grmconstraint=TRUE)
hom1.grm.2pl.test <- testIRTModels(physfun.grm.2pl, rand2a.physfun, gpcmconstraint=NULL, grmconstraint=FALSE)
hom1.grm.test.all <- rbind(hom1.grm.1pl.test, hom1.grm.2pl.test)
print(xtable(hom1.grm.test.all, label="tab:hom1physfungrmtest", caption="Performance of One and Two Parameter Graded Response Models from Sample One on a Subset of Sample Two (Split A)"))
@

As can be seen from Table \ref{tab:hom1physfungrmtest}, the one parameter model performed better on the unseen data. 

\subsubsection{Emotional Health Scale}
\label{sec:emot-health-scale}



Next, the performance of the two emotional health GRM\'s on unseen data was assessed. 
<<hom1emhealthtest, echo=FALSE, results=tex>>=
emhealth2a <- randitems2a[,paste(rand, c(16:19,25,28,29,31), sep="")]
hom1.emhealth.grm.1pl.test <- testIRTModels(hom1.emhealth.grm.1pl, emhealth2a, gpcmconstraint=NULL, grmconstraint=TRUE)
hom1.emhealth.grm.2pl.test <- testIRTModels(hom1.emhealth.grm.2pl, emhealth2a, gpcmconstraint=NULL, grmconstraint=FALSE)
hom1.emhealth.grm.all <- rbind(hom1.emhealth.grm.1pl.test, hom1.emhealth.grm.2pl.test)
print(xtable(hom1.emhealth.grm.all, caption="Performance of One and Two Parameter Graded Response Models from Sample One on a subset of Data from Sample Two (Split A)", label="tab:hom1emhealthgrmtest"))
@ 

As can be seen from Table \ref{tab:hom1emhealthgrmtest}, the one parameter model performed best on the unseen data.

\subsubsection{Energy Scale}
\label{sec:energy-scale}




Next, we assess the performance of the two graded response models on the energy scale.

<<hom1energygrmtest, echo=FALSE, results=tex>>=
energy2a <- randitems2a[,paste(rand, c(1,20:23,26:27,30,34,36), sep="")]
hom1.energy.grm.1pl.test <- testIRTModels(hom1.energy.grm.1pl, energy2a, gpcmconstraint=NULL, grmconstraint=TRUE)
hom1.energy.grm.2pl.test <- testIRTModels(hom1.energy.grm.2pl, energy2a, gpcmconstraint=NULL, grmconstraint=FALSE)
hom1.energy.grm.all <- rbind(hom1.energy.grm.1pl.test, hom1.energy.grm.2pl.test)
print(xtable(hom1.energy.grm.all, caption="Performance of One and Two Parameter Graded Response Models from Sample One on Sample Two (Split A)", label="tab:hom1energygrmtest"))
@ 

As can be seen from Table \ref{tab:hom1energygrmtest}, the one parameter GRM provided the best fit to the unseen data. 

\subsubsection{Physical Limitations Scale}
\label{sec:phys-limit-scale}

Next, we examine the performance of one and two parameter GRM's on the physical limitations scale from Sample One. 

<<hom1physlimtest, echo=FALSE, results=tex>>=
physlim2a <- randitems2a[,paste(rand, c(13:16), sep="")]
hom1.physlim.grm.1pl.test <- testIRTModels(hom1.physlim.grm.1pl, physlim2a, gpcmconstraint=NULL, grmconstraint=TRUE)
hom1.physlim.grm.2pl.test <- testIRTModels(hom1.physlim.grm.2pl, physlim2a, gpcmconstraint=NULL, grmconstraint=FALSE)
hom1.physlim.grm.all <- rbind(hom1.physlim.grm.1pl.test, hom1.physlim.grm.2pl.test)
rownames(hom1.physlim.grm.all) <- c("One Parameter GRM", "Two Parameter GRM")
print(xtable(hom1.physlim.grm.all, caption="Performance of One and Two Parameter GRM's from Sample One on a subset of Sample Two Data (Split A)", label="tab:hom1physlimgrmtest"))
@ 

As can be seen from Table \ref{tab:hom1physlimgrmtest}, the one parameter GRM provided the best fit to the unseen data. 

Next, the performance of the MAAS models are assessed on unseen data. 

<<hom1maasgrmtest, echo=FALSE, results=tex>>=
maas.irt2a <- maasitems2a[,paste(maas, c(1,3:5,7:15), sep="")]
hom1.maas.grm.1pl.test <- testIRTModels(maas.grm.1pl, maas.irt2a, gpcmconstraint=NULL, grmconstraint=TRUE) 
hom1.maas.grm.2pl.test <- testIRTModels(maas.grm.2pl, maas.irt2a, gpcmconstraint=NULL, grmconstraint=FALSE) 
hom1.maas.grm.all <- rbind(hom1.maas.grm.1pl.test, hom1.maas.grm.2pl.test)
rownames(hom1.maas.grm.all) <- c("One Parameter GRM", "Two Parameter GRM")
print(xtable(hom1.maas.grm.all, caption="Performance of MAAS One and Two Parameter GRM's on Unseen Data (Sample Two, Split A)", label="tab:hom1maasgrmtest"))
@ 

As can be seen from Table \ref{tab:hom1maasgrmtest}, the one parameter GRM provided a better fit to the unseen  data. 

Finally, the performance of the one and two parameter IRT models for the LOT-R were assessed. 

<<hom1lotrgrmtest, echo=FALSE, results=tex>>=
lotr.irt2a <- lotritems2a[,paste(lotr, c(3,4,7,9,10), sep="")]
hom1.lotr.grm.1pl.test <- testIRTModels(lotr.grm.1pl, lotr.irt2a, gpcmconstraint=NULL, grmconstraint=TRUE)
hom1.lotr.grm.2pl.test <- testIRTModels(lotr.grm.2pl, lotr.irt2a, gpcmconstraint=NULL, grmconstraint=FALSE)
hom1.lotr.grm.all <- rbind(hom1.lotr.grm.1pl.test, hom1.lotr.grm.2pl.test)
rownames(hom1.lotr.grm.all) <- c("One Parameter GRM", "Two Parameter GRM")
print(xtable(hom1.lotr.grm.all, caption="Performance of LOTR One and Two Parameter GRM\'s on Unseen Data (Sample Two, Split A)", label="tab:hom1lotrgrmtest"))
@ 


As can be seen from Table \ref{tab:hom1lotrgrmtest}, the one parameter model provided a better fit to the unseen data. 

\subsection{Regression Model Predictions}
\label{sec:regr-model-pred}

<<optsetup2a, echo=FALSE, results=hide>>=
scales2a.full <- na.omit(scales2a)
scales2a.full2 <- scales2a.full[,-11]
opt.test2a <- with(scales2a.full, optimism)
opt.train.ind <- with(scales2a.full, createDataPartition(optimism, p=0.75, list=FALSE))
scales2a.opt.train <- scales2a.full[opt.train.ind,]
scales2a.opt.test <- scales2a.full[-opt.train.ind,]
opt.train2a <- with(scales2a.opt.train, optimism)
opt.pred.train2a <- scales2a.opt.train[,-11]
## opt.test2a <- with(scales2a.opt.test, optimism)
opt.pred.test2a <- scales2a.opt.test[,-11]
@ 

<<optlassotest, echo=FALSE, results=tex>>=
opt.lasso.test2a <- penalisedRegression(x=trainpred, y=trainopt, traindata=hom1.train.opt, testdata=scales2a.full2, newy=opt.test2a, alpha=1, nfolds=10, type="coefficients")
opt.lasso.test2a.pred <- penalisedRegression(x=trainpred, y=trainopt, traindata=hom1.train.opt, testdata=scales2a.full2, newy=opt.test2a, alpha=1, nfolds=10, type="response")
print(xtable(as.matrix(opt.lasso.test2a), caption="Coefficients for Optimism Regression from Study One on Sample Two (Split A)", label="tab:homoptlasso2a"))
opt.lasso.test2a.resp <- penalisedRegression (x=trainpred, y=trainopt, traindata=hom1.train.opt, testdata=scales2a.full2, newy=opt.test2a, alpha=1, nfolds=10, type="response")
@ 

As can be seen from Table \ref{tab:homoptlasso2a}, the general trend for coefficients was similar across the two samples. 

Next, the accuracy of the predictions was assessed using a RMSEA approach. For the optimism lasso data. The error in the estimation was equal to 0.755, suggesting that the predictions were approximately 25\% accurate. This can be seen more clearly in Figure \ref{fig:plotoptlassopred} below. As can be seen there is no real systematic error in the predictions, they are just inaccurate.


\begin{figure}
<<plotlassopredobs, echo=FALSE, figure=TRUE>>=
print (ggplot (opt.lasso.test2a.resp, aes (x=pred, y=obs))+geom_point ()+geom_smooth (method="lm"))
@   
  \caption{Predicted versus observed values of Optimism in second sample, Split A, lasso regression}
  \label{fig:plotoptlassopred}
\end{figure}


Next, the ridge regression model was applied to the new data. 

<<optridge2a, echo=FALSE, results=tex>>=
opt.ridge.test2a <- penalisedRegression(x=trainpred, y=trainopt, traindata=hom1.train.opt, testdata=scales2a.full2, newy=opt.test2a, alpha=0, nfolds=10, type="coefficients")
print(xtable(as.matrix(opt.ridge.test2a), caption="Coefficients for Optimism Regression from Study One on Sample Two (Split A)", label="tab:homoptridge2a"))
opt.ridge.test2a.resp <- penalisedRegression (x=trainpred, y=trainopt, traindata=hom1.train.opt, testdata=scales2a.full2, newy=opt.test2a, alpha=0, nfolds=10, type="response")
rmsea.ridge.opt2a <- rmsea (opt.ridge.test2a.resp)
@ 

As can be seen from Table \ref{tab:homoptridge2a} mindfulness again appears to be the most useful predictor of optimism, and in general the coefficients are quite similar to those from Study One. The RMSEA approach showed that the ridge regression model had an error of 0.749, which is extremely similar to that of the lasso model. 
\begin{figure}
<<optridgeplot2a, echo=FALSE, figure=TRUE>>=
print (ggplot (opt.ridge.test2a.resp, aes (x=pred, y=obs))+geom_point()+geom_smooth)
@   
  \caption{Ridge Regression Predictions, Optimism, Split 2A}
  \label{fig:ridgepredplot2a}
\end{figure}


As can be seen from Figure \ref{ridgepredplot2a}, again the errors do not appear to be systematicall biased, but rather spread both evenly above and below the line of perfect prediction. 

Finally, the model developed by stepwise selection was applied to the new data. 

<<optstep2a, echo=FALSE, results=hide>>=
opt.step.test2a <- lm(optimism~generalhealth+Age+emwellbeing, data=scales2a.full)
opt.step.pred.resp <- predict (opt.step.test, newdata=scales2a.full, type="response")
opt.step.pred.obs <- data.frame (pred=opt.step.pred.resp, obs=opt.test2a)
rmsea.opt.step2a <- rmsea (opt.step.pred.obs)
@ 
<<optstep2aprint, echo=FALSE, results=tex>>=
print(xtable(summary(opt.step.test), caption="Coefficients for Stepwise Selected Model on Test Data", label="tab:homstepopttest2a"))
@ 

\begin{figure}
<<optsteppredplot, echo=FALSE, figure=TRUE>>=
print (ggplot (opt.step.pred.obs, aes (x=pred, y=obs))+geom_point ()+geom_smooth (method="lm"))
@   
  \caption{Stepwise Regression Predictions, Optimism Split 2A}
  \label{fig:optstepredplot}
\end{figure}


As can be seen from Table \ref{tab:homstepopttest2a}, the stepwise model has an extremely significant coefficient for emotional well being, and a moderately significant coefficient for general health and age. The coefficient signs are all in line with those from study one, and appear to match those from the other regressions. The RMSEA metric gave a value of 0.746, which is marginally better than either the lasso or ridge predictions. Figure \ref{fig:optsteppredplot} shows this in more detail, and in fact is quite similar to the plots for the lasso and ridge solutions. This would seem to suggest that each of the three models are converging on a relatively common solution, increasing our confidence that we have explored the linear relationships between these variables. 


\part{Sample Two}

Next, sample two was examined in and of itself. This sample was collected through online methods, as described above and had large amounts of missing data concentrated in RAND Q13-16. 

\begin{figure}
<<optplot2, echo=FALSE, fig=TRUE>>=
optplot2 <- ggplot(hom2, aes(x=generalhealth, y=optimism))+layer(geom="smooth", method="lm")
print(optplot2)
@  
  \caption{Scatterplot with Linear Regression Smooth of Relationship between General Health and Optimism}
  \label{fig:genhealthoptplot2}
\end{figure}


The relationship between optimism and health is even clearer in sample 2 (Figure \ref{fig:genhealthoptplot2}) (note the narrower confidence intervals and the steeper slope of the line).

\section{Regression Analyses}
\label{sec:regression-analyses}

Similiar regression analyses were carried out on the test sample from Study 2 as were carried out on the first sample.  The first action taken was to predict the coefficients in the new data set from the models developed on the first sample.







%% \subsubsection{Split B}
%% \label{sec:split-b}

\subsection{MAAS}
\label{sec:maas-samp-two}

\subsection{Factor Analyses}
\label{sec:fact-maas}

<<maas2bparallel, echo=FALSE, fig=TRUE, results=hide>>=
maas2b.parallel <- fa.parallel(na.omit(maasitems2b))
vss.maas.2b <- VSS(na.omit(maasitems2b))
@

The parallel analysis procedure for Split B suggests that this sample of the responses to the MAAS has five factors, while the MAP criterion suggests that it has only one.  Following our previous approach, each of these factor solutions will be examined and interpreted before a CFA is applied on the remainder of the dataset.

For split C, the same procedure was carried out, and the various methods both suggested five and one factors, respectively. 

<<maas2cparallel, echo=FALSE, fig=TRUE, results=hide>>=
maas2c.parallel <- fa.parallel(na.omit(maasitems2c))
vss.maas.2c <- VSS(na.omit(maasitems2c))
@



<<maas2b1, echo=FALSE, results=tex>>=
maas2b.fact1 <- fa(maasitems2b, 1, rotate="oblimin", fm="pa")
print(FactorXtab(maas2b.fact1,label="tab:maas2bfact1", caption="MAAS One Factor Solution, Sample 2B"))
@

The solution shown above in Table \ref{tab:maas2bfact1} shows adequate loadings of all the questions on a first factor which can be named mindfulness.  This solution explained 41\% of the variance, which is quite low for a factor solution.

<<maas2c1, echo=FALSE, results=tex>>=
maas2c.fact1 <- fa(maasitems2c, 1, rotate="oblimin", fm="pa")
print(FactorXtab(maas2c.fact1,label="tab:maas2cfact1", caption="MAAS One Factor Solution, Sample 2C"))
@

Table \ref{tab:maas2cfact1} shows the one factor loadings for split C. In this split, the same solution only explained 36\% of the variance in the item loadings, which is in line with Split B, though much lower than the original published research in which the MAAS was developed. 



<<maas2bfact5, echo=FALSE, results=tex>>=
maas2b.fact5 <- fa(maasitems2b, 5, rotate="oblimin", fm="pa")
print(FactorXtab(maas2b.fact5,label="tab:tcq2bmaasfact5", caption="Factor Loadings, MAAS Five Factor Solution, Sample Two, Split B"))
@
This five factor solution explained 54\% of the variance in the sample.

PA1: "MAASQ5",  "MAASQ6",  "MAASQ7",  "MAASQ8",  "MAASQ9",  "MAASQ10".  This factor has come through in most of the previous solutions, and can again be termed distractability.

PA2: "MAASQ1", "MAASQ2", "MAASQ3".  Again, these items have clustered together previously, and this factor is again termed lack of present awareness.

PA3: "MAASQ4", "MAASQ5".  This factor is again termed lack of somatic awareness.

PA4: "MAASQ13", "MAASQ14".  This factor can best be termed as lack of attention.

PA5: "MAASQ10", "MAASQ11", "MAASQ12", "MAASQ14", "MAASQ15".  This factor again can be termed distractability.

<<maas2cfact5, echo=FALSE, results=tex>>=
maas2c.fact5 <- fa(maasitems2c, 5, rotate="oblimin", fm="pa")
print(FactorXtab(maas2c.fact5,label="tab:tcq2cmaasfact5", caption="Factor Loadings, MAAS Five Factor Solution, Sample Two, Split B"))
@

PA1: "MAASQ7"  "MAASQ8"  "MAASQ9"  "MAASQ10" "MAASQ11" "MAASQ12" "MAASQ14"
All of the items in this factor relate to a lack of attention to the present, and it can probably be best termed lack of present focus. 
PA2: "MAASQ9"  "MAASQ13"
Q9 loads on both PA1 and PA2, and as PA2 has really only Q13 loading to any major extent on it, no interpretation of this factor was performed. It was named lack of present awareness. 
PA4: "MAASQ2"  "MAASQ3"  "MAASQ14"
This factor can probably best be termed lack of attention. 
PA5: "MAASQ1" "MAASQ4" "MAASQ5"
Most of these items relate to lack of bodily attention, and this can probably best be termed lack of somatic awareness. 
PA3: "MAASQ4" "MAASQ6" "MAASQ7" "MAASQ8"
Note that MAASQ4 loads slightly on both PA3 and PA5, and is not considered in the interpretation here. These items can probably best be termed lack of awareness. 

The factor solution for Split C (shown in Table \ref{tab:hom2cmaasfact5}) is somewhat different from the five factor solution for Split B. Note that PA3 is somewhat similar to PA1 (Distractability) from Split B. 

Now, we examine the fit indices for the five solutions.

<<maas2bfitindices, echo=FALSE, results=tex>>=
maas2b.1fit <- FitIndices(maas2b.fact1)
maas2b.5fit <- FitIndices(maas2b.fact5)
maas2bfit <- as.data.frame(cbind(maas2b.1fit,maas2b.5fit))
print(xtable(maas2bfit,label="tab:hom2bmaassemcomp", caption=" Comparison of Fit Indices for MAAS Factor Solutions, Sample Two Split B"), scalebox=0.5)
@


<<maas2cfitindices, echo=FALSE, results=tex>>=
maas2c.1fit <- FitIndices(maas2c.fact1)
maas2c.5fit <- FitIndices(maas2c.fact5)
maas2cfit <- as.data.frame(cbind(maas2c.1fit,maas2c.5fit))
print(xtable(maas2cfit,label="tab:hom2cmaassemcomp", caption=" Comparison of Fit Indices for MAAS Factor Solutions, Sample Two Split B"), scalebox=0.8)
@

<<maas2bsemon2c, echo=FALSE, results=hide>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Unawareness")
unawareness <- paste(maas, c(1:15), sep="")
Maas1model2b <- mxModel(name="MAAS12b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Unawareness", to=unawareness),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2c)), type="cov", numObs=370)
                      )
maas1fit2b <- mxRun(Maas1model2b)
maas1summ2b <- summary(maas1fit2b)
@ 


<<maas2csemon2b, echo=FALSE, results=hide>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Unawareness")
unawareness <- paste(maas, c(1:15), sep="")
Maas1model2c <- mxModel(name="MAAS12c",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Unawareness", to=unawareness),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2b)), type="cov", numObs=370)
                      )
maas1fit2c <- mxRun(Maas1model2c)
maas1summ2c <- summary(maas1fit2c)
@ 




<<maas5sem2b, echo=FALSE, results=hide>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Lack of Bodily Awareness", "Distractability", "Lack of present focus", "Lack of somatic awareness", "Lack of attention")
bodyunaware <- paste(maas, c(6:10), sep="")
distractability <- paste(maas, c(11:15), sep="")
lackpresfocus <- paste(maas, c(1:3), sep="")
lacksomaware <- paste(maas, c(4,5), sep="")
lackattention <- paste(maas, c(13,14), sep="")
Maas5model2b <- mxModel(name="MAAS52b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Lack of Bodily Awareness", to=bodyunaware),
                        mxPath(from="Distractability", to=distractability),
                        mxPath(from="Lack of present focus", to=lackpresfocus),
                        mxPath(from="Lack of somatic awareness", to=lacksomaware),
                        mxPath(from="Lack of attention", to=lackattention),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2c)), type="cov", numObs=370)
                      )
maas5fit2b <- mxRun(Maas5model2b)
maas5summ2b <- summary(maas4fit2b)
@

<<maas5sem2c, echo=FALSE, results=hide>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("lack of present focus", "lack of present awareness", "lack of attention", "Lack of somatic awareness", "lack of awareness")
bodyunaware <- paste(maas, c(1, 4, 5), sep="")
lackaware <- paste(maas, c(6:8), sep="")
lackpresfocus <- paste(maas, c(7:12, 14), sep="")
lacksomaware <- paste(maas, c(4,5), sep="")
lackattention <- paste(maas, c(2, 3, 14), sep="")
Maas5model2c <- mxModel(name="MAAS52c",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Lack of somatic awareness", to=bodyunaware),
                        mxPath(from="lack of present awareness", to=lackpresfocus),
                        mxPath (from="lack of attention", to=lackattention),
                        mxPath (from="lack of awareness", to=lackaware),
                        mxPath (from="lack of present focus", to=lackpresfocus),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2b)), type="cov", numObs=370)
                      )
maas5fit2c <- mxRun(Maas5model2c)
maas5summ2c <- summary(maas5fit2c)
@

<<maas2bsemcompare, echo=FALSE, results=tex>>=
maas2b.semcomp <- mxCompare(base=maas1fit2b, comparison=c( maas5fit2b))
print(xtable(maas2b.semcomp,label="tab:maas2bsemcompare", caption="Comparison of Factor Structures for MAAS 2B Solutions, Tested on Split C"), scalebox=0.9)
@

As can be seen from Table \ref{tab:maas2bsemcompare}, the one factor solution again performs best, further increasing our confidence in its adequacy.


<<maas2csemcompare, echo=FALSE, results=tex>>=
maas2c.semcomp <- mxCompare(base=maas1fit2c, comparison=c( maas5fit2c))
print(xtable(maas2c.semcomp,label="tab:maas2csemcompare", caption="Comparison of Factor Structures for MAAS 2C Solutions, Tested on Split B"), scalebox=0.9)
@

As can be seen from Table \ref{tab:maas2csemcompare}, the one factor model was again superior to the five factor model in Split C. 

\subsubsection{IRT Analyses}
\label{sec:irt-analyses}

Next, we examine the Mindful Attention Awareness Scale, using the same methodologies. 

First, the assumptions underlying item response theory modelling are checked. 

<<maas2bcheck, echo=FALSE, results=hide>>=
maas2b.iio <- check.iio(na.omit(maasitems2b))
maas2b.mono <- check.monotonicity(na.omit(maasitems2b))
@ 

<<maas2bitemord, echo=FALSE, results=tex>>=
print(xtable(maas2b.iio[["violations"]], caption="Item Ordering Assumption Check, MAAS Split B", label="tab:maas2bitemord"))
maas2b.s <- maasitems2b[,paste(maas, c(1:4, 7:15), sep="")]
@ 

As can be seen from Table \ref{tab:maas2bitemord}, questions 5 and 6 failed the item ordering assumptions for Split B and so are removed from the scale before further analysis. The reduced scale had no violations of the monotonicity assumption. 

<<maas2ccheck, echo=FALSE, results=hide>>=
maas2c.iio <- check.iio(na.omit(maasitems2c))
maas2c.mono <- check.monotonicity(na.omit(maasitems2c))
maas2c.s <- maasitems2c[,paste(maas, c(1:4, 7:10, 12:14), sep="")]
@ 

For Split C, items 5 and 11 fail the item ordering check, and so is removed. The reduced scale had no failures of the monotonicity assumption, so modelling continues with the reduced scale. 

Firstly, three partial credit models are fit to Split B.

<<maas2bpcm, echo=FALSE, results=hide>>=
maas2b.pcm.rasch <- gpcm(maas2b.s, constraint="rasch")
maas2b.pcm.1PL <- gpcm(maas2b.s, constraint="1PL")
maas2b.pcm.gpcm <- gpcm(maas2b.s, constraint="gpcm")
@ 

An ANOVA suggested that the two parameter model provided a better fit to the data ($p\le 0.001$). 

Firstly, three partial credit models are fit to Split C. 

<<maas2cgpcm, echo=FALSE, results=hide>>=
maas2c.gpcm.rasch <- gpcm(maas2c.s, constraint="rasch")
maas2c.gpcm.1PL <- gpcm(maas2c.s, constraint="1PL")
maas2c.gpcm.gpcm <- gpcm(maas2c.s, constraint="gpcm")
@ 

None of the partial credit models are analysed further, as they all had non-monotonically increasing parameter estimates. 

Next, one and two parameter graded response models were fit to Split B. 

<<maas2bgrm, echo=FALSE, results=hide>>=
maas2b.grm.1pl <- grm(maas2b.s, constrained=TRUE)
maas2b.grm.2pl <- grm(maas2b.s, constrained=FALSE)
@ 


<<maas2bgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas2b.grm.1pl), caption="Coefficient Estimates for MAAS One Parameter Graded Response Model, Split B", label="tab:maas2bgrm1pl"))
@ 


It can be seen from Table \ref{tab:maas2bgrm1pl} that there were no problems with the parameter estimates for this model. 

<<maas2bgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas2b.grm.2pl), caption="Coefficient Estimates for MAAS Two Parameter Graded Response Model, Split B", label="tab:maas2bgrm2pl"))
@ 

It can be seen from Table \ref{tab:maas2bgrm2pl} that there were no problems with the coefficient estimates for this solution, and the estimates show the usual tradeoff between discrimination and ability estimate parameters. 

Finally, we examine the performance of these models on unseen data.

<<maas2bgrmtest, echo=FALSE, results=hide>>=
maas.irt.notb <- maasitems.notb[,paste(maas, c(1:4, 7:15), sep="")]
maas2b.grm.1pl.test <- testIRTModels(maas2b.grm.1pl, maas.irt.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
maas2b.grm.2pl.test <- testIRTModels(maas2b.grm.2pl, maas.irt.notb, gpcmconstraint=NULL, grmconstraint=FALSE)
maas2b.grm.test.all <- rbind(maas2b.grm.1pl.test, maas2b.grm.2pl.test)
print(xtable(maas2b.grm.test.all, caption="Performance of One and Two Parameter Graded Response Models for MAAS on Unseen Data", label="tab:maas2bgrmtest"))
@ 


As can be seen from Table \ref{tab:maas2bgrmtest}, the one parameter model provided the best fit to the unseen data. 

Next, one and two parameter Graded Response Models are fit to Split C.

<<maas2cgrm, echo=FALSE, results=hide>>=
maas2c.grm.1pl <- grm(maas2c.s, constrained=TRUE)
maas2c.grm.2pl <- grm(maas2c.s, constrained=FALSE)
@ 

<<maas2cgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas2c.grm.1pl), caption="Coefficient Estimates for MAAS One Parameter Graded Response Model, Split C", label="tab:maas2cgrm1pl"))
@ 


As can be seen from Table \ref{tab:maas2cgrm1pl}, the coefficient estimates appear reasonable. The discrimination parameter is relatively low, suggesting that this scale is good for all levels of abilities, even though the highest estimated difficulty parameter is only 2.048, for Q13 which is ``I often find myself occupied with the future or the past'', which is a relatively concise summary of the entire construct of mindfulness. 

Next, a two parameter model is fit to this data. 

<<maas2cgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas2c.grm.2pl), caption="Coefficient Estimates for MAAS, Two Parameter Graded Response Model, Split C", label="tab:maas2cgrm2pl"))
@ 

The two parameter model (shown in Table \ref{tab:maas2cgrm2pl}), is not that much different from the one parameter model. Of interest is that Q13 remains the most difficult question, but its discrimination parameter has come down, suggesting that it behaves similarly for participants of all ability levels. Q8 has the highest discrimination parameter of all the items and is ``I rush through activities without being really attentive to them'' and it appears that this question is the best at discrimination between those higher and lower on the construct of mindfulness. 


The final step in the analysis of the MAAS scale is to test the performance of the models on unseen  data. 

<<maas2cgrmtest, echo=FALSE, results=hide>>=
maas.irt.notc <- maasitems.notc[,paste(maas, c(1:4, 7:10, 12:14), sep="")]
maas2c.grm.1pl.test <- testIRTModels(maas2c.grm.1pl, maas.irt.notc, gpcmconstraint=NULL, grmconstraint=TRUE)
maas2c.grm.2pl.test <- testIRTModels(maas2c.grm.2pl, maas.irt.notc, gpcmconstraint=NULL, grmconstraint=FALSE)
maas2c.grm.test.all <- rbind(maas2c.grm.1pl.test, maas2c.grm.2pl.test)
print(xtable(maas2c.grm.test.all, caption="Performance of MAAS One and Two Parameter Graded Response Models on Unseen Data (Splits A and B)", label="tab:maas2cgrmtest"))
@ 

As can be seen from Table \ref{tab:maas2cgrmtest}, the one parameter model provided a better fit to the unseen data (though neither model was particularly good). 

%INSERT backtest on Split A here. 

\subsection{LOTR}
\label{sec:lotr-1}



<<lotr2bparallel, echo=FALSE, results=hide>>=
sink("tmp.txt")
lotr2b.parallel <- fa.parallel(na.omit(lotritems2b))
lotr2b.vss <- VSS(na.omit(lotritems2b))
sink(NULL)
@

<<lotr2cparallel, echo=FALSE, results=hide>>=
sink("tmp.txt")
lotr2c.parallel <- fa.parallel(na.omit(lotritems2c))
lotr2c.vss <- VSS(na.omit(lotritems2c))
sink(NULL)
@
<<lotrparallelplot2b, echo=FALSE, fig=TRUE>>=
sink("tmp.txt")
par(mfrow=c(1,2))
print(lotr2b.parallel)
print(lotr2b.vss)
sink(NULL)
@ 

Again, the parallel analysis criterion suggests two factors, while the MAP criterion suggests one, so both solutions will be examined and interpreted for both splits, as the results were broadly similar. 

<<lotr2bfact1, echo=FALSE, results=tex>>=
lotr2b.fact1 <- fa(lotritems2b, 1, rotate="oblimin", fm="pa")
print(FactorXtab(lotr2b.fact1,label="tab:hom2blotr1", caption="One Factor Solution, LOT-R, Sample Two, Split B"))
@


<<lotr2cfact1, echo=FALSE, results=tex>>=
lotr2c.fact1 <- fa(lotritems2c, 1, rotate="oblimin", fm="pa")
print(FactorXtab(lotr2c.fact1,label="tab:hom2clotr1", caption="One Factor Solution, LOT-R, Sample Two, Split C"))
@

Tables \ref{tab:hom2blotr1} and \ref{tab:hom2clotr1} show the estimated coefficients for the one factor solutions in both splits. The two solutions are somewhat different. The communalities for Q1 are much higher in Split C than in Split B, while those for Q4 are much lower in Split C than in Split B. To some extent this probably represents sampling error, but it is quite strange that the two negatively worded items should show much of the variance across samples. 


<<lotr2bfact2, echo=FALSE, results=tex>>=
lotr2b.fact2 <- fa(lotritems2b, 2, rotate="oblimin", fm="pa")
print(FactorXtab(lotr2b.fact2,label="tab:hom2blotr2"))
@

<<lotr2cfact2, echo=FALSE, results=tex>>=
lotr2c.fact2 <- fa(lotritems2c, 2, rotate="oblimin", fm="pa")
print(FactorXtab(lotr2c.fact2,label="tab:hom2clotr2"))
@

Tables \ref{tab:hom2blotr2} and \ref{tab:hom2clotr3} show some differences. In general, though the major outline of the structure is the same, Split B's solution is much cleaner than that of Split C. For example, in Split C it is unclear whether or not Q10 belongs to the first or second factor. Therefore, it would seem advisable to prefer the structure from Split B, but this will be tested. 

<<lotr1sem2b, echo=FALSE, results=hide>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism")
optimism <- paste(lotr, c(1,3,4,7,9,10), sep="")
Lotr1model2b <- mxModel(name="LOTR1b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2c)), type="cov", numObs=370)
                      )
lotr1fit2b <- mxRun(Lotr1model2b)
lotr1summ2b <- summary(lotr1fit2b)
@


<<lotr1sem2c, echo=FALSE, results=hide>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism")
optimism <- paste(lotr, c(1,3,4,7,9,10), sep="")
Lotr1model2c <- mxModel(name="LOTR1c",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2b)), type="cov", numObs=370)
                      )
lotr1fit2c <- mxRun(Lotr1model2c)
lotr1summ2c <- summary(lotr1fit2c)
@

<<lotr2sem2b, echo=FALSE, results=hide>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism", "Pessimism")
optimism <- paste(lotr, c(1,4,10), sep="")
pessimism <- paste(lotr, c(3,7,9), sep="")
Lotr2model2b <- mxModel(name="LOTR2b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from="Pessimism", to=pessimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2c)), type="cov", numObs=370)
                      )
lotr2fit2b <- mxRun(Lotr2model2b)
lotr2summ2b <- summary(lotr2fit2b)
@

<<lotr2sem2c, echo=FALSE, results=hide>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism", "Pessimism")
optimism <- paste(lotr, c(1,4,10), sep="")
pessimism <- paste(lotr, c(3,7,9), sep="")
Lotr2model2c <- mxModel(name="LOTR2c",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from="Pessimism", to=pessimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2b)), type="cov", numObs=370)
                      )
lotr2fit2c <- mxRun(Lotr2model2c)
lotr2summ2c <- summary(lotr2fit2c)
@

<<lotrcompare2b, echo=FALSE, results=hide>>=
lotrcomp2b <- mxCompare(base=lotr1fit2b, comparison=lotr2fit2b)
lotrcomp.xtab2b <- xtable(lotrcomp2b,label="tab:tcq2blotrcomp", caption="Comparison of LOT-R Split B Factor Solutions, Tested on Split C")
print(lotrcomp.xtab2b)
@


<<lotrcompare2c, echo=FALSE, results=hide>>=
lotrcomp2c <- mxCompare(base=lotr1fit2c, comparison=lotr2fit2c)
lotrcomp.xtab2c <- xtable(lotrcomp2c,label="tab:tcq2clotrcomp", caption="Comparison of LOT-R Split C Factor Solutions, Tested on Split B")
print(lotrcomp.xtab2c)
@ 

\subsubsection{IRT Analyses}
\label{sec:irt-analyses-1}

Next, we examine the LOT-R using IRT approaches. 

<<lotr2bcheck, echo=FALSE, results=hide>>=
lotr2b.iio <- check.iio(na.omit(lotritems2b))
lotr2b.mono <- check.monotonicity(na.omit(lotritems2b))
@ 

The LOT-R for Split B had no problems with either item ordering or monotonicity. 

Firstly, three partial credit models were fit to the data. 

<<lotr2bpcm, echo=FALSE, results=hide>>=
lotr2b.pcm.rasch <- gpcm(lotritems2b, constraint="rasch")
lotr2b.pcm.1PL <- gpcm(lotritems2b, constraint="1PL")
lotr2b.pcm.gpcm <- gpcm(lotritems2b, constraint="gpcm")
@ 

All of the partial credit models had non-monotonically increasing parameter estimates and so are not reported further here. 

Next, one and two parameter Graded Response Models were fit to the data. 

<<lotr2bgrm, echo=FALSE, results=hide>>=
lotr2b.grm.1pl <- grm(lotritems2b, constrained=TRUE)
lotr2b.grm.2pl <- grm(lotritems2b, constrained=FALSE)
@ 


<<lotr2bgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(lotr2b.grm.1pl), caption="Coefficient Esitmates for One Parameter Graded Response Model, LOT-R, Split B", label="tab:lotr2bgrm1pl"))
@ 

Table \ref{tab:lotr2bgrm1pl} clearly shows that there were no obvious problems with this model. The parameter estimates are relatively low in comparison with other scales, suggesting that these items were easier to endorse. 

<<lotr2bgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef(lotr2b.grm.2pl), caption="Coefficient Estimates for Two Parameter Graded Response Model, LOT-R, Split B", label="tab:lotr2bgrm2pl"))
@ 


It can be seen from Table \ref{tab:lotr2bgrm2pl} that the ability estimates have risen while the discrimination parameters have fallen for the majority of the items (except for 7). 


Finally, we assess the performance of each of these models on unseen data. 

<<lotr2bgrmtest, echo=FALSE, results=tex>>=
lotr2b.grm.1pl.test <- testIRTModels(lotr2b.grm.1pl, lotritems.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
lotr2b.grm.2pl.test <- testIRTModels(lotr2b.grm.2pl, lotritems.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
lotr2b.grm.test.all <- rbind(lotr2b.grm.1pl.test, lotr2b.grm.2pl.test)
print(xtable(lotr2b.grm.test.all, caption="Performance of One and Two Parameter Graded Response Models on Unseen Data (Splits A and C)", label="tab:lotr2bgrmtest"))
@ 

As can be seen from Table \ref{tab:lotr2bgrmtest}, the one parameter model performed best on the unseen data. 

<<lotr2caisp, echo=FALSE, results=hide>>=
lotr2c.aisp <- aisp(na.omit(lotritems2c))
@ 

<<lotr2ccheck, echo=FALSE, results=hide>>=
lotr2c.iio <- check.iio(na.omit(lotritems2c))
lotr2c.mono <- check.monotonicity(na.omit(lotritems2c))
lotr2c.s <- lotritems2c[,paste(lotr, c(3,4,7,9,10), sep="")]
@ 

An examination of the item ordering assumption showed that Q1 did not fit this model, and so was removed from the scale. There were no failures of the monotonicity assumption, and thus the modelling could commence. 

<<lotr2cpcm, echo=FALSE, results=hide>>=
lotr2c.pcm.rasch <- gpcm(lotr2c.s, constraint="rasch")
lotr2c.pcm.1PL <- gpcm(lotr2c.s, constraint="1PL")
lotr2c.pcm.gpcm <- gpcm(lotr2c.s, constraint="gpcm")
@ 

All Partial Credit Models suffered from non-monotonically increasing parameter estimates, and so are not analysed further here. 

Next, one and two parameter Graded Response Models were fit to the scale.

<<lotr2cgrm, echo=FALSE, results=hide>>=
lotr2c.grm.1pl <- grm(lotr2c.s, constrained=TRUE)
lotr2c.grm.2pl <- grm(lotr2c.s, constrained=FALSE)
@ 

<<lotr2cgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(lotr2c.grm.1pl), caption="Coefficient Esitmates for LOT-R One Parameter Graded Response Model, Split C", label="tab:lotr2cgrm1pl"))
@ 

Table \ref{tab:lotr2cgrm1pl} shows the estimated difficulty parameters for the one parameter Graded Response Model. It can be seen that the discrimination parameter is quite high, and that the most difficult question is Q10 which is ``Overall, I expect more good things to happen to me than bad''. The ``easiest'' question is Q7, which is one of the negative.ly fphrased question, suggesting that the two of these questions would be enough to garner a rough estimate of ability from participants. 


<<lotr2cgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef(lotr2c.grm.2pl), caption="Coefficient Estimates for LOT-R, Two Parameter Graded Response Model", label="tab:lotr2cgrm2pl"))
@ 

Table \ref{tab:lotr2cgrm2pl} shows the estimates for the two parameter GRM. It can be seen that Q7 is  the most discriminating question, while still having the lowest ability estimates, suggesting that it is a very good question for seperating out optimism and pessimism. Q10 is still the most difficult, but not as discriminating as Q1 and Q10 (which refer to hopes around the future and indeed are very similar questions). 

Finally, the performance of these two models is tested against unseen data. 

<<lotr2cgrmtest, echo=FALSE, results=tex>>=
lotr.irt.notc <- lotritems.notc[,paste(lotr, c(3,4,7,9,10), sep="")]
lotr2c.grm.1pl.test <- testIRTModels(lotr2c.grm.1pl, lotr.irt.notc, gpcmconstraint=NULL, grmconstraint=TRUE)
lotr2c.grm.2pl.test <- testIRTModels(lotr2c.grm.2pl, lotr.irt.notc, gpcmconstraint=NULL, grmconstraint=FALSE)
lotr2c.grm.test.all <- rbind(lotr2c.grm.1pl.test, lotr2c.grm.2pl.test)
print(xtable(lotr2c.grm.test.all, caption="Performance of LOT-R Split C One and Two Parameter Graded Response Models on Unseen Data", label="tab:lotr2cgrmtest"))
@ 


As can be seen from Table \ref{tab:lotr2cgrmtest}, the one parameter model provides the best fit to the unseen data. 


\section{RAND MOS}
\label{sec:rand-mos}

The final step in the analysis was to model  the RAND MOS using the approaches outlined above. 

The first step was to examine the breakdown of items within scales. 

<<rand2aaiasp, echo=FALSE, results=tex>>=
rand2a.aisp <- aisp(na.omit(randitems2a))
rand2a.aisp <- as.data.frame(rand2a.aisp)
print(xtable(rand2a.aisp, caption="Item Selection Procedure Results for RAND MOS, Split A", label="tab:rand2aaisp"))
@ 

Table \ref{tab:rand2aaisp} shows the breakdown of items and their assignment to particular scales. 
The first scale consists of items 3 to 12 and is the physical functioning scale and so retains that name. 
The second scale consists of the role limitations scales and the negatively worded questions from the emotional well being and energy/faatigue scales, and can probably be best termed as negative health. 

The third scale consists of the general health, pain and positively worded questions for emotional well being and energy/fatigue scales, and can probably best be termed as overall health. 

Two items (2 and 24) do not load on any scale, while 33 and 35 load on a separate scale. This scale (4 in the table) was not analysed, as two items is not enough to make a useful scale. 

<<randcheckassumptions, echo=FALSE, results=hide>>=
rand.scales <- aisp(na.omit(randitems2b))
@

<<rand2caisp, echo=FALSE, results=tex>>=
rand2c.aisp <- aisp(na.omit(randitems2c))
print(xtable(rand2c.aisp, caption="Item Selection Procedure for RAND MOS, Split C", label="tab:rand2caisp"))
@ 



<<rand2ascales, echo=FALSE, results=hide>>=
rand2a.physfun <- randitems2a[,paste(rand, 3:12, sep="")]
rand2a.neghealth <- randitems2a[,paste(rand, c(13:19, 25,28, 29,31,32), sep="")]
rand2a.ovhealth <- randitems2a[,paste(rand, c(1, 22,23, 26,27,34,36), sep="")]
@ 

Firstly, the item ordering and monotonicity assumptions were checked for the physical functioning scale. 



<<rand2aphysfuncheck, echo=FALSE, results=hide>>=
physfun2a.iio <- check.iio(na.omit(rand2a.physfun))
physfun2a.mono <- check.monotonicity(na.omit(rand2a.physfun))
@ 

There were no violations of either the item ordering or monotonicity assumptions for the physical functioning scale in this split, so IRT modelling can proceed.

The first step was to fit three partial credit models to this scale.

<<physfun2apcm, echo=FALSE,results=hide >>=
physfun2a.pcm.rasch <- gpcm(rand2a.physfun, constraint="rasch")
physfun2a.pcm.1PL <- gpcm(rand2a.physfun, constraint="1PL")
physfun2a.pcm.gpcm <- gpcm(rand2a.physfun, constraint="gpcm")
@ 

All three partial credit models had non increasing parameter estimates, and thus are not considered further here. 

Next, one and two parameter Graded Response Models were fit to this scale. 

<<physfun2agrm, echo=FALSE, results=hide>>=
physfun2a.grm.1pl <- grm(rand2a.physfun, constrained=TRUE)
physfun2a.grm.2pl <- grm(rand2a.physfun, constrained=FALSE)
@ 

<<physfun2agrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(physfun2a.grm.1pl)), caption="Coefficient Estimates for One Parameter Graded Response Model Physical Functioning Scale, Split A", label="tab:rand2aphysfungrm1pl"))
@ 

Table \ref{tab:rand2aphysfungrm1pl} shows the coefficient estimates for this scale. It can be seen that the discrimination parameter is quite high, while the ability estimates are quite low. This is presumably because many of these items are intended more for a clinical sample than the non-clincial sample used in this research. 

<<physfun2agrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(physfun2a.grm.2pl)), caption="Coefficient Estimates for Two Parameter Graded Response Model for Physical Functioning Scale, Split A", label="tab:physfun2agrm2pl"))
@ 

Table \ref{tab:physfun2agrm2pl} shows the coefficient estimates for the two parameter model. It can  be seen that while the ability estimates have remained quite low, the discrimination parameters have altered significantly. Q7 and 11 have the two highest discrimination parameters, which makes sense as they ask respectively regarding difficulties in climbing one flight of stairs or walking one block. Again, the ability estimates for each threshold are quite low. 

The next step is to examine the performance of each of these models on unseen data. 

<<rand2agrmtest, echo=FALSE, results=tex>>=
physfun.nota <- randitems.nota[,paste(rand, 3:12, sep="")]
physfun2a.grm.1pl.test <- testIRTModels(physfun2a.grm.1pl, physfun.nota, gpcmconstraint=NULL, grmconstraint=TRUE)
physfun2a.grm.2pl.test <- testIRTModels(physfun2a.grm.2pl, physfun.nota, gpcmconstraint=NULL, grmconstraint=FALSE)
physfun2a.grm.all <- rbind(physfun2a.grm.1pl.test, physfun2a.grm.2pl.test)
print(xtable(physfun2a.grm.all, caption="Performance of Physical Functioning One and Two Parameter Graded Response Models (Split A) on unseen data", label="tab:physfun2agrmtest"))
@ 


As can be seen from Table \ref{tab:physfun2agrmtest}, the one paramter model provided the best fit to the unseen data. 


Next, the negative health scale is checked for violations of the item ordering and monotonicity assumptions. 

<<neghealth2acheck, echo=FALSE, results=hide>>=
neghealth2a.iio <- check.iio(na.omit(rand2a.neghealth))
neghealth2a.mono <- check.monotonicity(na.omit(rand2a.neghealth))
@

There were no violations of either the item ordering assumption or the monotonicity assumption for this scale. 

The next step was to fit three partial credit models to this scale. 

<<neghealth2apcm, echo=FALSE, results=hide>>=
neghealth2a.pcm.rasch <- gpcm(rand2a.neghealth, constraint="rasch")
neghealth2a.pcm.1PL <- gpcm(rand2a.neghealth, constraint="1PL")
neghealth2a.pcm.gpcm <- gpcm(rand2a.neghealth, constraint="gpcm")
@ 

<<neghealth2apcmraschprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(neghealth2a.pcm.rasch)), caption="Coefficient Estimates for Rasch Partial Credit Model, Negative Health Scale, Split A", label="tab:neghealth2apcmrasch"))
@ 

Table \ref{tab:neghealth2apcmrasch} shows the coefficient estimates for the Rasch partial credit model. It can be seen that there is an extremely wide spread of abillity estimates, the physical limitations items tend to have quite low thresholds, while the items drawn from the emotional well being and energy/fatigue scales have a much broader spread. For instance, item 32 has a top ability threshold of 6.11, which is extremely high. The item asks about health problems interfering with the social life of the respondent, and given that the sample was predominantly young and students, may be the reason for the low probability of endorsement of the items (thus leading to the high estimated ability threshold). 

<<neghealth2apcm1pl, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(neghealth2a.pcm.1PL)), caption="Coefficient Estimates for Negative health Scale, One Parameter Partial Credit Model, Split A", label="tab:neghealth2apcm1pl"))
@ 

Table \ref{tab:neghealth2apcm1pl} shows the coefficient estimates for the one parameter partial credit model. The overall estimated discrimination parameter is quite low, while the same pattern of ability esitmates as the rasch model remains. 

<<neghealth2apcmgpcm, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(neghealth2a.pcm.1PL)), caption="Coefficient Estimates for Negative Health Scale, Two Parameter Partial Credit Model, Split A", label="tab:neghealth2apcm2pl"))
@ 

Table \ref{tab:neghealth2apcm2pl} shows the estimated thresholds and discrimination parameters for the two parameter partial credit model. Interestingly enough, the estimated discrimination parameters have actually decreased for the majority of the scale, which is unexpected. The ordering of the ability estimates remains the same as in previous models. 

Next, the performance of these three partial credit models was assessed on unseen data. 

<<neghealth2apcmtest, echo=FALSE, results=tex>>=
neghealth.nota <- randitems.nota[,paste(rand, c(13:19, 25,28, 29,31,32), sep="")]
neghealth2a.pcm.rasch.test <- testIRTModels(neghealth2a.pcm.rasch, neghealth.nota, gpcmconstraint="rasch", grmconstraint=NULL)
neghealth2a.pcm.1PL.test <- testIRTModels(neghealth2a.pcm.1PL, neghealth.nota, gpcmconstraint="1PL", grmconstraint=NULL)
neghealth2a.pcm.gpcm.test <- testIRTModels(neghealth2a.pcm.gpcm, neghealth.nota, gpcmconstraint="gpcm", grmconstraint=NULL)
neghealth2a.pcm.test.all <- rbind(neghealth2a.pcm.rasch.test, neghealth2a.pcm.1PL.test, neghealth2a.pcm.gpcm.test)
print(xtable(neghealth2a.pcm.test.all, caption="Performance of Negative Health Partial Credit Models (Split A) on unseen data (Splits B and C)", label="tab:neghealth2apcmtest"))
@ 

Table \ref{tab:neghealth2apcmtest} shows that the one parameter model provided a better fit to the unseen data. 

Next, one and two parameter Graded Response Models were fit to the negative health scale. 

<<neghealth2agrm, echo=FALSE, results=hide>>=
neghealth2a.grm.1pl <- grm(rand2a.neghealth, constrained=TRUE)
neghealth2a.grm.2pl <- grm(rand2a.neghealth, constrained=FALSE)
@ 

<<neghealth2agrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(neghealth2a.grm.1pl)), caption="Coefficient Estimates for Negative Health One Parameter Graded Response Model, Split A"))
@ 

\subsubsection{Split B}
\label{sec:split-b-1}






<<randscales, echo=FALSE, results=tex>>=
print(xtable(rand.scales,caption="Automatic Item Selection Procedure, RAND MOS, Split B",label="tab:randscales2b"))
@
As can be seen from Table \ref{tab:randscales}, the item selection procedure suggests that there are four scales in the RAND MOS.  In addition, one item does not load on any scale.  This is RANDQ2, which is not surprising given that it is not supposed to score on any scale.  The analysis of the RAND scales continues now, but using the four scales suggested by the item selection procedure.

The scales can be termed as follows:

Scale 1: RANDQ3-RANDQ12: This maps exactly to the physical functioning scale.

Scale 2: RANDQ17-RANDQ19,24-25,28-32,35. This scale incorporates the Role Limitations, Social Functioning, Emotional Well Being and Energy scales, and can best be termed as Negative Health.

Scale 3: RANDQ1, RANDQ20, RANDQ23, RANDQ26, RANDQ27, RANDQ30, RANDQ34, RANDQ36: This scale maps to the general health and positively worded questions from the energy/fatigue and emotional health scales, and so can best be termed Positive Health.  

Scale 4: RAND21-RAND22. This maps to the  pain scale, and can probably best be termed as pain. This scale will not be analysed further as two items is not enough to model the response structure properly. 


<<randirtscales, echo=FALSE, results=hide>>=
irtphysfun2b <- randitems2b[,paste(rand, c(3:12), sep="")]
irtneghealth2b <- randitems2b[,paste(rand, c(17:19, 24:25,28,29,31,35 ), sep="")]
irtposhealth2b <- randitems2b[,paste(rand, c(1, 20, 23, 26,27,30, 34,36), sep="")]
@

<<randphysfun, echo=FALSE, results=hide>>=
irtphys2b.item.ord <- check.iio(na.omit(irtphysfun2b))
irtphys2b.monotonicity <- check.monotonicity(na.omit(irtphysfun2b))
@

The check for invariant item ordering shows that all items  meet the Invariant Item Ordering  assumption. The check on the monotonicity assumption shows that all of the items meet this assumption. 


<<randphysfuncitemord, echo=FALSE, results=tex>>=
print(xtable(irtphys2b.item.ord[["violations"]],label="tab:physfunc2bitemord", caption="Item Ordering Check for RAND Physical Functioning Scale, Split B"))
@

As can be seen from Table \ref{tab:physfunc2bitemord}, there were no violations of item ordering for this sample.  This demonstrates that there are no violations of monotonicity for this subscale, and suggests that item response theory modelling can proceed, for this scale at least.

<<randneghealth, echo=FALSE, results=hide>>=
irtneghealth2b.item.ord <- check.iio(na.omit(irtneghealth2b))
irtemhealth2b.monotonicity <- check.monotonicity(na.omit(irtneghealth2b))
@

<<randneghealthItemord, echo=FALSE, results=tex>>=
print(xtable(irtneghealth2b.item.ord[["violations"]],label="tab:randneghealthitemord", caption="Item Ordering Assumption Check for RAND Negative Health Scale, Split B"))
@

As can be seen from Table \ref{tab:randneghealthitemord},quite a number of items failed the item ordering assumption (RAND29, RAND35) and so were removed from the scale before further analysis. 

<<irtneghealth, echo=FALSE, results=tex>>=
irtneghealth2b.s <- irtneghealth2b[,paste(rand, c(25,28,24,29,31,17,19,18), sep="")]
neghealth.item.ord2 <- check.iio(na.omit(irtneghealth2b.s))
neghealth.mono.2 <- check.monotonicity(na.omit(irtneghealth2b.s))
print(xtable(neghealth.item.ord2[["violations"]], caption="IIO for reduced negative health scale", label="tab:randneghealth2biio"))
@

As shown in Table \ref{tab:randneghealth2biio}, the removal of items from the emotional health scale appears to have removed the problems with invariant item ordering. The reduced scale had no issues with non-monotonic item responses. 



<<randposhealth, echo=FALSE, results=hide>>=
irtposhealth2b.item.ord <- check.iio(na.omit(irtposhealth2b))
irtposhealth2b.monotonicity <- check.monotonicity(na.omit(irtposhealth2b))
@

<<poshealthitemord2b, echo=FALSE, results=tex>>=
print(xtable(irtposhealth2b.item.ord[["violations"]], caption="Item Ordering Checks for RAND MOS Positive Health Scale, Split B", label="tab:rand2bposhealthitemord"))
@ 
As can be seen from Table \ref{tab:rand2bposhealthitemord}, all items met the item ordering assumption. 
Additionally, the smaller scale showed no violations of the monotonicity assumption. 



Having checked the assumptions for all three scales, the next step was to fit partial credit and graded response models to each of the scales, and to examine their fit on unseen data. 


<<physfunrasch, echo=FALSE, results=hide>>=
physfun2b.gpcm.rasch <- gpcm(na.exclude(irtphysfun2b), constraint="rasch")
physfun2b.gpcm.1pl <- gpcm(na.exclude(irtphysfun2b), constraint="1PL")
physfun2b.gpcm.gpcm <- gpcm(na.exclude(irtphysfun2b), constraint="gpcm")
@

Three partial credit models were fit (rasch, one parameter and two parameter). However, all of them had non-monotonically increasing ability estimates, and so are not reported further here. 

The next step in the modelling process was to fit a one and two parameter GRM. 

<<physfun2bgrm, echo=FALSE, results=hide>>=
physfun2b.grm.1pl <- grm(na.omit(irtphysfun2b), constrained=TRUE)
physfun2b.grm.2pl <- grm(na.omit(irtphysfun2b), constrained=FALSE)
@ 

<<physfun2bgrm1plprint, echo=FALSE, results=hide>>=
print(xtable(coef(physfun2b.grm.1pl), caption="One Parameter Graded Response Model for Physical Functioning IRT Scale, Split B", label="tab:physfun2bgrm1pl"))
@ 

As can be seen from Table \ref{tab:physfun2bgrm1pl}, there were no violations of monotonicity for the one parameter GRM. Note that the discrimination parameter is extremely high, which is not [particularly surprising given the use of a non-clinical sample. 

<<physfun2bgrm2plprint, echo=FALSE, results=hide>>=
print(xtable(coef(physfun2b.grm.2pl), caption="Two Parameter Graded Response Model for Physical Functioning IRT Scale, Split B", label="tab:physfun2bgrm2pl"))
@ 

It can be seen from Table \ref{tab:physfun2bgrm2pl} that the average ability estimates have risen while discrimination parameters have dropped for most of the items. The next step is to examine the performance of each of these models on unseen data (i.e. that from Splits A and C). 

<<physfun2bgrmtest, echo=FALSE, results=tex>>=
physfun.notb <- randitems.notb[,paste(rand, c(3:12), sep="")]
physfun2b.grm.1pl.test <- testIRTModels(physfun2b.grm.1pl, physfun.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
physfun2b.grm.2pl.test <- testIRTModels(physfun2b.grm.2pl, physfun.notb, gpcmconstraint=NULL, grmconstraint=FALSE)
physfun2b.grm.test.all <- rbind(physfun2b.grm.1pl.test, physfun2b.grm.2pl.test)
print(xtable(physfun2b.grm.test.all, caption="Performance of One and Two Parameter GRM's on Unseen Data (Splits A and C)", label="tab:physfun2btest"))
@ 

As can be seen from Table \ref{tab:physfun2btest}, the one parameter model provided a better fit to the unseen data. 

The next sub-scale to be examined is Negative Health, described above.  This process begins, as above, with the fitting of a simple one parameter model, in this case, the partial credit model.

<<neghealthpcm, echo=FALSE, results=hide>>=
neghealth2b.pcm.rasch <- gpcm(irtneghealth2b, constraint="rasch")
neghealth2b.pcm.1PL <- gpcm(irtneghealth2b, constraint="1PL")
neghealth2b.pcm.gpcm <- gpcm(irtneghealth2b, constraint="gpcm")
@ 

Three partial credit models were fitted, but all had non-monotonically increasing parameter estimates, and so are not reported further here. 

Next, one and two parameter Graded Response Models were fitted. 

<<neghealthgrm, echo=FALSE, results=hide>>=
neghealth2b.grm.1pl <- grm(irtneghealth2b, constrained=TRUE)
neghealth2b.grm.2pl <- grm(irtneghealth2b, constrained=FALSE)
@ 

<<negghealth2bgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(neghealth2b.grm.1pl)), caption="Coefficients for One Parameter Graded Response Model Negative Health Scale, Split B", label="tab:neghealth2bgrm1pl"))
@ 


As shown in Table \ref{tab:neghealth2bgrm1pl}, there were no problemw ith non-monotonically increasing parameter estimates for this model. Of interests is the relatively low discrimination parameter, which suggests that these items were more applicable to the sample. Also of interest is that the emotional role limitations questions (17,18,19) did not have any responses that were not no, suggesting that this scale was not particularly useful for this sample. 

<<neghealth2bgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(neghealth2b.grm.2pl)), caption="Coefficients for Negative Health Scale, Two Parameter Graded Response Model, Split B", label="tab:neghealth2bgrm2pl"))
@ 


As can be seen from Table \ref{tab:neghealth2bgrm2pl}, the discrimination parameter has lowered (in line with previous models) while the estimated abilities have widened. Note that question 25 and 28 appear to have been the source of the high discrimination parameter in the one parameter model. 

Next, the fit of each of these models on unseen data was examined. 

<<neghealth2bgrmtest, echo=FALSE, results=tex>>=
neghealth.notb <- randitems.notb[,paste(rand, c(17:19, 24:25,28,29,31,35 ), sep="")] 
neghealth2b.grm.1pl.test <- testIRTModels(neghealth2b.grm.1pl, neghealth.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
neghealth2b.grm.2pl.test <- testIRTModels(neghealth2b.grm.2pl, neghealth.notb, gpcmconstraint=NULL, grmconstraint=FALSE)
neghealth2b.grm.test.all <- rbind(neghealth2b.grm.1pl.test, neghealth2b.grm.2pl.test)
print(xtable(neghealth2b.grm.test.all, caption="Performance of Negative health Split B Graded Response Models on Unseen Data", label="tab:neghealth2bgrmtest"))
@ 

As can be seen from Table \ref{tab:neghealth2bgrmtest}, the one parameter model performed slightly better on the unseen data, though there is not much in the difference. 

Finally, for this split, the positive health scale was examined using partial credirt and graded response models. 

<<irtposhealthpcm, echo=FALSE, results=hide>>=
irtposhealth2b.pcm.rasch <- gpcm(irtposhealth2b, constraint="rasch")
irtposhealth2b.pcm.1PL <- gpcm(irtposhealth2b, constraint="1PL")
irtposhealth2b.pcm.gpcm <- gpcm(irtposhealth2b, constraint="gpcm")
@ 

Three partial credit models were fit to the data, but all had problems with the parameter estimates and are not reported further here. 

<<poshealth2bgrm, echo=FALSE, results=hide>>=
poshealth2b.grm.1pl <- grm(irtposhealth2b, constrained=TRUE)
poshealth2b.grm.2pl <- grm(irtposhealth2b, constrained=FALSE)
@ 

<<poshealth2bgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(poshealth2b.grm.1pl)), caption="Coefficients for One Parameter Graded Response Model on Positive Health Scale, Split B", label="tab:poshealth2bgrm1pl"))
@ 


As can be seen from Table \ref{tab:poshealth2bgrm1pl}, there were no problems with the parameter estimates for this model. The questions marked as most difficult (1, 30 and 23) relate to emotional well being and the general health scale. The discrimination parameter is relatively low in comparison to other scales, again probably due to the non-clinical sample used in this study. 

<<poshealth2bgrm2plprint, echo=FALSE, results=hide>>=
print(xtable(coef2mat(coef(poshealth2b.grm.2pl)), caption="Coefficients for Two Parameter Graded Response Model on Positive Health Scale, Split B", label="tab:poshealth2bgrm2pl"))
@ 

Again, there were no problems with parameter estimates for this scale (Table \ref{tab:poshealth2bgrm2pl}). For the majority of items the ability estimates have increased while the discrimination parameter(s) have lowered. 

Finally, we assess the fit of each of these models on unseen data. 

<<poshealth2bgrmtest, echo=FALSE, results=tex>>=
poshealth.notb <- randitems.notb[,paste(rand, c(1, 20, 23, 26,27,30, 34,36), sep="")]
poshealth2b.grm.1pl.test <- testIRTModels(poshealth2b.grm.1pl, poshealth.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
poshealth2b.grm.2pl.test <- testIRTModels(poshealth2b.grm.2pl, poshealth.notb, gpcmconstraint=NULL, grmconstraint=FALSE)
poshealth2b.grm.test.all <- rbind(poshealth2b.grm.1pl.test, poshealth2b.grm.2pl.test)
print(xtable(poshealth2b.grm.test.all, caption="Performance of One and Two Parameter Graded Response Models on Unseen Data (Splits A and C)", label="tab:poshealth2btest"))
@ 

As can be seen from Table \ref{tab:poshealth2btest}, the one parameter model provided a better fit to the unseen data. 




\subsubsection{Split C}
\label{sec:split-c}

Firstly, the RAND MOS is examined to determine how many scales it consists of. 



As can be seen in Table \ref{tab:rand2caisp} again the RAND MOS divides into three scales. 

The first scale consists of items 3 through 13, and can be best termed as physical functioning (although it contains one item from the physical role limitations subscale). 

The second scale consists of most of the physical and role limitations scales and the negatively worded items from the emotional well being and social functioning scales and can again, as in the previous split, be termed as negative health. 

The third scale consists of the rest of the items, that is the general health, social functioning and emotional well being and energy/fatigue items which are positively worded, and can best be termed as positive health. 

<<rand2cscales, echo=FALSE, results=hide>>=
physfun2c <- randitems2c[,paste(rand, c(3:13), sep="")]
poshealth2c <- randitems2c[,paste(rand, c(1,20,22:23, 26:27, 30, 34, 36), sep="")]
neghealth2c <- randitems2c[,paste(rand, c(14:19, 24:25, 28:29, 31), sep="")]
@ 

Firstly, the assumptions underlying IRT modelling must be assessed before modelling can commence. 

<<physfun2ccheck, echo=FALSE, results=hide>>=
physfun2c.iio <- check.iio(na.omit(physfun2c))
physfun2c.mono <- check.monotonicity(na.omit(physfun2c))
@ 

There were no violations of either the item ordering assumption or the monotonicity assumptions for the physical functioning scale in this split. 

The preliminaries having been dealt with, the next step was to fit three partial credit models. 

<<physfun2cgpcm, echo=FALSE, results=hide>>=
physfun2c.gpcm.rasch <- gpcm(physfun2c, constraint="rasch")
physfun2c.gpcm.1pl <- gpcm(physfun2c, constraint="1PL")
physfun2c.gpcm.gpcm <- gpcm(physfun2c, constraint="gpcm")
@ 

All three partial credit models had problems with the ability estimates in that they were not montonotically increasing, and so are not discussed further here. 

Next, one and two parameter Graded Response Models were fit to the data.


<<physfun2cgrm, echo=FALSE, results=hide>>=
physfun2c.grm.1pl <- grm(physfun2c, constrained=TRUE)
physfun2c.grm.2pl <- grm(physfun2c, constrained=FALSE)
@ 

<<physfun2cgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(physfun2c.grm.1pl)), caption="Coefficient Estimates for One Parameter Graded Response Model, Physical Functioning Scale, Split C", label="tab:physfun2cgrm1pl"))
@ 

As can be seen from Table \ref{tab:physfun2cgrm1pl}, there are no obvious problems with this model. The rather low ability estimates are interesting in that they are all negative, but the discrimination parameter is quite high suggesting that there is much information in the choosing of one response over another. 

<<physfun2cgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(physfun2c.grm.2pl)), caption="Coefficient Estimates for Two Parameter Graded Response Model, Physical Functioning Scale, Split C", label="tab:physfun2cgrm2pl"))
@ 

As can be seen from Table \ref{tab:physfun2cgrm2pl}, the discrimination parameters have lowered for the majority of items, except for 7 and 11 which refer to either walking up one flight of stairs or along one block (which presumably the non American sample here understood). 

The final step in the model building process is to assess each models\' performance on unseen data. 

<<physfun2cgrmtest, echo=FALSE, results=tex>>=
physfun.notc <- randitems.notc[,paste(rand, c(3:13), sep="")]
physfun2c.grm.1pl.test <- testIRTModels(physfun2c.grm.1pl, physfun.notc, gpcmconstraint=NULL, grmconstraint=TRUE)
physfun2c.grm.2pl.test <- testIRTModels(physfun2c.grm.2pl, physfun.notc, gpcmconstraint=NULL, grmconstraint=FALSE)
physfun2c.grm.test.all <- rbind(physfun2c.grm.1pl.test, physfun2c.grm.2pl.test)
print(xtable(physfun2c.grm.test.all, caption="Performance of One and Two Parameter Physical Functioning Graded Response Models on Unseen Data (Splits A and B", label="tab:physfun2cgrmtest"))
@ 

As can be seen from Table \ref{tab:physfun2cgrmtest}, the one parameter model performed best on the unseen data. 

Next, the negative health scale was checked to ensure suitability for IRT modelling. 

<<neghealth2ccheck, echo=FALSE, results=hide>>=
neghealth2c.iio <- check.iio(na.omit(neghealth2c))
neghealth2c.mono <- check.monotonicity(na.omit(neghealth2c))
@ 

The negative health scale for this split showed no failures of the item ordering assumption or of the monotonicity assumption. 

Next, three partial credit models were fit to the data. 

<<neghealth2cgpcm, echo=FALSE, results=hide>>=
neghealth2c.gpcm.rasch <- gpcm(neghealth2c, constraint="rasch")
neghealth2c.gpcm.1PL <- gpcm(neghealth2c, constraint="1PL")
neghealth2c.gpcm.gpcm <- gpcm(neghealth2c, constraint="gpcm")
@ 

All three partial credit models suffered from non-monotonically increasing parameter estimate problems, and are not discussed here further. 

Next, one and two parameter Graded Response Models were fit to the data. 

<<neghealth2cgrm, echo=FALSE, results=hide>>=
neghealth2c.grm.1pl <- grm(neghealth2c, constrained=TRUE)
neghealth2c.grm.2pl <- grm(neghealth2c, constrained=FALSE)
@ 


<<neghealth2cgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(neghealth2c.grm.1pl)), caption="Coefficient Estimates for One Parameter Graded Response Model, Negative Health Scale, Split C", label="tab:neghealth2cgrm1pl"))
@ 

As can be seen from Table \ref{tab:neghealth2cgrm1pl}, there are no obvious problems with this model. The estimated discrimination parameter is quite low, suggesting that the items convey only the information in their location thresholds. 

<<neghealth2cgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(neghealth2c.grm.2pl)), caption="Coefficient Estimates for Two Parameter Graded Response Model, Negative Health Scale, Split C", label="tab:neghealth2cgrm2pl"))
@ 

It can be see from Table \ref{tab:neghealth2bgrm2pl}, that the ability estimates have lowered significantly, as have most of the discrimination parameters. 

Finally, the performance of each of these models was examined on unseen data. 

<<neghealth2cgrmtest, echo=FALSE, results=tex>>=
neghealth.notc <- randitems.notc[,paste(rand, c(14:19, 24:25, 28:29, 31), sep="")]
neghealth2c.grm.1pl.test <- testIRTModels(neghealth2c.grm.1pl, neghealth.notc, gpcmconstraint=NULL, grmconstraint=TRUE)
neghealth2c.grm.2pl.test <- testIRTModels(neghealth2c.grm.2pl, neghealth.notc, gpcmconstraint=NULL, grmconstraint=FALSE)
neghealth2c.grm.test.all <- rbind(neghealth2c.grm.1pl.test, neghealth2c.grm.1pl.test)
print(xtable(neghealth2c.grm.test.all, caption="Performance of One and Two Parameter Negative Health Graded Response Models on Unseen Data (Splits A and B)", label="tab:neghealth2cgrmtest"))
@ 

As can be seen from Table \ref{tab:neghealth2cgrmtest}, both models performed equivalently on the unseen data, therefore the simpler one parameter model is chosen. 

Next, the assumptions are checked for the positive health scale in this split. 

<<poshealth2ccheck, echo=FALSE, results=hide>>=
poshealth2c.iio <- check.iio(na.omit(poshealth2c))
poshealth2c.mono <- check.monotonicity(na.omit(poshealth2c))
@ 

There were no violations of the item ordering or monotonicity assumptions for this scale in this split.

Next, three partial credit models were fitted. 

<<poshealth2cgpcm, echo=FALSE, results=hide>>=
poshealth2c.gpcm.rasch <- gpcm(poshealth2c, constraint="rasch")
poshealth2c.gpcm.1PL <- gpcm(poshealth2c, constraint="1PL")
poshealth2c.gpcm.gpcm <- gpcm(poshealth2c, constraint="gpcm")
@ 

All three partial credit models has non-monotonic parameter estimates, and are not analysed further here. 


Next, one and two parameter Graded Response Models were fitted to this scale.

<<poshealth2cgrm, echo=FALSE, results=hide>>=
poshealth2c.grm.1pl <- grm(poshealth2c, constrained=TRUE)
poshealth2c.grm.2pl <- grm(poshealth2c, constrained=FALSE)
@ 

<<poshealth2cgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(poshealth2c.grm.1pl)), caption="Coefficient Estimates for Positive Health One Parameter Graded Response Model, Split C", label="tab:poshealth2cgrm1pl"))
@ 

The coefficient estimates for this model are shown in Table \ref{tab:poshealth2cgrm1pl}. It can be seen that the discrimination parameter is quite low, and the ability estimates (especially for question 1) are quite high. One would expect this to alter under a two parameter model. 

<<poshealth2cgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef2mat(coef(poshealth2c.grm.2pl)), caption="Coefficient Estimates for Positive Health Two Parameter Graded Response Model, Split C", label="tab:poshealth2cgrm2pl"))
@ 

Table \ref{tab:poshealth2cgrm2pl} shows that the speculation regarding the shape of ability estimates and discrimination parameters was quite wrong, ad Q1 retains its extremely high ability estimates and Question 22 lowers its discrimination parameter significantly while gaining an extremely high estimate for ability. Q22 refers to pain, and this appears to be an extremely strong predictor of overall health, as the other two questions with as high abilities represent overall health questions.

Finally for this scale, its performance on unseen data is assessed. 

<<poshealth2cgrmtest, echo=FALSE, results=tex>>=
poshealth.notc <- randitems.notc[,paste(rand, c(1,20,22:23, 26:27, 30, 34, 36), sep="")]
poshealth2c.grm.1pl.test <- testIRTModels(poshealth2c.grm.1pl, poshealth.notc, gpcmconstraint=NULL, grmconstraint=TRUE)
poshealth2c.grm.2pl.test <- testIRTModels(poshealth2c.grm.2pl, poshealth.notc, gpcmconstraint=NULL, grmconstraint=FALSE)
poshealth2c.grm.test.all <- rbind(poshealth2c.grm.1pl.test, poshealth2c.grm.2pl.test)
print(xtable(poshealth2c.grm.test.all, caption="Performance of Positive Health One and Two Parameter Models (Split C) on unseen data (Splits A and B)", label="tab:poshealth2cgrmtest"))
@ 

As is shown in Table \ref{tab:poshealth2cgrmtest}, the one parameter model provided a batter fit to the unseen data. 

The next stage of the analysis is to examine the usefulness of IRT models for the Mindful Attention Awareness Scale in this split.

The first step is to check the item ordering and monotonicity assumptions. 

The automated item selection procedure suggests that items 6 and 15 do not fit the scale well enough to be considered, so these items are removed first. 





\subsection{Discussion}

There are a number of interesting findings which have emerged from
the study.  The most striking is the large negative correlation between
optimism and self reported health.  The sample in this study is quite
large, so the result is unlikely to be a statistical fluke.  That being
said, the result is problematic to explain, given the large amount
of evidence of beneficial effects of optimism on health\cite{rasmussen2009optimism}.

That being said, there have been some findings, where higher optimism
has not has been associated with health outcomes.  There are two major
explanations for the curious and unexpected phenomenon,given the links
established by a recent meta-analysis \cite{rasmussen2009optimism}.
The first centers on the dimensionality of the optimism construct.
Many believe that these results are caused by optimism self report
measures being reflections of two interlinked constructs, optimism
and pessimism \cite{Herzberg2006} as established in a factor analytic
study in a sample of over 46000 participants.  Some authors claim that
the apparently contradictory results suggest that the pessimism part
of the construct is the driver of the effects on health, and that
the correlations between the two constructs decline with age.  This
viewpoint was partially supported by the recent meta-analysis which
found that pessimism had a larger effect on health, though the difference
between optimism and pessimism was not significant \cite{rasmussen2009optimism}.
The other viewpoint argues that the effects of optimism on health
are mediated by negative and positive affect, and that high levels
of negative affect can either negate or reverse the optimism-health
link \cite{Baker2007}.  The aforementioned Baker study found that
the optimism health link was entirely mediated by negative affect.
Nonetheless, the balance of the evidence suggests that optimism has
beneficial consequences for health and healthy behaviours.

An explanation for this finding might be that it was the result
of high levels of negative affect in the population.  However, this
variable was not measured, so such an explanation can be regarded
as speculative at best.  It is worth noting however, that the original
literature of the beneficial effects of optimism on health focused
on cellular immunity, which is obviously quite different from self
reported health.A problem with this explanation is that reports in the
literature indicate that the optimism-health link is larger when self
report methods are used \cite{rasmussen2009optimism}.  Age may also
have been a factor, as the regression weight for this vaiable was
negative, which suggests that the relationship may have been different
if this research had been carried out in a sample with a broader distribution
of ages.  We do not have a good explanation for this finding.
This issue could be resolved with a prospective study
measuring optimism and health at baseline, and having participants
report health problems and visits to medical professionals over the
course of a year.  Such a study could allow the casual chains of this
effect to be untangled.

Another interesting finding which arose from this research is the
impact of mindfulness scores on other health variables.  MAAS scores
correlated positively with all of the health sub-scales, very significantly
in the case of emotional well-being.  This may suggest that brief mindfulness
interventions may be of use for improving overall population health,
both physical and mental.  That being said, the researchers would like
to earnestly observe that the issues surrounding the mindfulness construct
itself and its relations with mindfulness meditation practice need
to be resolved before such strong conclusions can be drawn.

Another fascinating finding in this research is the strong negative correlation
between mindfulness and optimism.  Our study appears to have been the first to assess
these constructs using self-report measures, and this finding was not expected to occur.
MBSR programs have been found to increase optimism in a number of studies \cite{Carson2004},
but our results seem to show that mindfulness and optimism may be inversely related.

There are a number of reasons why this could be so.  Optimism is defined as generalised positive outcome expectancies about the future,
while mindfulness is defined as non-judgmental awareness of the content of thoughts.
It seems plausible that increased mindfulness could lead persons to become less optimistic,
 as their newfound awareness of their own thought patterns and behaviours makes them aware
that events have not always worked out well.  This increased awareness could temper future
assessments of the future, and decrease optimism as measured by the Life Orientation Test Revised.

This study also confirms the proposed one factor structure for the MAAS, in line with previous research.
This sample also appears to show that the LOT-R can be modelled without loss of information with
just one factor.  We also demonstrated a replicable and parsimonous 4 factor structure for the RAND MOS,
and our results cast further doubt on the notion that these factors are uncorrelated.

It is worth noting that in all cases, parallel analysis did not provide a good measure of
the best number of factors to retain.  For all three measures, the MAP criterion provided
a more accurate metric.  This may have resulted as parallel analysis procedures tend
to sample from a normal distribution, and this condition was not met for any of our
variables.  We would argue that the use of multiple decision criteria on a regular basis
in factor analytic research would help us to understand which method suits a particular
application best.

A major limitation of this study was the exclusive use of self report
measures, and a student sample.  That being said, the sample was large
and representative of the general student population.  Given that over
70\% of Irish people of this age group now attend college, it could
be argued that this sample is relatively representative of Irish young
people at large.  This, however, is somewhat speculative and further
research would need to investigate this proposition further.

In conclusion, this study points towards the importance of considering
psychosocial variables and their impact on health, and suggests that
further research is needed to examine how these psychological variables
are mediated by culture into differential biological outcomes.


<<writefileforexperiment, echo=FALSE, results=hide>>=
save.image("healthforthesis.rda")
@ 


%%% Local Variables:
%%% TeX-master: "PlaceboMeasurementByMultipleMethods"
% %%% End:
