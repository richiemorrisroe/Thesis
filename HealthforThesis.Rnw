
<<importdata, echo=FALSE, results=hide>>=
hom<-read.csv("HOM data for RFINAL.csv")
hom <- hom[,1:65]
write.csv(hom, "homfinal.csv")
hom1 <- hom[with(hom, CollectMeth=="Paper"),]
hom2 <- hom[with(hom, CollectMeth=="Online"),]
## setOptions("xtable.include.rownames"=FALSE)
## setOption("xtable.format.args", list=(big.mark=","))
@ 



\section{Introduction}
\label{sec:introduction}


The major experimental work of this thesis involved the placebo effect, and as the research considered the utility of a new measure to predict this response, it was considered important to administer some instruments which had been shown to be associated with it in previous studies.

The only measure for which a well-validated scale exists which has been shown to impact the placebo response is optimism \cite{Geers2005,morton2009reproducibility}. Specifically, the Life Orientation Test, Revised (LOT-R) has been shown in a number of studies to be associated with the response to placebo. 
%% Interestingly enough, the Morton study referenced above showed an effect for optimism only in a repeated measures design, but the Geers \textit{et al} study showed a significant relationship in a between-subjects design. The relationship between placebo effect and optimism is discussed in more detail in Section \ref{sec:optimism} below. 

Mindfulness, on the other hand is a construct that has both been associated with health \cite{Carmody2008} and moderation  of the relationship between explicit and implicit measures \cite{Levesque2007}, and as such provided a construct which had been associated with the IAT in previous work (c.f. \ref{cha:literature-review}) and could plausibly be associated with health. Some more in-depth discussions of this potential relationship were outlined in Chapter \ref{cha:methodology}, in Section \ref{sec:embod-cogn-plac}. 

Health (in the form of the RAND-MOS) was included in this piece of research both to replicate the optimism-health link and examine if the health-mindfulness link held when both constructs were measured using survey methods, and as a weak proxy for the placebo effect. This follows as if the placebo effect is the result of expectancies, then there should be some shared variance between the response to placebo and other health-related cognitions. Additionally, the study formed an opportunity to collect background data for the population of interest, to assess if the participants in the experiment were systematically different from those who had responded to a survey invitation. 


\section{Optimism and Placebo}
\label{sec:optimism}


The first trait to examine is that of dispositional optimism, which is often defined as \textit{generalised outcome expectancies about the future} \cite{Carver2010}. When using this definition, it seems relatively likely that there may be a relationship between optimism and placebo response, and yet this has only been investigated in recent years. 

Dispositional optimism appears to exert some influence on placebo effects, in some situations \cite{Geers2005,morton2009reproducibility}. The effect seems to be that those higher in optimism respond better to positive suggestions, while those higher in pessimism respond better to negative suggestions. Another study found that general (but not specific) expectancies had a significant impact on the response to placebo in a meta-analysis of randomised controlled trials of chronic back pain \cite{myers2008patient}. Generalised expectancies are essentially how optimism is defined, given that all expectancies around future states of health are essentially outcome expectancies. 

Other studies appear to  show that this optimism effect is not general, but rather depends on the context in which the experiment takes place \cite{Hyland2006}. In the Hyland experimental study, spirituality rather than optimism was a predictor of the response.  

Crucially, this only occurred when the treatment was classified as spiritual. When a gratitude based treatment was used, gratitude acted as a predictor. These results suggest that any trait which predicts placebo response will likely only be effective in certain situational settings \cite{Kaptchuk2008a}. 

Moerman's meaning response theory would seem to be the most apt theory to use as a framework for understanding these effects, as the only element which differs in these experimental designs is the meaning which participants assign to the stimulus which acts as a placebo. Alternatively, one could argue that expectancies drive these effects by mediating the impact of other contextually relevant variables. However, to take this position would require that the theory of Kirsch, that expectancies exert direct physiological effects, would need to be abandoned \cite{Kirsch1985}, and indeed this position was tested in the experimental portion of the research (c.f. Chapter~\ref{cha:primary-research})

A recent study \cite{morton2009reproducibility} in a placebo analgesia paradigm argues for a stronger interpretation of the role of optimism. This experimental study used a repeated measures design, and utilised a preconditioning method in the first session which is known to increase the size of the placebo response \cite{Voudouris1985}. 

While in the first session there was no effect of optimism on the results, in the second study dispositional optimism was significantly correlated with placebo analgesia, explaining 55\% of the variance. This would suggest that while optimism may not produce a placebo response in itself, once a response has been produced it can be effective in maintaining it over time. 

Hyland suggests that the optimism effects on placebo response are mediated through expectancies, and that when these are not a factor, neither is optimism. This sounds plausible, but the relationship could easily go the other way in that optimism could drive the observed effects of expectancies. This is not a question which can be answered without further empirical investigations, which were conducted as part of this research (see Chapter \ref{cha:primary-research}). 


The relationship between optimism and health is well known. 
Optimism was recently reviewed in relation to its 
effect on physical health and wellbeing \cite{Carver2010}, 
optimism has been used as a predictor for many years in the area of Psychoneuroimmunology (PNI)
and appears to be usually associated with better health outcomes than
is pessimism \cite{Baker2007} \cite{Conway2008}. Optimism has also
been associated with lower mortality risk in a large longitudinal
cross-sectional study of individuals at risk for cardio-vascular disease
(the Women's Health Initiative) \cite{Tindle2009}. 

A meta-analysis
has also confirmed this link between optimism and better coping styles,
as well as a strong negative relationship between optimism and negative
affect \cite{andersson1996benefits,Segerstrom2006}. Higher levels
of optimism have also been associated with quicker recovery from surgery,
insulin therapy and chemotherapy \cite{Allison2000}. A prospective
study looking at outcomes from a group of head and neck cancer patients
found that optimists consistently reported better health outcomes
than pessimists \cite{Allison2000}.%%  A meta-analysis reported on correlations
%% between optimism and post-traumatic growth, which found a mean effect
%% size of 0.2, but the studies were of quite poor quality, so this finding
%% must be regarded as tentative \cite{Bostock2009}.

The question of the mechanism by which optimism manifests differences
in health is still unclear. Some researchers argue for a direct effect
of optimism on immune function ,  while others
argue that optimism exerts its protective effects through the effects
of persistent striving after health goals \cite{Segerstrom2003}.

Additionally, there appears to be a curvi-linear relationship between optimism and health in some studies, which some authors attribute to participants holding both high optimistic and high pessimistic outcome expectancies simultaneously. 

\subsection{Factorial Structure of the LOT-R}
\label{sec:fact-struct-lot}

The factor structure of the LOT-R (and indeed, the dimensionality of the optimism construct more generally), has been a subject of some debate. The originators argue for a one-factor structure \cite{Carver2010}\cite{Scheier1994}, while other researchers have found that the scale is better modeled as having both optimism and pessimism components. The Geers work referenced above took a latter approach, dichotomising the scores on the scale to create groups of optimists and pessimists \footnote{this approach is not without its problems}. Therefore, both one and two factor models were applied to the scale to determine if a one or two factor structure fit better in the population of interest. 


\section{Mindfulness}

The construct of Mindfulness or 'attentional control' has been defined as: 'a mental ability which facilitates a direct and 
immediate perception of the present moment with non-judgemental awareness' \cite{kohls2009facets} (p. 2). Derived from the Buddhist 
contemplative traditions, mindfulness represents attention to the thought process, rather than to thought content \cite{brown2007addressing}. 
Jon Kabat-Zinn popularised the practice of mindfulness in the West \cite{Kabat-Zinn2003}, developing an eight-week 
long Mindfulness Based Stress Reduction (MBSR) programme which is now often used in clinical settings for patients with chronic illnesses. 

The usefulness of mindfulness in this research was twofold
\begin{enumerate}
\item It has been shown to be a predictor of health, and treatment programs developed around mindfulness practice have been associated with improved physiological and psychological outcomes

\item Additionally, the construct has also been implicated as a moderator of the relationship between explicit and implicit measures \cite{Levesque2007}
\end{enumerate}

The combination of these two features was why mindfulness (specifically the Mindful Attention Awareness Scale) was chosen as the measure of moderation between explicit and implicit expectancies. 

\subsection{Health benefits of Mindfulness}
\label{sec:health-benef-mindf}



In a study of cancer patients mindfulness training was found to reduce stress and to improve quality of life \cite{Carlson2007}, 
while in a study of patients who were HIV positive, MBSR training was associated with improved natural killer cell activity 
\cite{Robinson2003}. Meta-analyses have suggested that MBSR programmes have an effect size of d=.5 for psychological variables 
such as quality of life and mental health, and d=.2 for physical outcome variables such as cortisol levels and immune function \cite{Grossman2004}.
Measures of the construct of mindfulness date from the early years of this century. Mindfulness has been operationalised 
into a number of different scales including the Mindful Attention Awareness Scale (MAAS) \cite{brown2003benefits} , 
the Kentucky Inventory of Mindfulness Skills \cite{Ruth2004} and the Five Facet Mindfulness Questionnaire (FFMQ), 
which was developed by the factor analysis of items from a number of different scales \cite{Ruth2006}. The most popular instrument is
 the Mindful Attention Awareness Scale (MAAS) \cite{brown2003benefits}.

 Interestingly, while higher MAAS scores have been found to be associated with meditation experience in some 
studies \cite{brown2003benefits}, other research using student samples found no significant correlation between 
MAAS scores and experience with meditative practices (assessed by self report) \cite{MacKillop2007} . This finding which
 was also replicated by Thompson and Waltz \cite{thompson2007everyday}  who suggest that this may be due to the fact that 
mindfulness during meditation may be a state like construct, while mindfulness in everyday life may be a trait like construct. 
So while various rigorous reviews of clinical trials have found MBSR programmes to be effective for
 reducing stress \cite{Chiesa2009,praissman2008mindfulness} , it is still unclear whether individuals 
with higher levels of mindfulness are also psychologically healthier.
 MBSR programs have also been associated with higher levels of optimism
 \cite{Carson2004}, but mindfulness and optmism have not tended to be researched together. 
 only one study where mindfulness levels were correlated positively 
with self reported health \cite{Hansen2009} was identified as part of this research. Hansen, using a cross-sectional design, 
found that MAAS scores correlated with a five point one item measure of health,   
a very crude measure of health status. In a recent meta-analysis of 29 studies, 
Giluk explored the relationship between mindfulness, Big Five personality and affect \cite{giluk2009mindfulness}.
She found that mindfulness was strongly correlated with neuroticism and negative affect,
 though its not obvious whether being mindful lowers neuroticism or 
 whether neuroticism interferes with mindfulness. 


\section{Aims of the Research}

As discussed above, optimism is well known as a predictor of health, both self reported and objectively assessed and mindfulness, while a much newer construct, has also been associated with health in a number of studies.  In addition, optimism has been associated with the placebo response in some research, while mindfulness has been proposed as a mediator of the relationship between explicit and implicit measures.
The primary aim of this part of thesis was to develop and test psychometric models that could be used to predict scores according to IRT and factor analysis criteria in the experimental part of the research.

The major hypotheses of this part of the thesis were as follows:
\begin{itemize}
\item Optimism and mindfulness would be positively associated with health.

\item Optimism and mindfulness would be positively associated with one another. 

\item The RAND MOS would have 8 first order factors%% , and two second order factors, which would be correlated

\item The MAAS would have one factor

\item The LOT-R would have one factor.
\end{itemize}

The major types of analysis carried out are described below. 

The response rate for Sample One (paper) was approximately
90\% of those asked, while the response rate for Sample Two (Online)
was 10\%. Note that psuedo-random sampling was used for sample one, while simple random sampling was used for Sample Two. 

\section{Methods}
\label{sec:methods}

The methods used for this part of the thesis were primarily psychometric. Cross-validation approaches (described in Chapter \ref{cha:methodology}) were applied to both samples to increase generalisability of the models to the experimental portion of the research. This section describes the measures used for this part of the study, followed by a description of the sample, and concludes with a description of the methods of analysis used in this study. 

\subsection{Measures}
\label{sec:measures}
There were three measures used for this part of the research.
\begin{enumerate}
\item \textit{RAND-MOS}: The RAND Medical Outcomes Survey produced the most widely used instrument of HRQoL (Health Related Quality of Life) worldwide \cite{hays1993rand}. The instrument was later revised and the number of response categories standardised across scales and renamed the SF-36. The older version was used for this research, as they are extremely similar and the newer version is under copyright and expensive to use, even for non-commercial research. The RAND-MOS has 36 questions, and is divided into 8 sub-scales, General Health (GH),
Physical Functioning (PF), Role Limitations (RL), Emotional Role Limitations
(RLE), Pain (PN), Energy (EN), Emotional Well Being (EMWB) and Social
Functioning (SF). All sub-scales showed acceptable reliability
(>.7) in this and in other studies over the years\cite{Lam2007, Ferreira2000}.
The instrument has 8 first order factors and two higher order factors
\cite{Hann2008}. The scale involves dichotomous, trichotomous and
five and six point scales for various items, so all questions are
transformed to a 100 point scale before analysis, where higher scores
represent better functioning.
\item The \textit{Mindful Attention Awareness Scale (MAAS)}
\cite{brown2003benefits} is a 15 item scale which is scored on
a six point scale from ``almost always'' to ``almost never''. The scale uses questions which measure mindlessness. The summary score is produced
from the mean of all indvidual scores. The scale has shown adequate
psychometric validity in many samples, with alpha ranging from 0.7
to 0.9\cite{brown2003benefits,Ruth2006}.
\item \textit{Life Orientation Test, Revised:} The Life Orientation Test Revised (LOT-R) was developed and revised
by Scheier and Carver \cite{Scheier1994}, and consists of 10 items.
Three of the items load on pessimism, three on optimism and four are
distractor items. The LOT-R has shown excellent psychometric validity,
and is very commonly used as a measure of optimism/pessimism. The
scale is scored on a 5 point scale,
and the sum of all items after items 3,7 and 9 are reverse coded are taken to produce the overall score.
\end{enumerate}



\subsection{Sampling for this research}
\label{sec:sampl-this-rese}
The first 392 participants completed the forms by hand between August and October 2009. The participants were sampled pseudo-randomly from all of the public areas (coffeeshops, restuarants etc) of the campus.

Following this pen and paper approach to sampling, the survey was sent to a random selection of students via email on December 12th 2009, and data was collected and analysed from this point until the 24th of December 2009. Differences between the samples and the possible effects of these on the results obtained are discussed below. %% In addition, due to the unexpected results of the analysis, a third sample was collected in Summer 2011. 

\section{Analysis}
Analysis was carried out seperately on the two samples, to allow for development of factor analysis and IRT models on the first sample and validation on the second. In addition, it could not be assumed that two samples collected in different ways would be comparable. 

%% Firstly, the proportion of missing data was identified, and multiple imputation was employed to combat this problem in cases where there were substantial amounts of missing data. This was only necessary in the case of the sample collected by online methods. 

All missing data was assumed to be Missing Completely at Random (MCAR) \cite{little1987statistical}, and thus a complete-cases analysis was carried out on all data. 

The data was checked for errors in entry or recording using summary functions and plots. Following this, the question responses were recoded according to the instructions.

Following this, the summary scores were calculated. Next, summary statistics and characteristics of the data were reported. 
Following this, a correlation matrix for the data was calculated and analysed.



Next, simple reliability analyses (Cronbach's $\alpha$) were carried out on the scales themselves. Following this, parallel analysis, the MAP criterion and the scree plot were used to estimate the number of factors which could be extracted from the data. After this, factor solutions were extracted using principal axis methods, with maximum likelihood estimation used if these failed to converge. Primarily, direct oblimin methods of rotations were utilised, but promax rotations were also applied to ensure that the proposed structure was not overly sensitive to the methods of rotation.

After the various factor structures were obtained, they were plotted and analysed for interpretability. %% Communalities and uniquenesses were assessed to ensure that there was no over or under factoring in the solutions. Communalities were then graphed against the number of factors extracted and the methods of extraction to provide a simple graphical guide to the usefulness of each solution.

Following this procedure of extraction and interpretation, Structural Equation Modelling was applied to each of the proposed factor solutions. The optimal factor solution was chosen using the AIC of each fitted model, along with the RMSEA of the proposed solutions.

The successful SEM models from the first sample were then tested on a subset of the second sample, to determine their performance on new data. 

Following the development and testing of SEM models, each of these was tested on the validation set and factor scores were created for each of the measures.

Additionally, SEM models were applied to determine the best fitting model for the relationships between the three primary constructs of interest (general health, optimism and mindfulness). 

Following the investigation of structure with the methods of classical test theory, the scales were analysed using Rasch models and item response theory. Firstly, Mokken analyses  were run, in order to check the assumptions of monotonicity, local independence and to assess how many sub-scales the analysis should be carried out on. 

Following this, a Rasch model was fitted to the data, and person and item parameters estimated from the data. Item and person fit statistics were also calculated, and as this model did not fit any of the three scales, a two parameter model was fit to the data. Again, item and person estimates were obtained and the relevant fit statistics calculated. 


After this, linear regressions were run to examine the differential effects of each of the correlated variables. Stepwise selection on the training set was carried out, along with lasso and ridge regression  methods. Within each split, each regression model used ten-fold cross-validation to choose the optimal penalty criterion. 

The performance of all methods was then assessed on held-out data. In the case of Sample One, some of Sample Two was used as a heldout data set. For Sample Two, the entire dataset was split into three splits, and the cross validation procedure carried out for each. The splits were kept quite large (approximately 300 non missing observations) to allow for psychometric models to be fit to each split seperately. %% and to be able to compare the performance of simple mean/sum scores against the factor scores and ability estimates derived from the psychometric modelling procedures. 

The approach taken to the psychometric analysis of the second sample of data was as follows.  Firstly, factor models were built on the two remaining samples from this dataset (the first having been used to validate the results from Sample 1).  Next, a CFA was run on each of the other samples, such that if the model was developed on the B sample, it was tested on both the A and the C sample.  This provides a better measure of accuracy and replicability for each of the proposed factor structures.  Finally, the most successful model was back-tested on the data from Sample One. The model(s) chosen by this procedure was then used to predict these scores for the experimental portion of the research (and compared against the typical scoring methods). 

\subsection{Crossvalidation Approaches}
\label{sec:crossv-appr}

%% \subsubsection{Crossvalidation for factor selection}
%% \label{sec:crossv-fact-select}

%%  As discussed in the Methodology, there are two main approaches here, either item-subject CV (known as Gabriels method) or the typical ten fold validation common in  machine learning (Wolds method). Both of these approaches were examined for the RAND items, and the results are shown in Section \ref{sec:rand-mos-1}. In the gabriel approach, a leave two of item and subject out was used, while ten fold cross-validation was used for the Wold method. One problem with this approach is that there were a huge number of non-responses to questions 13-16 on the RAND MOS, for unknown reasons.  This brings down the potential sample on this instrument to 281, which is not enough for the full analysis.  However, it is enough to test the factor solutions from the first sample, and as the RAND was not used in the experimental part of the research, this was considered sufficient. 

\subsubsection{CrossValidation for Psychometric Models}
\label{sec:crossv-psych-models}

Cross-validation is a tool for developing and testing models where data is limited or it is desirable that all data should be used efficiently, while guarding against over-fitting, which is when transitory features of a particular data-set are modeled, which reduces generalisability. 

Typical cross-validation holds back some data in order to test the models developed on the rest.  This hold-out sample is typically of the order of 10\%.  However, for factor analyses, around 300 observations are typically needed for accurate estimation of parameters.  
In this research, models were developed on the first sample, and then were fitted to a subset of the second sample.  This procedure was then  repeated with the data not used for testing in the second sample and the first sample.  This reduced the potential influence of over-fitting, while developing psychometric models which could be applied to the experimental sample. 


The testing was carried out using a few different methods.  Firstly, the predict scores method will be used for all factor analytic solutions.  Secondly,  CFA models will be fitted to the new data, allowing for comparision of their effectiveness on unseen data.  

A different procedure was  followed for the IRT models, using  three fold cross validation used to build an IRT model on each of the ten segments, and then comparing the estimates ability scores from the model built on the 33\% of the data, and from the estimates built on the held-out data. The difference between these two measures will provide an estimate of error for each of the model\'s predictive capability. This will then provide a metric for the selection of the best model, which can then be applied to the experimental data. 


\section{Sample One - Results}

%% \section{Missing Data Analysis}

%% The first step in the analysis was the assessment of how much data was missing.

<<loadpackages, echo=FALSE, results=hide>>=
require(ggplot2)
require(psych)
require(xtable)
require(OpenMx)
require(car)
require(arm)
require(caret)
require(nFactors)
require(bcv)
require(glmnet)
require(mokken)
require(eRm)
require(ltm)
@

<<sourcefunc, echo=FALSE, results=hide>>=
source("func.R")
@




<<scorescalestransform, echo=FALSE, results=hide>>=
grep.rand <- grep("^RANDQ", x=names(hom))
randitems <- hom[,grep.rand]
rand <- "RANDQ"
randset1 <-paste(rand, c(1, 2, 20, 22, 34, 36), sep="")
randset1 <- hom[,randset1]
randrecode1 <- RecodeMany(randset1, vars=c("RANDQ1", "RANDQ2","RANDQ20", "RANDQ34","RANDQ36"), Recodings=("1=100;2=75;3=50;4=25;5=0"))
randset2 <- paste(rand, c(3:12), sep="")
randset2 <- hom[,randset2]
randrecode2 <- RecodeMany(randset2, vars=c("RANDQ3", "RANDQ4","RANDQ5","RANDQ6","RANDQ7","RANDQ8","RANDQ9", "RANDQ10","RANDQ11","RANDQ12"),Recodings="1=0;2=50;3=100")
randset3 <- paste(rand, c(13:19), sep="")
randset3 <- hom[,randset3]
randrecode3 <- RecodeMany(randset3, vars=c("RANDQ13", "RANDQ14", "RANDQ15", "RANDQ16","RANDQ17","RANDQ18", "RANDQ19"), Recodings="1=0;2=100")
randset4 <- paste(rand, c(21,23,26,27,30), sep="")
randset4 <- hom[,randset4]
randrecode4 <- RecodeMany(randset4, vars=c("RANDQ21", "RANDQ23", "RANDQ26","RANDQ27","RANDQ30"), Recodings="1=100;2=80;3=60;4=40;5=20;6=0")
randset5 <- paste(rand, c(24,25,28,29,31), sep="")
randset5 <- hom[,randset5]
randrecode5 <- RecodeMany(randset5, vars=c("RANDQ24", "RANDQ25", "RANDQ28", "RANDQ29", "RANDQ31"), Recodings="1=0;2=20;3=40;4=60;5=80;6=100")
randset6 <- paste(rand, c(32,33,35), sep="")
randset6 <- hom[,randset6]
randrecode6 <- RecodeMany(randset6,vars=c("RANDQ32","RANDQ33","RANDQ35"), Recodings="1=0;2=25;3=50;4=75;5=100")
randrecoding <- ls(pattern="randrecode")
randrecoding.df <- as.data.frame(lapply(randrecoding, function (x) get(x)))
randsortpaste <- paste(rand, c(1:36), sep="")
randitems.unscored <- hom[,grep.rand]
randitems.scored <- randrecoding.df[,randsortpaste]
hom[,grep.rand] <- randitems.scored
hom <- createSumScores(hom) 
hom1 <- hom[hom$CollectMeth=="Paper",]
hom2 <- hom[hom$CollectMeth=="Online",]
@




<<missingdata, echo=FALSE, results=hide, cache=TRUE>>=
paper.missing <- sapply(hom1, function (x) sum(is.na(x)))
online.missing <- sapply(hom2, function (x) sum(is.na(x)))
@
%% \begin{figure}
<<label=papermissingplot, echo=FALSE, results=hide,  eval=FALSE>>=
ggplot(as.data.frame(paper.missing),aes(x=paper.missing, y=..density..))+geom_density()
@
%%   \caption{Density Plot of Missing values in paper sample. x Axis is the proportion of responses with that  number of missing observations in each variable }
%%   \label{fig:papermissingplot}
%% \end{figure}

%% \begin{figure}
<<onlinemissingplot, echo=FALSE, results=hide, eval=FALSE>>=
  print(ggplot(as.data.frame(online.missing), aes(x=online.missing))+geom_density())
@
%%   \caption{Histogram of Missing Values, Online sample}
%%   \label{fig:onlinemissingplot}
%% \end{figure}




%% As can be seen from Figure \ref{fig:papermissingplot}, there are quite low levels of missing data for the sample collected by paper.


%% However, the situation is very different with the second sample, as can be seen from Figure \ref{fig:onlinemissingplot}, where there are a number of items which have between six and eight hundred missing values.  This was investigated further, as this amount of missing values in a small set of the data may cause problems in the course of the analysis. 

<<missingvaluestable, echo=FALSE, results=hide>>=
missing.many <- online.missing[online.missing>300]
missing.many.df <- as.data.frame(missing.many)
names(missing.many.df) <- "Number of Missing Observations"
missing.many.xtab <- xtable(missing.many.df, label="tab:missingmanytable", caption="Number of Missing Observations for RAND MOS items with greater than 10 percent missingness")
print(missing.many.xtab)
@

%% It can be seen from Table \ref{tab:missingmanytable} that the problems with missing data are concentrated in four consecutive questions, RAND questions 13 through 16. These questions all load on the Role Limitations subscale, explaining why this subscale shows up with lots of missing data.  The consecutive nature of the data suggests that the reason for this may be that participants believed that it was only necessary to answer one of the questions, though that does not explain why so many people did not answer the first question.

%% Next, we will impute the missing data using a multiple imputation procedure (as discussed in the Methodology).

<<impute, eval=FALSE, echo=FALSE, results=hide, cache=TRUE>>=
require(mice)
hom.imp <- mice(hom[,2:length(hom)], m=10) #remove first column as it causes problems with the imputation
hom.comp1 <- complete(hom.imp, 1)
hom.comp2 <- complete(hom.imp, 2)
hom.comp3 <- complete(hom.imp, 3)
hom.comp4 <- complete(hom.imp, 4)
hom.comp5 <- complete(hom.imp, 5)
hom.comp6 <- complete(hom.imp, 6)
hom.comp7 <- complete(hom.imp, 7)
hom.comp8 <- complete(hom.imp, 8)
hom.comp9 <- complete(hom.imp, 9)
hom.comp10 <- complete(hom.imp, 10)
homlist <- list(hom.comp1, hom.comp2, hom.comp3, hom.comp4, hom.comp5, hom.comp6, hom.comp7, hom.comp8, hom.comp9, hom.comp10)
for (i in 1:length(homlist)) {
  homi <- homlist[[i]]
  write.csv(homlist[[i]], file=paste("homcomp", i, ".csv", sep=""))
}
  
@ 

<<readcomphom, echo=FALSE, results=hide, cache=TRUE>>=
homfiles <- list.files(pattern="homcomp")
homlist <- lapply(homfiles, read.csv)
@ 

\section{Descriptive Statistics}



To begin the analysis, frequencies, means and ranges were calculated
for the major variables of interest (General Health, Mindfulness, Optimism and Emotional Well Being).  The results of this analysis can
be seen in Table \ref{tab:sumstatscales}, below.

<<sumstats, echo=FALSE,results=tex>>=
hom.tot <- hom1[,66:75]
hom.tot2 <- hom1[,c("generalhealth", "optimism", "mindfulness", "emwellbeing")]
tot.sum <- summary(hom.tot)
tot.sum2 <- summary(hom.tot2)
tot.xtab <- xtable(tot.sum2, label="tab:sumstatscales", caption="Summary Statistics for Health, Mindfulness and Optimism")
print(tot.xtab, include.rownames=FALSE) #include packing rotating if fails here
@



In Table \ref{tab:democollect} the breakdown of the demographics
of the sample by Collection Method is shown.

<<demostats, echo=FALSE, results=tex>>=
hom.demo <- hom1[,2:8]
hom.demo.xtab <- xtable(summary(hom.demo), label="tab:democollect", caption="Demographic Statistics for Sample One (paper sample)")
print(hom.demo.xtab, scalebox=0.6, include.rownames=FALSE)
@


Mindfulness levels were quite high, while optimism levels were
at the half way point of the scale. Health levels were quite high for all of the subscales, which makes sense given the non-clinical sample involved in this research. 



\section{Inferential Statistics}

Following these preliminary analyses, the main hypotheses can now
be addressed.



\begin{figure}
<<pairsplot, echo=FALSE, fig=TRUE, pdf=TRUE>>=
print(pairs.panels(na.omit(hom.tot)))
@
\caption{Pairs plot for Scale totals of Health, Optimism and Mindfulness Data.  Top triangle has correlations scaled by their size, bottom triangle has scatterplots with locally weighted regression lines, diagonal has histograms with density estimation. All correlations significant at p<=0.001.}
\label{fig:pairsplot}
\end{figure}





As can be seen from Figure \ref{fig:pairsplot}, the optimism hypothesis
was not supported.  Contrary to predictions, optimism was negatively
correlated with health.
Possible explanations are examined in the Discussion section.  In fact, optimism correlated negatively with all of the other totals, suggesting that something strange happened in the sample.

%% \begin{figure}
<<optplot1, echo=FALSE, eval=FALSE>>=
optplot1 <- ggplot(hom1, aes(x=generalhealth, y=optimism))+layer(geom="smooth", method="lm")
print(optplot1)
@  
%%   \caption{Plot of General Health against optimism with a linear regression smooth line. Dark areas on plot represent the confidence intervals (95\%)}
%%   \label{fig:optplot1}
%% \end{figure}


Figure~\ref{fig:pairsplot}, shows a linear regression of optimism against general health in the sample collected by paper.  It can be clearly seen that the relationship is negative, an unexpected and surprising occurence.  %a fact which is borne out in the examination of Figure \ref{fig:optplot2} below.




%% \begin{figure}
<<optplotgend, echo=FALSE, eval=FALSE>>=
optplotgend <- ggplot(na.omit(hom1), aes(x=generalhealth, y=optimism))+layer(geom="smooth", method="lm")+facet_grid(.~Gender)
print(optplotgend)
@  
%%   \caption{Plot of General Health against Optimism stratified by Gender using a linear regression smooth. Dark edges represent errors of estimation}
%%   \label{fig:optplotgend}
%% \end{figure}


Participants of both genders showed the relationship in the same direction, with participants reporting greater health reporting less optimism. The result is a general trend across all subgroups divided by college, suggesting that it is the result of a general pattern across the sample rather than being driven by some small number of abberant observations. Indeed, when participants were stratified by College of study, the same trend was apparent suggesting that the relationship was consistent across all sub-groups. 



%% \begin{figure}
<<healthmaassamp, echo=FALSE, eval=FALSE>>=
healthmaas.samp <- ggplot(na.omit(hom), aes(x=generalhealth, y=mindfulness, colour=CollectMeth))+layer(geom="smooth", method="lm")
print(healthmaas.samp)
@  
%%   \caption{General Health against Mindfulness, Stratified by Method of Collection, linear regression smooth, dark areas on plots represent confidence intervals}
%%   \label{fig:healthmaasmethplot}
%% \end{figure}


It can be seen from Figure \ref{fig:pairsplot} that the relationship between health and mindfulness positive, and of the same magnitude at that observed between health and optimism. 
%% \begin{figure}
<<healthmaasgend, echo=FALSE, eval=FALSE>>=
healthmaas.gend <- ggplot(na.omit(hom1), aes(x=generalhealth, y=mindfulness, colour=Gender))+layer(geom="smooth", method="lm")
print(healthmaas.gend)
@  
%%   \caption{General Health against Mindfulness Stratified by Gender using a linear regression smooth. Dark areas represent errors of estimation}
%%   \label{fig:healthmaasgend}
%% \end{figure}


Gender did not appear to have a substantial effect on mindfulness totals, although it is interesting to note that the range of health scores reported was much greater in the female participants.
%% \begin{figure}
<<healthmaas, echo=FALSE, eval=FALSE>>=
healthmaas1 <- ggplot(na.omit(hom1), aes(x=generalhealth, y=mindfulness, colour=College))+layer(geom="smooth", method="lm")
print(healthmaas1)
@  
%%   \caption{General Health against Mindfulness stratified by College using a linear regression smooth}
%%   \label{fig:healthmaascoll}
%% \end{figure}


%% As can be seen from Figure \ref{fig:healthmaascoll} the relationship between general health and mindfulness levels is positive, and constant across the different groups of students.

%% \begin{figure}
<<maasage, echo=FALSE, eval=FALSE>>=
maas.age <- ggplot(na.omit(hom1), aes(x=Age, y=mindfulness, ))+layer(geom="smooth", method="lm")
print(maas.age)
@  
%%   \caption{Age against Mindfulness Scores using a linear regression smooth}
%%   \label{fig:maasage}
%% \end{figure}


MAAS scores were associated with greater health as expected, as can be seen from Figure \ref{fig:pairsplot}. 

\section{Regression Analyses}

Given the correlation matrix reported shown above in Figure \ref{fig:pairsplot},  regression analyses were run on the three major variables (General Health, Mindfulness and Optimism) to determine which other variables were involved in the effect.

\subsection{Optimism}


<<cvsetup, echo=FALSE, results=hide, cache=TRUE>>=
hom1.full <- na.omit(hom1)
hom1.partition <- with(hom1.full, createDataPartition(optimism, list=FALSE, p=0.8))
hom1.train.opt <- hom1.full[hom1.partition,]
hom1.test.opt <- hom1.full[-hom1.partition,]
mytrain <- trainControl(method="cv")
@ 

<<opttrain, echo=FALSE, results=hide, cache=TRUE>>=
testpred <- hom1.test.opt[,c("Age", "pain", "mindfulness", "socialfunctioning", "rolelim", "rolelimem", "emwellbeing", "physfun", "energyfat", "generalhealth")]
trainopt <- with(hom1.train.opt, optimism)
trainpred <- hom1.train.opt[,c(4, 66:74)]
testopt <- with(hom1.test.opt, optimism)
@ 

<<optstep, echo=FALSE, results=hide, cache=TRUE>>=
opt.reg.first <- lm(optimism~(generalhealth+Age+College+pain+mindfulness+socialfunctioning+rolelim+rolelimem+emwellbeing+physfun+energyfat), data=hom1.train.opt)
opt.reg.simple <- lm(optimism~1, data=hom1.train.opt)
opt.step <- stepAIC(opt.reg.first, direction="both", k=2)
opt.step.pred.resp <- predict(opt.step, newdata=hom1.test.opt, type="response")
opt.step.pred.coef <- predict(opt.step, newdata=hom1.test.opt, type="terms")
step.pred.obs <- data.frame(pred=opt.step.pred.resp, obs=hom1.test.opt[,"optimism"])

@ 

<<optstepprint, echo=FALSE, results=hide>>=
print(xtable(summary(opt.step), caption="Coefficients for Stepwise Selected Regression Model (using AIC(k=3)), forward and backward selection from a full model", label="tab:hom1optstep"))
@ 

%% As Table \ref{tab:hom1optstep} shows, 

<<optsteppred, echo=FALSE, results=tex>>=
opt.step.test <- lm(optimism~generalhealth+Age+emwellbeing, data=hom1.test.opt)
print(xtable(summary(opt.step), caption="Coefficients for Stepwise Selected Model on Test Data", label="tab:hom1stepopttest"))
@ 

In the training sample, the stepwise selected models kept three predictors. General health and Age are retained, despite their lack of signifiance, suggesting that while they have an impact, they are moderated by the effect of emotional well being ($ p \le 0.0001$). 
As shown in Table \ref{tab:hom1stepopttest} the model performs much more poorly on unseen data. 

<<optridge, echo=FALSE, results=tex>>=
opt.ridge <- penalisedRegression(x=trainpred, y=trainopt, newy=testopt, testdata=testpred, alpha=0, nfolds=10, type="coefficients")
opt.ridge.pred <- penalisedRegression(x=trainpred, y=trainopt, newy=testopt, testdata=testpred, alpha=0, nfolds=10, type="response")
ridge.xtab <- xtable(as.matrix(opt.ridge), caption="Coefficients for Ridge Regression of Optimism on other variables", label="tab:hom1optridge")
digits(ridge.xtab) <- 4
print(ridge.xtab)
@

Next, a form of penalised regression known as ridge regression (or L2 regression) was used. Ridge regression penalises the coefficients by pushing them towards zero in the process of searching for the best model. However, it will not remove variables from the model, regardless of how small their coefficients get. It can be seen from Table \ref{tab:hom1optridge} that all of the coefficients are quite small, and mostly negative (which is the same as was seen in the OLS fit). Note that the coefficients shown are from the test data set, not used in the construction of the model. From this model, it would appear that mindfulness and emotional well being are the only useful predictors of optimism. 



The predictive ability of the ridge regression model was quite poor, as the model appeared to systematically underpredict the values for optimism. 

Next, a lasso regression model was fitted to the data. The lasso includes a penalty on the coefficients, similarly to ridge, but in contrast to ridge regression, lasso will remove predictors from the model.

<<optlasso, echo=FALSE, results=tex>>=
opt.lasso <- penalisedRegression(x=trainpred, y=trainopt,  testdata=hom1.test.opt, alpha=1, nfolds=10, type="coefficients")
opt.lasso.pred <- penalisedRegression(x=trainpred, y=trainopt, newy=testopt,  testdata=as.matrix(testpred), alpha=1, nfolds=10, type="response")
opt.lasso.xtab <- xtable(as.matrix(opt.lasso), caption="Coefficients for Lasso Regression on Optimism scores. Note that coefficients are for the fit of the chosen model on new data", label="tab:hom1optlasso")
digits(opt.lasso.xtab) <- 4
print(opt.lasso.xtab)
lasso.opt.pred <- penalisedRegression(x=trainpred, y=trainopt,  testdata=as.matrix(testpred), alpha=1, nfolds=10, type="link")
@ 

Table \ref{tab:hom1optlasso} shows that the lasso procedure has eliminated most of the predictors from the model. Only general health, energy fatigue and emotional well-being remain following the selection procedure. Note that mindfulness appears to be related to optimism under this model, but not when an OLS model is fitted with stepwise selection (see Table \ref{tab:hom1optstep}). %% However, these two models are not directly comparable as the OLS model was fitted to the same datset repeatedly, while the lasso and ridge models were fitted to new data. 





<<optloess, echo=FALSE, results=hide, cache=TRUE, eval=FALSE>>=
opt.loess <- loess(optimism~generalhealth*mindfulness*emwellbeing, data=hom1.train.opt)
## opt.loess.fit <- tuneLoess(optimism~generalhealth*mindfulness*emwellbeing, data=hom1.train.opt, tuneLength=10, newdata=hom1.test.opt)
opt.loess.pred <- predict(opt.loess, newdata=hom1.test.opt, type="response")
## opt.loess.pred.obs <- data.frame(pred=opt.loess.pred, obs=hom1.test.opt)
## train.gam.loess <- train(optimism~generalhealth*mindfulness*emwellbeing, data=hom1.train.opt, method="gamLoess", tuneLength=10)
@ 

<<optreglm, echo=FALSE, results=hide, cache=TRUE>>=
opt.reg.first <- lm(optimism~(generalhealth+Age+College+pain+mindfulness+socialfunctioning+rolelim+rolelimem+emwellbeing+physfun+energyfat), data=hom1.train.opt)
opt.reg.sum <- summary(opt.reg.first)
opt.reg.xtab <- xtable(opt.reg.sum, label="tab:optregfirst", caption="Maximal Model for Regression on Optimism")
print(opt.reg.xtab, table.placement="ht")
lm.pred.opt <- predict(opt.reg.first, hom1.test.opt, type="response")
@


%% In table \ref{tab:optregfirst} the summary of the regression results can be seen, the model is significant, with an $F$ value of \Sexpr{round(opt.reg.sum[["fstatistic"]],3)}  and an $R^2$ of \Sexpr{round(opt.reg.sum[["adj.r.squared"]],3)}.  


%% Below, in Table \ref{tab:optregfinal} can be seen the final model,
%% including all significant predictor variables.

<<optregfinal, echo=FALSE, results=hide, cache=TRUE>>=
opt.part <- createMultiFolds(na.omit(hom1$optimism), k=10)
optfolds <- TrainTestSets(opt.part, hom1)
opt.reg.final <- lm(optimism~generalhealth+mindfulness+emwellbeing+Age, data=hom1)
opt.reg.fin.stand <- standardize(opt.reg.final)
opt.fin.sum <- summary(opt.reg.final)
opt.fin.xtab <- xtable(opt.fin.sum, label="tab:optregfinal", caption="Final Regression Model for Optimism")
print(opt.fin.xtab , table.placement="ht")
@


%% The model was significant, F(15, 528) = \Sexpr{round(opt.fin.sum[["fstatistic"]], 3)}, and the adjusted $R^2$ was equal to \Sexpr{round(opt.fin.sum[["adj.r.squared"]],3)}
%% The model seems to show that the only important predictors of Optimism in the dataset are General Health, Mindfulness, Emotional Well Being and Age.  This corroborates the plots that were shown above for these variables. The strongest predictor was emotional well being, with age and general health being of similiar magnitude. 

%% The residuals were homoscedastic and normally distributed, meeting the assumptions of the model.

As has been shown above, all three kinds of regression models agreed that emotional well being and Age were important, and general health tended not to be removed from any of the models, though it did not have a particularly large coefficient. Surprisingly enough, stepwise selection performed better on unseen data than either lasso or ridge regression, suggesting that overfitting did not occur in this case. 




\subsection{Mindfulness Regressions}

A similiar procedure as described above for optimism was employed
in the midfulness regressions.  The results are shown below.


<<mindsetup, echo=FALSE, results=hide, cache=TRUE>>=
mind.ind <- with(hom1.full, createDataPartition(mindfulness, p=0.8, times=1, list=FALSE))
hom1.mind.train <- hom1.full[mind.ind, ]
hom1.mind.test <- hom1.full[-mind.ind,]
mind.train <- with(hom1.mind.train, mindfulness)
mind.pred.train <- hom1.mind.train[,c(4,67:73,75)]
mind.test <- with(hom1.mind.test, mindfulness)
mind.pred.test <- hom1.mind.test[,c(4,67:73,75)]
@ 


<<mindlasso, echo=FALSE, results=tex>>=
mind.lasso <- penalisedRegression(x=mind.pred.train, y=mind.train,  testdata=as.matrix(mind.pred.test), alpha=1, type="coefficients")
print(xtable(as.matrix(mind.lasso), caption="Coefficients on Unseen Data, Mindfulness Regression (lasso penalisation)", label="tab:hom1mindlasso"))
mind.lasso.pred <- penalisedRegression(x=mind.pred.train, y=mind.train,  testdata=as.matrix(mind.pred.test), newy=mind.test, alpha=1, type="response")
@ 

As shown in Table \ref{tab:hom1mindlasso}, the largest coefficient on mindfulness was optimism, and in line with the correlations seen above, it is negative. Energy Fatigue and Emotional Well being appear to be important in this particular model. 

Next, a ridge regression model was fit to this data. 

<<mindridge, echo=FALSE, results=tex>>=
mind.ridge <- penalisedRegression(x=mind.pred.train, y=mind.train,  testdata=as.matrix(mind.pred.test), alpha=0, type="coefficients")
print(xtable(as.matrix(mind.ridge), caption="Coefficients on Unseen Data, Mindfulness Regression (ridge penalisation)", label="tab:hom1mindridge"))
mind.ridge.pred <- penalisedRegression(x=mind.pred.train, y=mind.train,  testdata=as.matrix(mind.pred.test), newy=mind.test, alpha=0, type="response")
@ 

Table \ref{tab:hom1mindridge} shows the coefficients for the ridge regression, and they are almost identical to those of the lasso (at reasonable levels of precision). Next, a stepwise selection model procedure was employed on the dataset. 

<<maasregfirst, echo=FALSE, results=hide, cache=TRUE>>=
maas.reg.first <- lm(mindfulness~ generalhealth + optimism + pain+ socialfunctioning+physfun+rolelim+rolelimem+emwellbeing+energyfat+Age, data=hom1.mind.train)
mind.step <- stepAIC(maas.reg.first, data=hom1.mind.train, direction="both", k=3)
mind.step.test <- lm(mindfulness~rolelim+rolelimem+emwellbeing+energyfat, data=hom1.mind.test)

mind.step.pred <- predict(mind.step, hom1.mind.test, type="response")
mind.step.pred.obs <- data.frame(pred=mind.step.pred, obs=hom1.mind.test[["optimism"]])
@

<<mindstepprint, echo=false, results=tex>>=
print(xtable(summary(mind.step.test), caption="Coefficients for Stepwise Selected Regression on Mindfulness", label="tab:hom1mindsteptest"))
@ 

As can be seen from Table \ref{tab:hom1mindsteptest}, the stepwise selection process retained role limitation, emotional role limitations, emotional well being and energy fatigue. In contrast to the lasso and ridge fits, the model selected using this method did not include optimism. When the correlations between predicted and observed values were measured, all three model fits had correlations with the observed variables of approximately 0.44. However, in the case of the stepwise model, this correlation was negative. This unexpected result would seem to imply that this model is not particularly useful, and so either the lasso or ridge would appear to be the best model in terms of this data. 




\subsection{Health Regressions}

<<healthsetup, echo=FALSE, results=hide, cache=TRUE>>=
hom1.health.train.ind <- with(hom1.full, createDataPartition(generalhealth, p=0.75, list=FALSE))
hom1.health.train <- hom1.full[hom1.health.train.ind,]
hom1.health.test <- hom1.full[-hom1.health.train.ind,]
healthpred.train <- hom1.health.train[,c(4, 66:72, 74:75)]
health.train <- with(hom1.health.train, generalhealth)
healthpred.test <- hom1.health.test[,c(4, 66:72, 74:75)]
health.test <- with(hom1.health.test, generalhealth)
@ 


<<healthregfirst, echo=FALSE, results=hide, cache=TRUE>>=
health.mod1<-lm(generalhealth~mindfulness+physfun+optimism+energyfat+emwellbeing+Age+pain+Status+socialfunctioning+rolelim+rolelimem, na.action="na.omit", data=hom1.health.train)
health.step <- stepAIC(health.mod1, direction="both", k=3)
health.step.test <- lm(generalhealth~optimism+energyfat+rolelim, data=hom1.health.test)

@

<<healthregfirstprint, echo=FALSE, results=tex>>=
print(xtable(summary(health.step.test), caption="Coefficients for General Health Regression Model with predictors chosen by Stepwise Selection on unseen data", label="tab:hom1healthsteptest"))
@ 
As can be seen from Table \ref{tab:hom1healthsteptest}, optimism, role limitations and energy fatigue were retained as predictors in the model. However, only energy fatigue was a significant predictor in the test sample, though the coefficient for the effect of optimism was in line with the correlations and previous analyses carried out above. 

Next, a lasso regression model was fit to the same dataset. 


<<healthlasso, echo=FALSE, results=tex>>=
health.lasso <- penalisedRegression(x=as.matrix(healthpred.train), y=health.train,  testdata=healthpred.test, newy=health.test, alpha=1, type="coefficients")
health.lasso.pred <- penalisedRegression(x=as.matrix(healthpred.train), y=health.train,  testdata=healthpred.test, newy=health.test, alpha=1, type="response")
print(xtable(as.matrix(health.lasso), caption="Coefficients for Lasso Regression on General Health on unseen data", label="tab:hom1healthlasso"))
@ 

As can be seen from Table \ref{tab:hom1healthlasso}, optimism appears to be the best predictor in this case, along with energy fatigue and role limitations, supporting the set of predictors chosen by stepwise selection above. 

<<healthridge, echo=FALSE, results=tex>>=
health.ridge <- penalisedRegression(x=as.matrix(healthpred.train), y=health.train,  testdata=healthpred.test, newy=health.test, alpha=0, type="coefficients")
health.ridge.pred <- penalisedRegression(x=as.matrix(healthpred.train), y=health.train,  testdata=healthpred.test, newy=health.test, alpha=0, type="response")
print(xtable(as.matrix(health.ridge), caption="Coefficients for Ridge Regression on General Health on unseen data", label="tab:hom1healthridge"))
@ 

As can be seen from Table \ref{tab:hom1healthridge}, thee model for ridge regression is quite similiar, save that many of the coefficients are not shrunk nearly as much using this methodolody. Nonetheless, the overall magnitudes and relationships between the sizes of coefficients are quite similiar. 



\section{Psychometric Analyses}
<<scaleitems, echo=FALSE, results=hide, cache=TRUE>>=
## rand.grep <- grep("^RAND", x=names(hom1))
## randitems <- hom1[,rand.grep]
maas.grep <- grep("^MAASQ", x=names(hom1))
maasitems <- hom1[,maas.grep]
lotr.grep <- grep("^LOTRQ", x=names(hom1))
lotritems <- hom1[,lotr.grep]
@

<<calcalpha, echo=FALSE, results=hide>>=
maas.alpha <- alpha(maasitems)
lotr.alpha <- alpha(lotritems)
@ 


Alpha was calculated for the MAAS ($\alpha=0.88$) and for the LOT-R ($\alpha=0.78$). Alpha for all RAND subscales was above 0.7, with the exception of Social Functioning. 

\subsection{Number of Factors to retain}


<<retainfactors, echo=FALSE, results=hide, cache=TRUE, eval=FALSE>>=
sink("tmp.txt")
rand.nscree <- nScree(x=cor(na.omit(randitems.scored), use="pairwise.complete.obs"), model="factors")
sink(NULL)
@

<<retainfactorsprint, echo=FALSE, results=hide, eval=FALSE>>=
print(xtable(summary(rand.nscree), label="tab:randretain", caption="Comparison of Criteria to retain factors"))
@ 

The parallel analysis criterion and the kaiser criterion suggest eight factors. These two measures may be picking up on the higher order factor structure of the items, as the RAND is typically modelled as having two higher order factors (physical and mental health).

%% Another approach which can be applied to select the number of factors is a cross-validation method.

<<randfactorcv, echo=FALSE, results=hide, cache=TRUE, eval=FALSE>>=
rand.fact.gabriel <- cv.svd.gabriel(na.omit(randitems.scored), krow=2, kcol=2, maxrank=18)
rand.fact.wold <- cv.svd.wold(na.omit(randitems.scored), k=10, maxrank=12)
@ 

<<randcvwold, echo=FALSE, results=hide, eval=FALSE>>=
print(xtable(Svdcv(rand.fact.wold, label="tab:randfactwold", caption="Estimate of number of factors to retain based on 10-fold Cross Validation")))
@ 

%% It can be seen from Table \ref{tab:randfactwold} that the Wold method of cross validation suggests that four factors should be retained for further analysis. This seems in line with the results of the other criteria.



<<randcvgabriel, echo=FALSE, results=hide, eval=FALSE>>=
print(Svdcv(rand.fact.gabriel, label="tab:randfactgabriel", caption="Estimate of number of factors to retain based on leave out 2 item/subject Cross-validation"))
@

%% It can be seen from Table \ref{tab:randfactgabriel} that the Gabriel method of cross-validation suggests that there are thirteen factors which underlie this scale. This seems relatively implausible, but it will be tested. 

To summarise, two, four, eight  factor structures will be examined for the RAND MOS and their performance assessed on unseen data (from sample 2) to determine which of these provides the best fit. 

Next, the various metrics for determining the number of factors were applied to the Mindfulness scale. 

<<retainmaas, echo=FALSE, results=hide, cache=TRUE, eval=FALSE>>=
sink("tmp.txt")
maas.nscree <- nScree(x=cor(na.omit(maasitems), use="pairwise.complete.obs"), model="factors")
sink(NULL)
@ 


<<printretainmaas, echo=FALSE, results=hide, eval=FALSE>>=
print(xtable(summary(maas.nscree), label="tab:maasretain", caption="Comparison of Criteria to retain factors, MAAS"))
@ 

Looking at the eigenvalues, it is clear that one factor explains the majority of the variance. However, both the Kaiser criterion and parallel analysis suggest that three factors should be retained, while the VSS and the Minimum Average Partial criterion procedures suggest a one factor solution.

<<maasfactorcv, echo=FALSE, results=hide, eval=FALSE>>=
maasitems <- as.data.frame(lapply(maasitems, as.numeric))
maas.fact.gabriel <- cv.svd.gabriel(na.omit(maasitems), krow=2, kcol=2, maxrank=7)
maas.fact.wold <- cv.svd.wold(na.omit(maasitems), k=10, maxrank=7)
@


<<maascvwold, echo=FALSE, results=hide, eval=FALSE>>=
print(Svdcv(maas.fact.wold, label="tab:maasfactwold", caption="Estimate of number of factors to retain based on 10-fold Cross Validation, MAAS Sample One"))
@ 

%% The Wold method of cross-validation, (shown in Table \ref{tab:maasfactwold}), agrees with the other criteria in suggesting that one factor should be retained. 


<<maascvgabriel, echo=FALSE, results=hide, eval=FALSE>>=
print(Svdcv(maas.fact.gabriel, label="tab:maasfactgabriel", caption="Estimate of number of factors to retain based on leave out 2 item/subject Cross-validation, MAAS Sample One"))
@ 

%% It can be seen from Table \ref{tab:maasfactgabriel} that the Wold method of cross-validation suggests that there are seven factors which underlie this scale. This seems relatively implausible, but it will be tested. 


One and three  factor solutions were examined and their performance assessed on unseen data. 


<<retainlotr, echo=FALSE, results=hide, cache=TRUE, eval=FALSE>>=
lotr.nscree <- nScree(x=cor(na.omit(lotritems), use="pairwise.complete.obs"), model="factors")
@ 


<<lotrprint, echo=FALSE, results=hide, eval=FALSE>>=
print(xtable(summary(lotr.nscree), label="tab:lotrretain", caption="Comparison of Criteria to retain factors, LOTR, Sample One"))

@ 

All of the methods for determining the correct number of factors to retain suggest that a one factor solution fits the data matrix best. 

<<lotrfactorcv, echo=FALSE, results=hide, cache=TRUE>>=
lotritems <- as.data.frame(lapply(lotritems, as.numeric))
lotr.fact.gabriel <- cv.svd.gabriel(na.omit(lotritems), krow=3, kcol=3, maxrank=3)
lotr.fact.wold <- cv.svd.wold(na.omit(lotritems), k=10, maxrank=3)
@


<<lotrcvwold, echo=FALSE, results=hide,eval=FALSE>>=
print(Svdcv(lotr.fact.wold, label="tab:lotrfactwold", caption="Estimate of number of factors to retain based on 10-fold Cross Validation"))
@ 

%% The Wold method of cross-validation, (shown in Table \ref{tab:lotrfactwold}), agrees with the other criteria in suggesting that one factor should be retained. 


<<lotrcvgabriel, echo=FALSE, results=hide, eval=FALSE>>=
print(Svdcv(lotr.fact.gabriel, label="tab:lotrfactgabriel", caption="Estimate of number of factors to retain based on leave out 2 item/subject Cross-validation"))
@ 

%% It can be seen from Table \ref{tab:lotrfactgabriel} that the Gabriel method of cross-validation suggests that there are three factors which underlie this scale. This seems relatively implausible, but it will be tested. 

Despite all of the selection criteria pointing towards a one factor solution, one and two factor solutions will be assessed for the LOT-R, and their performance evaluated on unseen data. 

<<mapall, echo=FALSE, results=hide, cache=TRUE, eval=FALSE>>=
MAP.rand<-VSS(na.omit(randitems.scored), n=12, rotate="oblimin", fm="ml", plot=FALSE )
MAP.maas<-VSS(na.omit(maasitems),rotate="oblimin", fm="ml", plot=FALSE )
MAP.lotr<-VSS(na.omit(lotritems),rotate="promax", fm="gls", plot=FALSE )
@

%As can be seen from Figure ~\ref{fig:rand1}, the 8 factor structure was replicated.  However, the MAP criterion suggests a four factor solution, so both of these proposed solutions were examined and tested.




\subsection{RAND MOS}
\label{sec:rand-mos-1}
Two, four and eight  factor solutions were extracted and interpreted from the RAND MOS items. 

<<rand2fact, echo=FALSE, results=hide>>=
rand.fact.2<-fa(na.omit(randitems.scored), 2,fm="ml", rotate="promax")
print(FactorXtab(rand.fact.2, names=c("PhysEmHealth", "PhysicalFunc"),label="tab:rand2fact", caption="Factor Loadings, RAND MOS Two Factor Solution, Sample One"))
@
The results of the two factor solution (not shown) were as follows:
PA1: "RANDQ1",  "RANDQ14", "RANDQ16" ,"RANDQ17" ,"RANDQ18" ,"RANDQ19" ,"RANDQ20" ,"RANDQ21" ,"RANDQ22" ,"RANDQ23" ,"RANDQ24" ,"RANDQ25" ,"RANDQ26" ,"RANDQ27"
,"RANDQ28" ,"RANDQ29" ,"RANDQ30" ,"RANDQ31" ,"RANDQ32" ,"RANDQ33" ,"RANDQ34"
,"RANDQ35" ,"RANDQ36". Essentially this factor appears to contain all of the scales except for Physical Functioning, which loads on Factor 2. We can best term this factor as General and Emotional Health.

PA2:"RANDQ3"  "RANDQ4"  "RANDQ5"  "RANDQ6"  "RANDQ7"  "RANDQ8"  "RANDQ9"  "RANDQ10" "RANDQ11" "RANDQ12". This factor maps exactly to the Physical Functioning Scale, and so retains that name.



The non-normed fit index was equal to \Sexpr{round(rand.fact.2[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(rand.fact.2[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(rand.fact.2[["RMSEA"]][2],3 )} to \Sexpr{round(rand.fact.2[["RMSEA"]][3],3 )}.

This factor solution does not appear to be useful, as it has extremely low fit indices (NNFI=0.69), and the breakdown of the factors is rather strange. If the factors had broken down in terms of Physical and Mental Health, then this would have made more sense. The factor loadings were invariant under a number of rotations (varimax, oblimin and promax), so it appears to be a real (if less than interpretable) factor structure. 

<<rand2corr, echo=FALSE, results=hide, eval=FALSE>>=
print(xtable(rand.fact.2$r.scores, label="tab:hom1rand4corr", caption="Factor Correlations, RAND MOS Two Factor Solution, Sample One"))
@

The factor correlations were quite low for this solution, at 0.17. This suggests that an orthogonal rotation might be more appropriate, but attempting this did not change any of the loadings. 



\paragraph{Rand MOS 4 Factor Solution}

<<rand4fact, echo=FALSE, results=hide>>=
rand.fact.4<-factor.pa(na.omit(randitems.scored), 4, rotate="oblimin")
print(FactorXtab(rand.fact.4, names=c("PhysFunc", "SocEmFunc", "PhysLim", "GenHealth"), label="tab:rand4fact", caption="Four Factor Solution, RAND MOS, Sample One (Oblimin Rotation)"))
@

The loadings on the four factor solution (not shown) broke down as follows. 
PA2: "RANDQ3" , "RANDQ4" , "RANDQ5" , "RANDQ6" , "RANDQ7" , "RANDQ8" , "RANDQ9" , "RANDQ10" ,"RANDQ11" ,"RANDQ12". This factor maps exactly to the Physical Functioning scale, and so retains that name. 

PA1: "RANDQ17" ,"RANDQ18" ,"RANDQ19" ,"RANDQ20" ,"RANDQ23" ,"RANDQ24" ,"RANDQ25" ,"RANDQ26" ,"RANDQ27" ,"RANDQ28" ,"RANDQ29" ,"RANDQ30" ,"RANDQ31" ,"RANDQ32". This factor maps to the emotional role limitations, social functioning and emotional well being scales, and so can probably best be termed as Social and Emotional Functioning. 

PA3: "RANDQ13" ,"RANDQ14" ,"RANDQ15" ,"RANDQ16" ,"RANDQ21" ,"RANDQ22". This factor maps to the role limitations and pain sub-scales, and so can probably best be termed as physical limitations. 

PA4: "RANDQ1" , "RANDQ3" , "RANDQ21" ,"RANDQ27" ,"RANDQ33" ,"RANDQ34" ,"RANDQ35" ,"RANDQ36". This scale maps to the General Health scale, with one item from the Physical Functioning and one item from Energy/Fatigue (27). However, both these items have higher loadings on other factors, and so this factor can probably best be termed as General Health. 



The non-normed fit index was equal to \Sexpr{round(rand.fact.4[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(rand.fact.4[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(rand.fact.4[["RMSEA"]][2],3 )} to \Sexpr{round(rand.fact.4[["RMSEA"]][3],3 )}.

The fit indices are somewhat better for this solution than for the two factor solution, though the NNFI is still quite low. The RMSEA is somewhat too high for comfort, also. 

<<rand4corr, echo=FALSE, results=hide, eval=FALSE>>=
print(xtable(rand.fact.4[["r.scores"]], label="tab:hom1rand4corr", caption="Factor Correlations, RAND MOS Four Factor Solution, Sample One"))
@

The factor correlations are quite low in this solution also, though they are all high enough to retain the oblique rotations.




\paragraph{RAND MOS 8 Factor Solution}

<<rand8fact, echo=FALSE, results=tex>>=
rand.fact.8<-factor.pa(na.omit(randitems.scored), 8, rotate="oblimin")
print(FactorXtab(rand.fact.8, names=c( "PhysFunc", "SocEmWB", "GenHealth", "EmRoleLim", "RoleLim", "Fatigue","Pain", "Energy"), label="tab:tcq1rand8fact", caption="Factor Loadings Eight Factor Solution, RAND MOS, Sample One"), scalebox=0.7)
@

PA2: "RANDQ3" , "RANDQ4" , "RANDQ5" , "RANDQ6" , "RANDQ7" , "RANDQ8" , "RANDQ9" , "RANDQ10" ,"RANDQ11" ,"RANDQ12". Again, the first factor extracted maps exactly to the Physical Functioning scale, and retains that name. 

PA1: "RANDQ20" ,"RANDQ23" ,"RANDQ24" ,"RANDQ25" ,"RANDQ26" ,"RANDQ27" ,"RANDQ28" ,"RANDQ30" ,"RANDQ32". This scale maps to the Social Functioning, and the Emotional Well Being Scale. There are some items taken from the energy faitigue scale, and as these are the positively worded items, this scale can probably best be termed as Social and Emotional Well Being. 

PA4: "RANDQ1" , "RANDQ33" ,"RANDQ34" ,"RANDQ36". These items map exactly to the General Health scale. Item 35 also loads on this scale, and as its loading was 0.29 while the cutoff was 0.30, it can be best characterised as part of that scale. Therefore, this factor can be best termed as General Health.

PA7: "RANDQ17" ,"RANDQ18" ,"RANDQ19" ,"RANDQ20". This scale maps to the Emotional Role Limitations and one item (20) from the Social Functioning scale (which asks about social events that have been missed due to health problems) and so this can probably best be termed as Emotional Role Limitations.

PA3: "RANDQ13" ,"RANDQ14" ,"RANDQ15" ,"RANDQ16". This scale maps exactly to the Role Limitations sub-scale, and so retains that name. 

PA5: "RANDQ29" ,"RANDQ31". These items are the negative items from the Energy/Fatigue scale, and so this factor can probably best be termed as Fatigue. 

PA6: "RANDQ21" ,"RANDQ22". These items map exactly to the Pain scale, and so retain that name. 

PA8: "RANDQ2" , "RANDQ23" ,"RANDQ27". The loadings on this factor are all below .4, which argues against its unproblematic interpretation. In addition, Q2 loads on this factor, when it is not typically associated with any factor. The other two items are the positively worded items from the  Energy/Fatigue scales, and so this factor can best be termed Energy. 



The non-normed fit index was equal to \Sexpr{round(rand.fact.8[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(rand.fact.8[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(rand.fact.8[["RMSEA"]][2],3 )} to \Sexpr{round(rand.fact.8[["RMSEA"]][3],3 )}.

This factor structure definitely makes sense, and the fit indices are relatively acceptable, although the RMSEA is a little higher than would be wanted. In addition, the items map quite well to the subscales, which further reinforces our confidence in this solution.


<<rand8corr, echo=FALSE, results=hide, eval=FALSE>>=
print(xtable(rand.fact.8[["r.scores"]], label="tab:hom1rand8corr", caption="Factor Correlations, Eight Factor Solution RAND MOS, Sample One"))
@

The factor correlations are moderate and in line with expectations. The correlations are definitely too high to use an orthogonal rotation. 




\paragraph{CFA for RAND MOS}


<<rand2sem, echo=FALSE, results=hide, cache=TRUE>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "General and Emotional Health")
physfun <- paste(rand, c(3:12), sep="")
genemhealth <- paste(rand,c(1,14,17:36), sep="")
Rand2model <- mxModel(name="RAND2Samp1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="General and Emotional Health", to=genemhealth),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=364)
                      )
rand2fit <- mxRun(Rand2model)
rand2summ <- summary(rand2fit)
@

<<rand4sem, echo=FALSE, results=hide, cache=TRUE>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "Social and Emotional Functioning", "General Health", "Physical Limitations")
physfun <- paste(rand, c(3:12), sep="")
genhealth <- paste(rand, c(1,33:36), sep="")
physlim <- paste(rand, c(13:16,21:22), sep="")
emsocfunc <- paste(rand, c(17:20, 23:32), sep="")
Rand4model <- mxModel(name="RAND4Samp1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="Social and Emotional Functioning", to=emsocfunc),
                      mxPath(from="General Health", to=genhealth),
                      mxPath(from="Physical Limitations", to=physlim),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=364)
                      )
rand4fit <- mxRun(Rand4model)
rand4summ <- summary(rand4fit)
@

<<rand8sem, echo=FALSE, results=hide, cache=TRUE>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "Social and Emotional Wellbeing", "General Health", "Emotional Role Limitations", "Role Limitations", "Fatigue", "Pain", "Energy")
physfun <- paste(rand, c(3:12), sep="")
socemwellbeing <- paste(rand, c(20,23:28, 30, 32), sep="")
genhealth <- paste(rand, c(1,33:36), sep="")
emrolelim <- paste(rand, c(17:20), sep="")
rolelim <- paste(rand, c(13:16), sep="")
energy <- paste(rand, c(29, 31), sep="")
pain <- paste(rand, c(21:22), sep="")
fatigue <- paste(rand, c(2, 23, 27), sep="")
Rand8model <- mxModel(name="RAND8Samp1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="Social and Emotional Wellbeing", to=socemwellbeing),
                      mxPath(from="General Health", to=genhealth),
                      mxPath(from="Emotional Role Limitations", to=emrolelim),
                      mxPath(from="Role Limitations", to=rolelim),
                      mxPath(from="Energy", to=energy),
                      mxPath(from="Pain", to=pain),
                      mxPath(from="Fatigue", to=fatigue),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=364)
                      )
rand8fit <- mxRun(Rand8model)
rand8summ <- summary(rand8fit)
@



<<randsemcompare, echo=FALSE, results=tex>>=
randsemcomp <- mxCompare(base=rand2fit, comparison=c(rand4fit, rand8fit), all=TRUE)
print(xtable(randsemcomp,label="tab:randsemcompare", caption="SEM Comparison for RAND MOS Factor Solutions, Sample One"), scalebox=0.8, table.placement="ht")
@

As can be seen from \ref{tab:randsemcompare}, the 8 factor solution appears to fit better (lower AIC), so on the basis of this analysis, this is the solution which should be retained.

\section{Mindfulness Attention Awareness Scale}

For the MAAS, parallel analysis, MAP, VSS, Kaisers rule methods suggested a one factor solution, while other methods suggested a three factor solution. Therefore, one and three factor solutions were extracted and the results interpreted, as shown below.

%% \subsection{MAAS One Factor Solution}
%% \label{sec:maas-one-factor}

<<maas1fact, echo=FALSE, results=tex>>=
maas.fact.1<-factor.pa(na.omit(maasitems), 1, rotate="oblimin")
print(FactorXtab(maas.fact.1, names=c("Mindfulness"),label="tab:hom1maas1fact", caption="Factor Loadings, One Factor Solution, MAAS, Sample One"))
@

The results of the one factor solution for the MAAS are shown in Table \ref{tab:hom1maas1fact}. 

The non-normed fit index was equal to \Sexpr{round(maas.fact.1[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(maas.fact.1[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(maas.fact.1[["RMSEA"]][2],3 )} to \Sexpr{round(maas.fact.1[["RMSEA"]][3],3)}.



%% \subsection{MAAS Three Factor Solution}
%% \label{sec:maas-seven-factor}

The next factor solution to be examined was the three factor solution (loadings not shown). 


<<maas7fact, echo=FALSE, results=hide>>=
maas.fact.3<-factor.pa(na.omit(maasitems), 3, rotate="oblimin")
print(FactorXtab(maas.fact.3, names=c("LackAware", "PhysUnaware", "LackPresAttention"), label="tab:tcq1maas3fact", caption="Factor Loadings, Three Factor Solution, MAAS, Sample One"))
@


PA1: "MAASQ8"  "MAASQ10" "MAASQ11" "MAASQ12" "MAASQ13" "MAASQ14" "MAASQ15"
All of these questions relate to lack of awareness, and so this factor can best be termed this. 
PA3: "MAASQ4" "MAASQ5" "MAASQ6" "MAASQ7" "MAASQ8" "MAASQ9"
These mostly relate to sensations of physical unawareness, and so this factor can best be termed physical unawareness. 
PA2: "MAASQ1"  "MAASQ2"  "MAASQ3"  "MAASQ14". This factor can perhaps best be termed as lack of present attention.



The non-normed fit index was equal to \Sexpr{round(maas.fact.3[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(maas.fact.3[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(maas.fact.3[["RMSEA"]][2],3 )} to \Sexpr{round(maas.fact.3[["RMSEA"]][3],3)}.




\subsection{CFA for MAAS}
\label{sec:cfa-maas}

<<maas3sem, echo=FALSE, results=hide, cache=TRUE>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Lack Present Awareness", "Lack of Awareness", "Physical Unawareness")
lackaware <- paste(maas, c(8, 10:15), sep="")
lackpresaware <- paste0(maas, c(4:9))
physunawareness <- paste0(maas, c(1:3, 14))

Maas3model <- mxModel(name="MAAS3",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Lack Present Awareness"
                             ,to=lackpresaware),
                      mxPath(from="Lack of Awareness", to=lackaware),
                      mxPath(from="Physical Unawareness", to=physunawareness),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems)), type="cov", numObs=364)
                      )
maas3fit <- mxRun(Maas3model)
maas3summ <- summary(maas3fit)
@




<<maas1fit, echo=FALSE, results=hide, cache=TRUE>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Unawareness")
unawareness <- paste(maas, c(1:15), sep="")
Maas1model <- mxModel(name="MAAS1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Unawareness", to=unawareness),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems)), type="cov", numObs=364)
                      )
maas1fit <- mxRun(Maas1model)
maas1summ <- summary(maas1fit)
@

<<maassemcompare, echo=FALSE, results=tex>>=
maascomp <- mxCompare(base=maas1fit, comparison= maas3fit)
maascomp.xtab <- xtable(maascomp,label="tab:maassemcomp")
print(maascomp.xtab)
                      
@

Factor solutions for one and three factors were extracted, and the results were subjected to CFA.

From Table \ref{tab:maassemcomp} it can be seen that the best model is the one factor model, which is in line with previous research.
The factor structure is not reported here as all factors loaded on the first factor.  This factor explained 35\% of the variance
in the sample, which is low.  Possible explanations for this are discussed below. 


\section{Life Orientation Test, Revised}

Parallel Analysis indicated that two factors should be extracted, while the MAP criterion suggested one.  Therefore, both one and two factor solutions were extracted from the matrix and their results examined for adequacy and interpretability.

%% \subsection{LOTR One Factor Solution}
%% \label{sec:lotr-one-factor}


<<lotr1fact, echo=FALSE, results=tex>>=
lotr.fact.1<-factor.pa(na.omit(lotritems), 1, rotate="oblimin")
print(FactorXtab(lotr.fact.1, names=c("Optimism"),label="tab:hom1lotr1fact", caption="Factor Loadings, One Factor Solution, LOT-R, Sample One"))
@

Table \ref{tab:hom1lotr1fact} shows the loadings for the one factor solution. The communalities are relatively high, except for question one which is a sign that perhaps this solution is not optimal. 


The non-normed fit index was equal to \Sexpr{round(lotr.fact.1[["TLI"]], 3)}
and the RMSEA was equal to \Sexpr{round(lotr.fact.1[["RMSEA"]][1],3 )},with confidence intervals from \Sexpr{round(lotr.fact.1[["RMSEA"]][2],3)} to \Sexpr{round(lotr.fact.1[["RMSEA"]][3],3 )}.  This solution does not seem optimal, as the RMSEA is well outside the recommended bounds, and the NNFI is quite low.  %% In addition, the communalities are not very high, with over half the variance being left out of the solution.




%% \subsection{LOTR Two Factor Solution}
%% \label{sec:lotr-two-factor}

<<lotr3fact, echo=FALSE, results=hide>>=
lotr.fact.2<-factor.pa(na.omit(lotritems), 2, rotate="oblimin")
print(FactorXtab(lotr.fact.2, names=c("Pessimism", "Optimism"), label="tab:tcq1lotr2fact", caption="Factor Loadings, Two Factor Solution, LOT-R, Sample One"))
@
The two factor solution (not shown) broke down as follows:
PA2: ''LOTRQ1'',  ``LOTRQ4'',  ``LOTRQ10''. These items are all the positively framed items, and so this factor can best be termed as Optimism. 

PA1: ``LOTRQ3'', ``LOTRQ7'', ``LOTRQ9'', ``LOTRQ10'' This factor, and the next, consist of the pessimism items, and so can best be termed Pessimism. Note that Item 10 has extremely poor loadings on both factors, which is surprising given that it can often be taken as an indicator for the entire construct. 


\subsection{CFA for LOTR}
\label{sec:cfa-lotr}

<<lotr1sem, echo=FALSE, results=hide, cache=TRUE>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism")
optimism <- paste(lotr, c(1,3,4,7,9,10), sep="")
Lotr1model <- mxModel(name="LOTR1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems)), type="cov", numObs=364)
                      )
lotr1fit <- mxRun(Lotr1model)
lotr1summ <- summary(lotr1fit)
@


<<lotr3sem, echo=FALSE, results=hide, cache=TRUE>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism", "Pessimism")
optimism <- paste(lotr, c(1,4,10), sep="")
pessimism <- paste(lotr, c(3,7, 9, 10), sep="")
Lotr2model <- mxModel(name="LOTR2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from="Pessimism", to=pessimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems)), type="cov", numObs=364)
                      )
lotr2fit <- mxRun(Lotr2model)
lotr2summ <- summary(lotr2fit)
@

<<lotrcompare, echo=FALSE, results=tex>>=
lotrcomp <- mxCompare(base=lotr1fit, comparison=lotr2fit)
lotrcomp.xtab <- xtable(lotrcomp,label="tab:semlotrcomp", caption="Comparison of CFA for the LOT-R")
print(lotrcomp.xtab)
@

As can be seen from Table \ref{tab:semlotrcomp}, the one factor solution provided the best fit to the data.  Therefore, this solution will be tested on the second sample.

 \section{Item Response Theory Analyses}
\label{sec:item-response-theory}


\subsection{MAAS}
\label{sec:maas}



The next instrument examined was the MAAS.  Firstly, the instrument was examined using Mokken analysis to check if it could be considered one scale, and whether or not there were violation of monotonicity.

<<maasassumptioncheck, echo=FALSE, results=hide>>=
maas.scales <- aisp(na.omit(maasitems))
print(xtable(as.matrix(maas.scales),label="tab:maasassumptioncheck", caption="Item Attribution to Scales, MAAS, Sample One"))
@

The mokken analysis suggests that two items should be dropped from the scale, items 2 and 6. This leaves a thirteen item scale for further analysis. There were no violations of the monotonicity assumption for the reduced scale. The item coefficients (ItemH) are quite low, many of them hang around 0.30, which is the minimum allowed. 

<<maasreduced, echo=FALSE, results=hide, cache=TRUE>>=
maas.irt <- paste(maas, c(1,3:5,7:15), sep="")
maas.irt <- maasitems[,maas.irt]
@

Next, item ordering was examined for this scale.

<<maasitemord, echo=FALSE, results=hide, cache=TRUE>>=
maas.iio <- check.iio(na.omit(maas.irt))
print(xtable(maas.iio[["violations"]],label="tab:maasitemord", caption="Invariant Item Ordering Check Results for MAAS"))
@


<<maasmonotonicity, echo=FALSE, results=hide, cache=TRUE>>=
maas.mono <- check.monotonicity(na.omit(maas.irt))
print(xtable(summary(maas.mono),label="tab:maasmono", caption="Monotonicity Check Results for MAAS"))
@

<<maasitemspcm, echo=FALSE, results=hide, cache=TRUE>>=
maas.pcm.rasch <- gpcm(na.omit(maas.irt), constraint="rasch")
maas.pcm.1pl <- gpcm(na.omit(maas.irt), constraint="1PL")
maas.pcm.2pl <- gpcm(na.omit(maas.irt), constraint="gpcm")
@

<<maasirtraschprint, echo=FALSE, results=hide>>=
print(xtable(coef(maas.pcm.rasch), label="tab:maaspcmrasch", caption="MAAS Partial Credit Model Coefficients, Rasch"))
@ 

%% As can be seen from Table \ref{tab:maaspcmrasch}, the Rasch partial credit model is not a good fit to the data. There are numerous violations of the increasing ability scores assumption. Next, a one parameter model (with discrimination estimated from the data) was fitted to the maas items. 

<<maasirtraschprint, echo=FALSE, results=hide>>=
print(xtable(coef(maas.pcm.1pl), label="tab:maaspcm1pl", caption="MAAS Partial Credit Model Coefficients, One Parameter Model"))
@ 

%% The estimated one parameter model is shown in Table \ref{tab:maaspcm1pl}, and it can clearly be seen that it suffers from the same problems as the rasch model fitted above. Finally, a two parameter partial credit model was fitted to these items. 

<<maasirtraschprint, echo=FALSE, results=hide>>=
print(xtable(coef(maas.pcm.2pl), label="tab:maaspcm2pl", caption="MAAS Partial Credit Model Coefficients, Two Parameter Model"))
@ 

%% Table \ref{tab:maaspcm2pl} clearly shows that even the two parameter model does not provide a good fit to the data.
Three Partial Credit Models were fitted, but as they did not provide a good fit to the data, are not reported further here. 
The next stage of analysis for the Mindfulness scale was to fit one and two parameter Graded Response Models to the data. 

<<maasgrmfit, echo=FALSE, results=hide, cache=TRUE>>=
maas.grm.1pl <- grm(maas.irt, constrained=TRUE)
maas.grm.2pl <- grm(maas.irt, constrained=FALSE)
@

%% \begin{figure}
<<maasgrm1plplot, echo=FALSE, results=hide>>=
maasp <- ggplotGRM(maas.grm.1pl)
print(maasp)
@   
%%   \caption{MAAS Graded Response Model (One Parameter) Ability Thresholds}
%%   \label{fig:maasgrm1plplot}
%% \end{figure}

There were no obvious scaling violations for this scale. 

<<maasgrm1plprint, echo=FALSE, results=tex, cache=TRUE>>=
print(xtable(coef(maas.grm.1pl), caption="Coefficient Estimates for MAAS One Parameter Graded Response Model, Sample One", label="tab:maasgrm1pl"))
@ 

Table \ref{tab:maasgrm1pl} shows the estimated ability thresholds and discrimination parameter for the one parameter Graded Response Model on the MAAS. The discrimination parameter is moderate, as are the ability estimates, suggesting that this scale may not be suitable for respondents particularly high in mindfulness. 

Next, a two parameter Graded Response Model was examined for the same scale. 

\begin{figure}
<<maasgrm2plot, echo=FALSE, fig=TRUE>>=
maas2plp <- ggplotGRM(maas.grm.1pl)
print(maas2plp)
@   
  \caption{MAAS Graded Response Model (Two parameter) Item Ability Thresholds}
  \label{fig:maasgrm2plplot}
\end{figure}

<<maasgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas.grm.2pl), caption="Coefficient Estimates for MAAS Two Parameter Graded Response Model", label="tab:maasgrm2pl"))
@ 


Table \ref{tab:maasgrm2pl} shows the estimated coefficients for a two parameter Graded Response Model. IT can be seen that Q14 has the highest discriminatory power, and that Q11 has the highest ability threshold, while Q1 has the lowest. Q11 refers to listening to others while engaging in other tasks, and its ability estimates sugegst that it is a good question for pinpointing the abilities of respondents high on the construct of mindfulness. 

<<maasanovacomp, echo=FALSE, results=hide, eval=FALSE>>=
anova.rasch.maas.1pl <- anova.gpcm(maas.pcm.rasch, maas.pcm.1pl)
anova.1pl.maas.2pl <- anova.gpcm(maas.pcm.1pl, maas.pcm.2pl)
anova.rasch.maas.2pl <- anova.gpcm(maas.pcm.rasch, maas.pcm.2pl)
@

The three models were subjected to ANOVA comparison and the 1 parameter model was significantly ($p\le0.001$) better than the Rasch model, and the two parameter model was significantly better than the one parameter model ($p\le0.001$). However, the ultimate test is the ability of the model to predict out-of-sample data. 

\subsection{LOTR}
\label{sec:lotr}

The Life Orientation Test was the next instrument to be examined using the IRT approach.

Firstly, the scale analysis was conducted to determine which items fit best together.

<<lotrscales, echo=FALSE, results=hide>>=
lotr.scales <- aisp(na.omit(lotritems))
print(xtable(as.matrix(lotr.scales),label="tab:lotrscales"))
@

All of the items meet the assumptions of a unidimensional scale.  Next, the item orderings were examined.

<<lotritemord, echo=FALSE, results=hide, cache=TRUE>>=
lotr.iio <- check.iio(na.omit(lotritems))
print(xtable(lotr.iio[["violations"]],label="tab:lotritemord"))
@


Q1 needs to be removed from the scale in order to meet the assumptions of the model. There were no violations of monotonicity in the sample.

<<lotrmono, echo=FALSE, results=hide, cache=TRUE>>=
lotr.mono <- check.monotonicity(na.omit(lotritems))
print(xtable(summary(lotr.mono),label="tab:lotrmono"))
@


<<lotrreduced, echo=FALSE, results=hide, cache=TRUE>>=
lotr.paste <- paste(lotr, c(3,4,7,9,10), sep="")
lotr.irt <- lotritems[,lotr.paste]
@

After this process of model checking, a five item scale remains for further analysis. 

<<lotrmodelsirt, echo=FALSE, results=hide, cache=TRUE>>=
lotr.pcm.rasch <- gpcm(na.omit(lotr.irt), constraint="rasch")
lotr.pcm.1pl <- gpcm(na.omit(lotr.irt), constraint="1PL")
lotr.pcm.2pl <- gpcm(na.omit(lotr.irt), constraint="gpcm")
@

<<lotrpcmraschprint, echo=FALSE, results=hide>>=
print(xtable(coef(lotr.pcm.rasch), label="tab:lotrpcmrasch", caption="Rasch Partial Credit Model for Life Orientation Test, Revised"))
@ 

%% As shown in Table \ref{tab:lotrpcmrasch}, the model does not provide a good fit to the data. There are numerous violations of the ordering assumptions of the model. Next, a one parameter model was fit to the data. 

<<lotrpcm1plprint, echo=FALSE, results=hide>>=
print(xtable(coef(lotr.pcm.1pl), label="tab:lotrpcm1pl", caption="One parameter Partial Credit Model for Life Orientation Test, Revised"))
@ 

%% As can be seen in Table \ref{tab:lotrpcm1pl}, there are again some violations of the ability ordering assumption (LOTR4, category 3). Next, a two parameter PCM was fitted to the LOTR items. 

<<lotrpcm1plprint, echo=FALSE, results=hide>>=
print(xtable(coef(lotr.pcm.2pl), label="tab:lotrpcmgpcm", caption="Two Parameter Partial Credit Model for Life Orientation Test, Revised"))
@ 

%% The coefficients in Table \ref{tab:lotrpcmgpcm} show that again, LOTR4 ensures that the model does not fit correctly. 

<<lotranovacomp, echo=FALSE, results=hide, cache=TRUE>>=
anova.rasch.lotr.1pl <- anova.gpcm(lotr.pcm.rasch, lotr.pcm.1pl)
anova.1pl.lotr.2pl <- anova.gpcm(lotr.pcm.1pl, lotr.pcm.2pl)
anova.rasch.lotr.2pl <- anova.gpcm(lotr.pcm.rasch, lotr.pcm.2pl)
@

%% The results of the model comparison showed that the rasch model was not significantly different from the one parameter model ($p=0.862$), but that the two parameter model provided a significantly better fit to the data ($p \le 0.001$), even with a penalty for the extra parameters.

%% However, the difference in likelihoods was extremely small between the Rasch and two parameter models (20.15), and the BIC suggested that the Rasch model was a better fit.  One issue for the BIC is that it presumes that a true model exists amongst the candidate models, which is almost certainly not the case in this (or indeed any other psychological research) case.
Again, the fit of three partial credit models was not acceptable, and so they are not reported further here. 
The next stage in the analysis of the LOTR was the fitting of one and two parameter Graded Response Models. 

<<lotrgrmfit, echo=FALSE, results=hide, cache=TRUE>>=
lotr.grm.1pl <- grm(lotr.irt, constrained=TRUE)
lotr.grm.2pl <- grm(lotr.irt, constrained=FALSE)
@ 
\begin{figure}
<<lotrgrm1plplot, echo=FALSE, fig=TRUE>>=
lotr1plgrm <- ggplotGRM(lotr.grm.1pl)
print(lotr1plgrm)
@   
  \caption{One Parameter Graded Response Model for LOTR Item Ability Thresholds}
  \label{fig:lotr1plgrm}
\end{figure}

Figure \ref{fig:lotr1plgrm} shows the estimated threshold points for each of the categories for each item. It can be seen that there are no violations of item ordering, and that LOTR4 is the hardest item to endorse, while LOTR3 appears to be the easiest. 

\begin{figure}
<<lotr2plgrmplot, echo=FALSE, fig=TRUE>>=
lotr2plgrm <- ggplotGRM(lotr.grm.2pl)
print(lotr2plgrm)
@   
  \caption{Two Parameter Graded Response Model Item Ability Threshold Plot}
  \label{fig:lotr2plgrm}
\end{figure}


Again, Figure \ref{fig:lotr2plgrm} shows no violations of the modelling assumptions for the two parameter GRM. The next step was to assess which of the models provided the best fit to the data using a likelihood ratio test.

<<anovagrmlotr, echo=FALSE, results=hide, cache=TRUE>>=
anova.grm.lotr <- anova(lotr.grm.1pl, lotr.grm.2pl)
@ 

The two parameter model was significantly better than the one parameter model ($p=0.001$), but the BIC suggested that the one parameter model provided a better overall fit to the data. 

<<lotr2plestimates, echo=FALSE, results=tex>>=
print(xtable(coef(lotr.pcm.2pl),label="tab:lotr2plestimates", caption="Coefficient Estimates for LOTR Two Paramter Graded Response Model"))
@

The estimates for the two parameter model can be seen in Table \ref{tab:lotr2plestimates}.

\section{Modeling the relationship between health, optimism and mindfulness}
\label{sec:model-relat-betw}

The next step in the analytic procedure was to examine the relationships between health, optimism and mindfulness somewhat more deeply, drawing on the work carried out so far with the data. 

The first step was to assess the fit of the factor models and the regression models together, in a Structural Equation Model. The best model for each scale was used, and the relationships between the latent traits estimated. This model was then compared to a model fit using the summary scores. Additionally, each of the IRT Models were used as a base for the relationships between the scales (essentially treated as alternative factor solutions). The regression model was formed by using the predictors from the lasso, ridge and stepwise regressions (on unseen data). 


\subsection{Optimism and Health}
\label{sec:optimism-health}

The most unexpected finding of this piece of research was the direction of the relationship between the LOT-R and the RAND MOS General Health scale. This section examines a number of different models, to determine which of the solutions is more likely, given the data. 

The models used in this section were as follows:

\begin{itemize}
\item A model where both health and mindfulness, along with the other regression predictors, directly affect optimism;

\item A model where the effect of health on optimism is mediated through the other predictor variables;

\item Each of these models was fit both to the summary scores and the latent factors.
\end{itemize}

The lasso model suggested that Age, EnergyFatigue, Emotional Well BEing, General Health and Mindfulness were associated with optimism, so these variables are included in the model. 

<<lotrranddirect, echo=FALSE, results=hide>>=
hom1.opt.sem <- hom1[,c("Age", "energyfat", "emwellbeing", "mindfulness", "generalhealth", "optimism")]
predictors <- c("Age", "energyfat", "emwellbeing", "generalhealth", "mindfulness")
response <- "optimism"
variables <- c(predictors, response)
lotr.rand.direct <- mxModel(name="LOTRRandDirect",
                            type="RAM",
                            mxData(observed=na.omit(hom1.opt.sem),
                                   type="raw" ,
                                   means=NA,
                                   numObs=NA),
                            manifestVars=variables,
                            mxPath(from=variables, arrows=2, free=FALSE, values=1),
                       ##      mxPath(from="mindfulness", to="generalhealth", free=TRUE, values=1, arrows=2),
                       ## mxPath(from="mindfulness", to="emwellbeing", free=TRUE, values=1, arrows=2),     
                       ##      mxPath(from="mindfulness", to="energyfat", free=TRUE, values=1, arrows=2),
                       ##      mxPath(from="mindfulness", to="Age", free=TRUE, values=1, arrows=2),
                            mxPath(from="mindfulness", to=c("Age", "emwellbeing", "energyfat", "generalhealth", "optimism" ), connect="unique.pairs", arrows=2),
                            mxPath(from="generalhealth", to=c("Age", "emwellbeing", "energyfat", "mindfulness", "optimism"), connect="unique.pairs", arrows=2),
                            mxPath(from="Age", to=c("emwellbeing", "energyfat", "mindfulness", "optimism", "generalhealth"), connect="unique.pairs", arrows=2),
                            mxPath(from="optimism", to=c("emwellbeing", "energyfat", "mindfulness", "generalhealth", "Age"), connect="unique.pairs", arrows=2),
                            mxPath(from="emwellbeing", to=c("optimism", "generalhealth", "energyfat", "mindfulness", "Age"), connect="unique.pairs", arrows=2),
                            mxPath(from="energyfat", to=c("optimism", "generalhealth", "mindfulness", "emwellbeing", "Age"), connect="unique.pairs", arrows=2),
## mxPath(from="generalhealth", to="emwellbeing", free=TRUE, values=1, arrows=2),
## mxPath(from="generalhealth", to="energyfat", free=TRUE, values=1, arrows=2),
##                             mxPath(from="generalhealth", to="energyfat", free=TRUE, values=1, arrows=2),
##                             mxPath(from="generalhealth", to="Age", free=TRUE, values=1, arrows=2),
##                             mxPath(from="emwellbeing", to="energyfat", free=TRUE, values=1, arrows=2),
##                             mxPath(from="emwellbeing", to="Age", free=TRUE, values=1, arrows=2),
                            mxPath(from="one", to=variables, arrows=1, free=FALSE, values=1),
                            mxPath(from="Age", to="optimism", arrows=1),
                            mxPath(from="energyfat", to="optimism", arrows=1),
                            mxPath(from="emwellbeing", to="optimism", arrows=1),
                            mxPath(from="mindfulness", to="optimism", arrows=1),
                            mxPath(from="generalhealth", to="optimism",arrows=1))
                            
## mxPath(from=predictors, to=respnse))

@ 

<<opthealthdirectfit, echo=FALSE, results=hide>>=
opt.direct.fit <- mxRun(model=lotr.rand.direct)
omxGraphviz(opt.direct.fit, dotFilename="lotrdirect.dot")
@ 

\begin{figure}
\includegraphics{lotrdirect}  
  \caption{Direct Model for Optimism and Health}
  \label{fig:lotrdirect}
\end{figure}



<<opthealthmaaasmdediated, echo=FALSE, results=hide>>=
opt.health.indirect.maas <- mxModel(lotr.rand.direct, mxPath(from=predictors, to=response), remove=TRUE)
rand.lotr.indirect.done <- mxModel(opt.health.indirect.maas,
                                   name="LOTRRandIndirectMAAS",
                                   mxPath(from="generalhealth", to="mindfulness"),
                                   mxPath(from="Age", to="mindfulness"),
                                   mxPath(from="energyfat", to="mindfulness"),
                                   mxPath(from="emwellbeing", to="mindfulness"),
                                   mxPath(from="mindfulness", to="optimism"), independent=TRUE)

@ 

<<fitopthealthmediated1, echo=FALSE, results=hide>>=
rand.lotr.indirect.maas.fit <- mxRun(rand.lotr.indirect.done)
omxGraphviz(rand.lotr.indirect.maas.fit, dotFilename="lotrindirectmaas.dot")
@ 

\begin{figure}
\includegraphics{lotrindirectmass}  
  \caption{Optimism and Health Mediated by Mindfulness}
  \label{fig:lotrhealthmind}
\end{figure}


<<randindirectemwelleing, echo=FALSE, results=hide>>=
rand.indirect.emwellbeing <- mxModel(rand.lotr.indirect.done, remove=TRUE, mxPath(from=predictors, to=response))
rand.indirect.emwellbeing2 <- mxModel(rand.indirect.emwellbeing,
                                      name="RandIndirectEmWellBeing", 
                                      mxPath(from="Age", to="emwellbeing", arrows=1),
                                      mxPath(from="energyfat", to="emwellbeing"),
                                      mxPath(from="generalhealth", to="emwellbeing"),
                                      mxPath(from="mindfulness", to="emwellbeing"),
                                      mxPath(from="emwellbeing", to="optimism"))

@ 

<<opthealthmediatedfit2, echo=FALSE, results=hide>>=
lotr.indirect.emwellbeing <- mxRun(rand.indirect.emwellbeing2)
omxGraphviz(lotr.indirect.emwellbeing, dotFilename="lotrindirectemwellbeing.dot")
@ 


\begin{figure}
  \includegraphics{lotrindirectemwellbeing}
  \caption{Optimism and Health, Mediated by Emotional Well Being}
  \label{fig:lotrhealthemwellbeing}
\end{figure}

<<lotrindirectenergyfat, echo=FALSE, results=hide>>=
lotr.indirect <- mxModel(lotr.rand.direct, remove=TRUE, mxPath(from=predictors, to=response))
lotr.indirect.energyfat <- mxModel(lotr.indirect,
                                   name="LOTRIndirectEnergyFatigue",
                                   mxPath(from="Age", to="energyfat"),
                                   mxPath(from="emwellbeing", to="energyfat"),
                                   mxPath(from="generalhealth", to="energyfat"),
                                   mxPath(from="mindfulness", to="energyfat"),
                                   mxPath(from="energyfat", to="optimism"))

@ 

<<opthealthmediatedfit3, echo=FALSE, results=hide>>=
lotr.indirect.energyfat.fit <- mxRun(lotr.indirect.energyfat)
omxGraphviz(lotr.indirect.energyfat.fit, dotFilename="lotrindirectenergyfat.dot")
@ 

\begin{figure}
  \includegraphics{lotrindirectenergyfat}
  \caption{Optimism and Health Mediated by Energy/Fatigue}
  \label{fig:lotrhealthenergyfat}
\end{figure}

<<lotrindirectgenhealth, echo=FALSE, results=hide>>=
lotr.indirect.genhealth <- mxModel(lotr.indirect, 
                                   name="LOTRIndirectGenHealth",
                                   mxPath(from="Age", to="generalhealth"),
                                   mxPath(from="energyfat", to="generalhealth"),
                                   mxPath(from="emwellbeing", to="generalhealth"),
                                   mxPath(from="mindfulness", to="generalhealth"),
                                   mxPath(from="generalhealth", to="optimism"))

@ 

<<lotrindirectfit4, echo=FALSE, results=hide>>=
lotr.indirect.genhealth.fit <- mxRun(lotr.indirect.genhealth)
omxGraphviz(lotr.indirect.genhealth.fit, dotFilename="lotrindirectgenhealth.dot")
@ 


\begin{figure}
  \includegraphics{lotrindirectgenhealth}
  \caption{Optimism Directly Affected by Health}
  \label{fig:optindirectgenhealth}
\end{figure}

<<lotrmediatedhealth, echo=FALSE, results=hide, cache=TRUE>>=
lotr.health.med <- mxModel(lotr.rand.direct, remove=TRUE, mxPath(from="generalhealth", to="optimism"))
lotr.health.med2 <- mxModel(lotr.rand.direct, 
                            name="LOTRHealthMediated",
                            mxPath(from="generalhealth", to="mindfulness"),
                            mxPath(from="mindfulness", to="optimism"))
lotr.health.med.fit <- mxRun(lotr.health.med2)
omxGraphviz(lotr.health.med.fit, dotFilename="lotrindirectmindfulness.dot")
@ 

\begin{figure}
  \includegraphics{lotrindirectmindfulness}
  \caption{Optimism and Health Mediated by Mindfulness}
  \label{fig:lotrhealthmind2}
\end{figure}

<<lotrmodcomp, echo=FALSE, results=tex>>=
lotr.comp <- mxCompare(base=opt.direct.fit, comp=c(rand.lotr.indirect.maas.fit,lotr.indirect.emwellbeing, lotr.indirect.energyfat.fit, lotr.indirect.genhealth.fit, lotr.health.med.fit))
print(xtable(lotr.comp, label="tab:lotrmodcomp1", caption="Comparison of Different Models between Optimism and Other Variables, Sample One"), scalebox=0.8)
@ 

The different models for how optimism is related to the other variables (directly versus mediated) were compared and the results are shown in Table \ref{tab:lotrmodcomp1}. As can be seen from this Table, the best fitting model was that where Emotional Well Being mediated the relationship between optimism and health. Note that all of these models fitted quite poorly, so this finding needs to be tested on the second sample. 

\section{Predictions}
\label{sec:predictions}

As discussed in the methodology , one of the problems with psychometric methods is the problem of overfitting. This problem was dealt with for this research by building the models on the first dataset, and then fitting them on a seperate sample~\footnote{a process conventionally known as replication}. 

This section covers the following assessments of models developed on Sample One

\begin{enumerate}
\item Confirmatory Factor Analyses are carried out for the RAND MOS, the LOTR and the MAAS. 

\item The IRT models are assessed using a process of prediction of factor scores using the old model and a new one developed on the second sample. 

\item The regression models are refit to new data and the errors in prediction are examined. 

\item The SEM models for the relationship between optimism and health were re-examined on the new data. 
\end{enumerate}

<<hom2scales, echo=FALSE, results=hide, cache=TRUE>>=
randitems.paste <- paste(rand, c(1:36), sep="")
randitems2 <- hom2[,randitems.paste]
maasitems.paste <- paste(maas, c(1:15), sep="")
maasitems2 <- hom2[,maasitems.paste]
lotritems.paste <- paste(lotr, c(1,3,4,7,9,10), sep="")
lotritems2 <- hom2[,lotritems.paste]
@



\subsection{RAND MOS CFA on Sample Two (Split A)}
\label{sec:rand-mos-cfa}



<<rand2semhom2, echo=FALSE, results=hide>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "General and Emotional Health")
physfun <- paste(rand, c(3:12), sep="")
genemhealth <- paste(rand,c(1,14,17:36), sep="")
Rand2model2 <- mxModel(name="RAND2Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="General and Emotional Health", to=genemhealth),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                       mxData(observed=cov(na.omit(randitems2)), type="cov", numObs=281)
                      )
rand2fit2 <- mxRun(Rand2model2)
rand2summ2 <- summary(rand2fit2)
@

<<rand4semhom2, echo=FALSE, results=hide, cache=TRUE>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "Social and Emotional Functioning", "General Health", "Physical Limitations")
physfun <- paste(rand, c(3:12), sep="")
genhealth <- paste(rand, c(1,33:36), sep="")
physlim <- paste(rand, c(13:16,21:22), sep="")
emsocfunc <- paste(rand, c(17:20, 23:32), sep="")
Rand4model2 <- mxModel(name="RAND4Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="Social and Emotional Functioning", to=emsocfunc),
                      mxPath(from="General Health", to=genhealth),
                      mxPath(from="Physical Limitations", to=physlim),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=281)
                      )
rand4fit2 <- mxRun(Rand4model2)
rand4summ2 <- summary(rand4fit2)
@ 


<<rand8semhom2, echo=FALSE, results=hide, cache=TRUE>>=
randitems.cols <- paste(rand, 1:36, sep="")
manifests <- randitems.cols
latents <- c("Physical Functioning", "Social and Emotional Wellbeing", "General Health", "Emotional Role Limitations", "Role Limitations", "Fatigue", "Pain", "Energy")
physfun <- paste(rand, c(3:12), sep="")
socemwellbeing <- paste(rand, c(20,23:28, 30, 32), sep="")
genhealth <- paste(rand, c(1,33:36), sep="")
emrolelim <- paste(rand, c(17:20), sep="")
rolelim <- paste(rand, c(13:16), sep="")
energy <- paste(rand, c(29, 31), sep="")
pain <- paste(rand, c(21:22), sep="")
fatigue <- paste(rand, c(2, 23, 27), sep="")
Rand8model2 <- mxModel(name="RAND8Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Physical Functioning", to=physfun),
                      mxPath(from="Social and Emotional Wellbeing", to=socemwellbeing),
                      mxPath(from="General Health", to=genhealth),
                      mxPath(from="Emotional Role Limitations", to=emrolelim),
                      mxPath(from="Role Limitations", to=rolelim),
                      mxPath(from="Energy", to=energy),
                      mxPath(from="Pain", to=pain),
                      mxPath(from="Fatigue", to=fatigue),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(randitems.scored)), type="cov", numObs=281))
                      
rand8fit2 <- mxRun(Rand8model2)
rand8summ2 <- summary(rand8fit2)
@



<<randsemcomparehom2, echo=FALSE, results=tex>>=
randsemcomp.hom2 <- mxCompare(base=rand2fit2, comparison=c(rand4fit2, rand8fit2))
print(xtable(randsemcomp,label="tab:randsemcomparehom2", caption="Model Comparison for RAND MOS data using Models from Sample 1 on Sample 2" ), scalebox=0.9)
@

As Table \ref{tab:randsemcomparehom2} shows, the eight factor model provided the best fit to this unseen data, similarly to the data on which the model was created. 

\subsubsection{MAAS CFA on Sample Two (Split A)}
\label{sec:maas-cfa-sample}

<<splitsample, echo=FALSE, results=hide, cache=TRUE>>=
set.seed(17)
maassamp <- sample(1:1109, 1109)
ms1 <- maassamp[1:370]
ms2 <- maassamp[371:740]
ms3 <- maassamp[741:length(maassamp)]
maasitems2a <- maasitems2[ms1,]
maasitems2b <- maasitems2[ms2,]
maasitems2c <- maasitems2[ms3,]
maasitems.nota <- maasitems2[c(ms2, ms3),]
maasitems.notb <- maasitems2[c(ms1, ms3),]
maasitems.notc <- maasitems2[c(ms1, ms2),]
lotrsamp <- sample(1:1109, 1109)
lr1 <- lotrsamp[1:370]
lr2 <- lotrsamp[371:740]
lr3 <- lotrsamp[741:length(lotrsamp)]
lotritems2a <- lotritems2[lr1,]
lotritems2b <- lotritems2[lr2,]
lotritems2c <- lotritems2[lr3,]
lotritems.nota <- lotritems2[c(lr2, lr3),]
lotritems.notb <- lotritems2[c(lr1, lr3),]
lotritems.notc <- lotritems2[c(lr1, lr2),]
randsamp <- sample(1:1109, 1109)
r1 <- randsamp[1:370]
r2 <- randsamp[371:740]
r3 <- randsamp[741:length(randsamp)]
randitems2a <- randitems[r1,]
randitems2b <- randitems[r2,]
randitems2c <- randitems[r3,]
randitems.nota <- randitems[c(r2, r3),]
randitems.notb <- randitems[c(r1, r3),]
randitems.notc <- randitems[c(r1, r2),]
scales <- hom2[,c(4,66:75)]
scales.samp <- sample(1:1109, 1109)
s1.samp <- scales.samp[1:370]
s2.samp <- scales.samp[371:740]
s3.samp <- scales.samp[741:length(scales.samp)]
scales2a <- scales[s1.samp,]
scales2b <- scales[s2.samp,]
scales2c <- scales[s3.samp,]
@

<<maas7sem2, echo=FALSE, results=hide, cache=TRUE>>=
maas3model2 <- mxModel(Maas3model, name="MAAS3Fit2", mxData(observed=cov(na.omit(maasitems2a)), type="cov", numObs=313))
maas3fit2 <- mxRun(maas3model2)
maas3fitsum <- summary(maas3fit2)
@


<<maas1sem2, echo=FALSE, results=hide, cache=TRUE>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Unawareness")
unawareness <- paste(maas, c(1:15), sep="")
Maas1model2 <- mxModel(name="MAAS1Samp2",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Unawareness", to=unawareness),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2a)), type="cov", numObs=313)
                      )
maas1fit2 <- mxRun(Maas1model2)
maas1summ2 <- summary(maas1fit2)
@




<<maassemcompare2, echo=FALSE, results=tex>>=
maascomp2 <- mxCompare(base=maas1fit2, comparison=maas3fit2)
maascomp.xtab2 <- xtable(maascomp2,label="tab:maassemcompare2", caption="Comparison of Sample One MAAS Factor Models on a subset of Sample Two Data (Split A)")
print(maascomp.xtab2)
@

Table \ref{tab:maassemcompare2} demonstrates that the MAAS 1 factor model provided the best fit to the subsample of data ($n=313$) used to test the model.

\subsubsection{LOTR CFA on Sample Two (Split A)}
\label{sec:lotr-cfa-sample}



<<lotr1sem2, echo=FALSE, results=hide, cache=TRUE>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism")
optimism <- paste(lotr, c(1,3,4,7,9,10), sep="")
Lotr1model2 <- mxModel(name="LOTR1",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2a)), type="cov", numObs=313)
                      )
lotr1fit2 <- mxRun(Lotr1model2)
lotr1summ2 <- summary(lotr1fit2)
@


<<lotr3sem, echo=FALSE, results=hide, cache=TRUE>>=
Lotr2model2 <- mxModel(Lotr2model, 
                       name="LOTR2Model2",
                      mxData(
                          observed=cov(na.omit(lotritems2a)), 
                          type="cov", numObs=313))
lotr2fit2 <- mxRun(Lotr2model2)
lotr2summ2 <- summary(lotr2fit2)
@

<<lotrcompare2, echo=FALSE, results=tex>>=
lotrcomp2 <- mxCompare(base=lotr1fit2, comparison=lotr2fit2)
lotrcomp.xtab2 <- xtable(lotrcomp2,label="tab:lotrcompare2", caption="Comparison of SEM Results for Sample One LOTR Factor Models on a Subset of Sample Two (Split A)")
print(lotrcomp.xtab2)
@

Table \ref{tab:lotrcompare2}, shows that the one factor model provides the best fit to the subsample of data used to examine the models performance on new data.

The next part of the analyses was examining the predictive ability of the IRT models developed on sample one.

\subsection{Confirmatory IRT Analyses}
\label{sec:conf-irt-analys}


Firstly, the performance of the MAAS models are assessed on unseen data. 

<<hom1maasgrmtest, echo=FALSE, results=tex>>=
maas.irt2a <- maasitems2a[,paste(maas, c(1,3:5,7:15), sep="")]
hom1.maas.grm.1pl.test <- testIRTModels(maas.grm.1pl, maas.irt2a, gpcmconstraint=NULL, grmconstraint=TRUE) 
hom1.maas.grm.2pl.test <- testIRTModels(maas.grm.2pl, maas.irt2a, gpcmconstraint=NULL, grmconstraint=FALSE) 
hom1.maas.grm.all <- rbind(hom1.maas.grm.1pl.test, hom1.maas.grm.2pl.test)
rownames(hom1.maas.grm.all) <- c("One Parameter GRM", "Two Parameter GRM")
print(xtable(hom1.maas.grm.all, caption="Performance of MAAS One and Two Parameter GRM's on Unseen Data (Sample Two, Split A)", label="tab:hom1maasgrmtest"))
@ 

As can be seen from Table \ref{tab:hom1maasgrmtest}, the one parameter GRM provided a better fit to the unseen  data. 

Finally, the performance of the one and two parameter IRT models for the LOT-R were assessed. 

<<hom1lotrgrmtest, echo=FALSE, results=tex>>=
lotr.irt2a <- lotritems2a[,paste(lotr, c(3,4,7,9,10), sep="")]
hom1.lotr.grm.1pl.test <- testIRTModels(lotr.grm.1pl, lotr.irt2a, gpcmconstraint=NULL, grmconstraint=TRUE)
hom1.lotr.grm.2pl.test <- testIRTModels(lotr.grm.2pl, lotr.irt2a, gpcmconstraint=NULL, grmconstraint=FALSE)
hom1.lotr.grm.all <- rbind(hom1.lotr.grm.1pl.test, hom1.lotr.grm.2pl.test)
rownames(hom1.lotr.grm.all) <- c("One Parameter GRM", "Two Parameter GRM")
print(xtable(hom1.lotr.grm.all, caption="Performance of LOTR One and Two Parameter GRM\'s on Unseen Data (Sample Two, Split A)", label="tab:hom1lotrgrmtest"))
@ 


As can be seen from Table \ref{tab:hom1lotrgrmtest}, the one parameter model provided a better fit to the unseen data. 

\subsection{Regression Model Predictions}
\label{sec:regr-model-pred}

<<optsetup2a, echo=FALSE, results=hide, cache=TRUE>>=
scales2a.full <- na.omit(scales2a)
scales2a.full2 <- scales2a.full[,-11]
opt.test2a <- with(scales2a.full, optimism)
opt.train.ind <- with(scales2a.full, createDataPartition(optimism, p=0.75, list=FALSE))
scales2a.opt.train <- scales2a.full[opt.train.ind,]
scales2a.opt.test <- scales2a.full[-opt.train.ind,]
opt.train2a <- with(scales2a.opt.train, optimism)
opt.pred.train2a <- scales2a.opt.train[,-11]
## opt.test2a <- with(scales2a.opt.test, optimism)
opt.pred.test2a <- scales2a.opt.test[,-11]
@ 

<<optlassotest, echo=FALSE, results=tex>>=
opt.lasso.test2a <- penalisedRegression(x=trainpred, y=trainopt,  testdata=scales2a.full2, newy=opt.test2a, alpha=1, nfolds=10, type="coefficients")
opt.lasso.test2a.pred <- penalisedRegression(x=trainpred, y=trainopt,  testdata=scales2a.full2, newy=opt.test2a, alpha=1, nfolds=10, type="response")
print(xtable(as.matrix(opt.lasso.test2a), caption="Coefficients for Optimism Regression from Study One on Sample Two (Split A)", label="tab:homoptlasso2a"))
opt.lasso.test2a.resp <- penalisedRegression (x=trainpred, y=trainopt,  testdata=scales2a.full2, newy=opt.test2a, alpha=1, nfolds=10, type="response")
@ 

As can be seen from Table \ref{tab:homoptlasso2a}, the general trend for coefficients was similar across the two samples. 

Next, the accuracy of the predictions was assessed using a RMSEA approach. For the optimism lasso data. The error in the estimation was equal to 0.755, suggesting that the predictions were approximately 25\% accurate. This can be seen more clearly in Figure \ref{fig:plotoptlassopred} below. As can be seen there is no real systematic error in the predictions, they are just inaccurate.


\begin{figure}
<<plotlassopredobs, echo=FALSE, figure=TRUE>>=
print (ggplot (opt.lasso.test2a.resp, aes (x=pred, y=obs))+geom_point ()+geom_smooth (method="lm"))
@   
  \caption{Predicted versus observed values of Optimism in second sample, Split A, lasso regression}
  \label{fig:plotoptlassopred}
\end{figure}


Next, the ridge regression model was applied to the new data. 

<<optridge2a, echo=FALSE, results=tex>>=
opt.ridge.test2a <- penalisedRegression(x=trainpred, y=trainopt,  testdata=scales2a.full2, newy=opt.test2a, alpha=0, nfolds=10, type="coefficients")
print(xtable(as.matrix(opt.ridge.test2a), caption="Coefficients for Optimism Regression from Study One on Sample Two (Split A)", label="tab:homoptridge2a"))
opt.ridge.test2a.resp <- penalisedRegression (x=trainpred, y=trainopt,  testdata=scales2a.full2, newy=opt.test2a, alpha=0, nfolds=10, type="response")
rmsea.ridge.opt2a <- rmsea (opt.ridge.test2a.resp)
@ 

As can be seen from Table \ref{tab:homoptridge2a} mindfulness again appears to be the most useful predictor of optimism, and in general the coefficients are quite similar to those from Study One. The RMSEA approach showed that the ridge regression model had an error of 0.749, which is extremely similar to that of the lasso model. 
\begin{figure}
<<optridgeplot2a, echo=FALSE, figure=TRUE>>=
print (ggplot (opt.ridge.test2a.resp, aes (x=pred, y=obs))+geom_point()+geom_smooth())
@   
  \caption{Ridge Regression Predictions, Optimism, Split 2A}
  \label{fig:ridgepredplot2a}
\end{figure}


As can be seen from Figure~\ref{fig:ridgepredplot2a}, again the errors do not appear to be systematically biased, but rather spread both evenly above and below the line of perfect prediction. 

Finally, the model developed by stepwise selection was applied to the new data. 

<<optstep2a, echo=FALSE, results=hide, cache=TRUE>>=
opt.step.test2a <- lm(optimism~generalhealth+Age+emwellbeing, data=scales2a.full)
opt.step.pred.resp <- predict (opt.step.test, newdata=scales2a.full, type="response")
opt.step.pred.obs <- data.frame (pred=opt.step.pred.resp, obs=opt.test2a)
rmsea.opt.step2a <- rmsea (opt.step.pred.obs)
@ 
<<optstep2aprint, echo=FALSE, results=tex>>=
print(xtable(summary(opt.step.test), caption="Coefficients for Stepwise Selected Model on Test Data", label="tab:homstepopttest2a"))
@ 

\begin{figure}
<<optsteppredplot, echo=FALSE, figure=TRUE>>=
print (ggplot (opt.step.pred.obs, aes (x=pred, y=obs))+geom_point ()+geom_smooth (method="lm"))
@   
  \caption{Stepwise Regression Predictions, Optimism Split 2A}
  \label{fig:optstepredplot}
\end{figure}


As can be seen from Table \ref{tab:homstepopttest2a}, the stepwise model has an extremely significant coefficient for emotional well being, and a moderately significant coefficient for general health and age. The coefficient signs are all in line with those from study one, and appear to match those from the other regressions. The RMSEA metric gave a value of 0.746, which is marginally better than either the lasso or ridge predictions. Figure \ref{fig:optstepredplot} shows this in more detail, and in fact is quite similar to the plots for the lasso and ridge solutions. This would seem to suggest that each of the three models are converging on a relatively common solution, increasing our confidence that we have explored the linear relationships between these variables. 

\section{Discussion}
\label{sec:discussion}

A number of interesting findings, some expected and others not have arisen from this study. Firstly, in contrast to published research, optimism and health were negatively associated. This relationship appears to be mediated through Emotional Well Being, which may point to some earlier work suggesting that the optimism-health connection was mediated through negative affect. This result would seem to contribute some new information to this debate. 

Secondly, the factor structures for the RAND-MOS, the MAAS and the LOT-R were more or less confirmed, and more importantly from the perspective of this thesis, reduced versions of these scales were developed which were more predictive on new data. 

The aims for the second sample collected were as follows:

\begin{itemize}
\item To replicate the finding of a negative relationship between optimism and health

\item To determine if this relationship was still moderated by emotional well-being. 

\item To improve the models for the MAAS and the LOT-R to aid in the prediction of placebo in the experimental portion of the research. 
\end{itemize}


\section{Sample Two Results}

Sample Two was collected through online methods, as described above and had large amounts of missing data concentrated in RAND Q13-16, which was assumed to be MCAR (Missing Completely At Random). 

%% \begin{figure}
<<optplot2, echo=FALSE, results=hide>>=
optplot2 <- ggplot(hom2, aes(x=generalhealth, y=optimism))+layer(geom="smooth", method="lm")
print(optplot2)
@  
%%   \caption{Scatterplot with Linear Regression Smooth of Relationship between General Health and Optimism}
%%   \label{fig:genhealthoptplot2}
%% \end{figure}


The relationship between optimism and health is even clearer in sample 2, with the same strange negative relationship being apparent. 

\section{Regression Analyses}
\label{sec:regression-analyses}

Similiar regression analyses were carried out on the test sample from Study 2 as were carried out on the first sample, focusing mainly on the relationship of optimism to the other variables in the sample, as this was the major finding from Sample One. Similarly to the earlier work, stepwise linear regression models, along with lasso and ridge regression were applied to the data. 

<<optsetup2b, echo=FALSE, results=hide, cache=TRUE>>=
scales2b.full <- na.omit(scales2b)
scales2b.full2 <- scales2b.full[,-11]
opt.test2b <- with(scales2b.full, optimism)
opt.train.ind <- with(scales2b.full, createDataPartition(optimism, p=0.75, list=FALSE))
scales2b.opt.train <- scales2b.full[opt.train.ind,]
scales2b.opt.test <- scales2b.full[-opt.train.ind,]
opt.train2b <- with(scales2b.opt.train, optimism)
opt.pred.train2b <- scales2b.opt.train[,-11]
opt.test2b <- with(scales2b.opt.test, optimism)
opt.pred.test2b <- scales2b.opt.test[,-11]
@ 

<<optlasso2b, echo=FALSE, results=hide, cache=TRUE>>=
opt.lasso.test2b <- penalisedRegression(x=opt.pred.train2b, y=opt.train2b,  testdata=opt.pred.test2b, newy=opt.test2b, alpha=1, nfolds=10, type="coefficients")
opt.lasso.test2b.pred <- penalisedRegression(x=opt.pred.train2b, y=opt.train2b,  testdata=opt.pred.test2b, newy=opt.test2b, alpha=1, nfolds=10, type="response")
print(xtable(as.matrix(opt.lasso.test2b), caption="Coefficients for Optimism Regression from Study One on Sample Two (Split A)", label="tab:homoptlasso2b"))
opt.lasso.test2b.resp <- penalisedRegression (x=opt.pred.train2b, y=opt.train2b,  testdata=opt.pred.test2b, newy=opt.test2b, alpha=1, nfolds=10, type="response")
@ 

<<optlassoprint, echo=FALSE, results=tex>>=
print(xtable(as.matrix(opt.lasso.test2b), caption="Coefficients for Lasso Regression, Split B", label="tab:optlasso2b"))
@ 


Table \ref{tab:optlasso2b} shows the estimated coefficients for the lasso regression model. It can be seen that the pattern is quite similar to that from Sample one, with Age, emotional well being, general health and mindfulness being retained by the model, with the same sign as in the previous sample. 

<<optridge2b, echo=FALSE, results=hide, cache=TRUE>>=
opt.ridge.test2b <- penalisedRegression(x=opt.pred.train2b, y=opt.train2b,  testdata=opt.pred.test2b, newy=opt.test2b, alpha=1, nfolds=10, type="coefficients")
opt.ridge.test2b.pred <- penalisedRegression(x=opt.pred.train2b, y=opt.train2b,  testdata=opt.pred.test2b, newy=opt.test2b, alpha=1, nfolds=10, type="response")
print(xtable(as.matrix(opt.ridge.test2b), caption="Coefficients for Optimism Regression from Study One on Sample Two (Split A)", label="tab:homoptridge2b"))
opt.ridge.test2b.resp <- penalisedRegression (x=opt.pred.train2b, y=opt.train2b,  testdata=opt.pred.test2b, newy=opt.test2b, alpha=1, nfolds=10, type="response")
@ 

<<optridge2bprint, echo=FALSE, results=tex>>=
print(xtable(as.matrix(opt.ridge.test2b), caption="Coefficients for Ridge Regression Model on Optimism, Split B", label="tab:optridge2b"))
@ 

Table \ref{tab:optridge2b} shows the estimated coefficients for the ridge regression model on Split B, and it can be seen that the coefficients are broadly in line with those from the lasso fit, and also with the coefficients from Sample One. The fit of the SEM models for this relationship are described below. 



\subsection{MAAS}
\label{sec:maas-samp-two}

\subsection{Factor Analyses}
\label{sec:fact-maas}

<<maas2bparallel, echo=FALSE, results=hide, cache=TRUE, eval=FALSE>>=
maas2b.parallel <- fa.parallel(na.omit(maasitems2b))
vss.maas.2b <- VSS(na.omit(maasitems2b))
@

The parallel analysis procedure for Split B suggests that this sample of the responses to the MAAS has five factors, while the MAP criterion suggests that it has only one.  Following our previous approach, each of these factor solutions will be examined and interpreted before a CFA is applied on the remainder of the dataset.

For split C, the same procedure was carried out, and the various methods both suggested five and one factors, respectively. 

<<maas2cparallel, echo=FALSE, results=hide, cache=TRUE, eval=FALSE>>=
maas2c.parallel <- fa.parallel(na.omit(maasitems2c))
vss.maas.2c <- VSS(na.omit(maasitems2c))
@



<<maas2b1, echo=FALSE, results=tex>>=
maas2b.fact1 <- fa(maasitems2b, 1, rotate="oblimin", fm="pa")
print(FactorXtab(maas2b.fact1, names=c("Mindfulness"),label="tab:maas2bfact1", caption="MAAS One Factor Solution, Sample 2B"))
@

The solution shown above in Table \ref{tab:maas2bfact1} shows adequate loadings of all the questions on a first factor which can be named mindfulness.  This solution explained 41\% of the variance, which is quite low for a factor solution.

<<maas2c1, echo=FALSE, results=tex>>=
maas2c.fact1 <- fa(maasitems2c, 1, rotate="oblimin", fm="pa")
print(FactorXtab(maas2c.fact1, names=c("Mindfulness"), label="tab:maas2cfact1", caption="MAAS One Factor Solution, Sample 2C"))
@

Table \ref{tab:maas2cfact1} shows the one factor loadings for split C. In this split, the same solution only explained 36\% of the variance in the item loadings, which is in line with Split B, though much lower than the original published research in which the MAAS was developed. 



<<maas2bfact5, echo=FALSE, results=hide>>=
maas2b.fact5 <- fa(maasitems2b, 5, rotate="oblimin", fm="pa")
print(FactorXtab(maas2b.fact5, names=c("Distractability", "LackPresAware", "LackSomaticAware", "Inattention", "LackPresFocus"),  label="tab:tcq2bmaasfact5", caption="Factor Loadings, MAAS Five Factor Solution, Sample Two, Split B"), scalebox=0.8)
@
This five factor solution (not shown) explained 54\% of the variance in the sample.

PA1: "MAASQ5",  "MAASQ6",  "MAASQ7",  "MAASQ8",  "MAASQ9",  "MAASQ10".  This factor has come through in most of the previous solutions, and can again be termed distractability.

PA2: "MAASQ1", "MAASQ2", "MAASQ3".  Again, these items have clustered together previously, and this factor is again termed lack of present awareness.

PA3: "MAASQ4", "MAASQ5".  This factor is again termed lack of somatic awareness.

PA4: "MAASQ13", "MAASQ14".  This factor can best be termed as lack of attention.

PA5: "MAASQ10", "MAASQ11", "MAASQ12", "MAASQ14", "MAASQ15".  This factor again can be termed distractability.

<<maas2cfact5, echo=FALSE, results=hide>>=
maas2c.fact5 <- fa(maasitems2c, 5, rotate="oblimin", fm="pa")
print(FactorXtab(maas2c.fact5, names=c("LackPresFocus", "LackPresAware", "LackAtt", "LackSomAware", "LackAware"), label="tab:hom2cmaasfact5", caption="Factor Loadings, MAAS Five Factor Solution, Sample Two, Split C"), scalebox=0.8)
@
The five factor solution for split C (not shown) broke down as follows:
PA1: "MAASQ7"  "MAASQ8"  "MAASQ9"  "MAASQ10" "MAASQ11" "MAASQ12" "MAASQ14"
All of the items in this factor relate to a lack of attention to the present, and it can probably be best termed lack of present focus. 
PA2: "MAASQ9"  "MAASQ13"
Q9 loads on both PA1 and PA2, and as PA2 has really only Q13 loading to any major extent on it, no interpretation of this factor was performed. It was named lack of present awareness. 
PA4: "MAASQ2"  "MAASQ3"  "MAASQ14"
This factor can probably best be termed lack of attention. 
PA5: "MAASQ1" "MAASQ4" "MAASQ5"
Most of these items relate to lack of bodily attention, and this can probably best be termed lack of somatic awareness. 
PA3: "MAASQ4" "MAASQ6" "MAASQ7" "MAASQ8"
Note that MAASQ4 loads slightly on both PA3 and PA5, and is not considered in the interpretation here. These items can probably best be termed lack of awareness. 

The factor solution for Split C (shown in Table \ref{tab:hom2cmaasfact5}) is somewhat different from the five factor solution for Split B. Note that PA3 is somewhat similar to PA1 (Distractability) from Split B. 


<<maas2bsemon2c, echo=FALSE, results=hide, cache=TRUE>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Unawareness")
unawareness <- paste(maas, c(1:15), sep="")
Maas1model2b <- mxModel(name="MAAS12b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Unawareness", to=unawareness),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2c)), type="cov", numObs=370)
                      )
maas1fit2b <- mxRun(Maas1model2b)
maas1summ2b <- summary(maas1fit2b)
@ 


<<maas2csemon2b, echo=FALSE, results=hide, cache=TRUE>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Unawareness")
unawareness <- paste(maas, c(1:15), sep="")
Maas1model2c <- mxModel(name="MAAS12c",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Unawareness", to=unawareness),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2b)), type="cov", numObs=370)
                      )
maas1fit2c <- mxRun(Maas1model2c)
maas1summ2c <- summary(maas1fit2c)
@ 




<<maas5sem2b, echo=FALSE, results=hide, cache=TRUE>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("Lack of Bodily Awareness", "Distractability", "Lack of present focus", "Lack of somatic awareness", "Lack of attention")
bodyunaware <- paste(maas, c(6:10), sep="")
distractability <- paste(maas, c(11:15), sep="")
lackpresfocus <- paste(maas, c(1:3), sep="")
lacksomaware <- paste(maas, c(4,5), sep="")
lackattention <- paste(maas, c(13,14), sep="")
Maas5model2b <- mxModel(name="MAAS52b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Lack of Bodily Awareness", to=bodyunaware),
                        mxPath(from="Distractability", to=distractability),
                        mxPath(from="Lack of present focus", to=lackpresfocus),
                        mxPath(from="Lack of somatic awareness", to=lacksomaware),
                        mxPath(from="Lack of attention", to=lackattention),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2c)), type="cov", numObs=370)
                      )
maas5fit2b <- mxRun(Maas5model2b)
maas5summ2b <- summary(maas5fit2b)
@

<<maas5sem2c, echo=FALSE, results=hide, cache=TRUE, cache=TRUE>>=
maas <- "MAASQ"
maasitems.cols <- paste(maas, 1:15, sep="")
manifests <- maasitems.cols
latents <- c("lack of present focus", "lack of present awareness", "lack of attention", "Lack of somatic awareness", "lack of awareness")
bodyunaware <- paste(maas, c(1, 4, 5), sep="")
lackaware <- paste(maas, c(6:8), sep="")
lackpresfocus <- paste(maas, c(7:12, 14), sep="")
lacksomaware <- paste(maas, c(4,5), sep="")
lackattention <- paste(maas, c(2, 3, 14), sep="")
Maas5model2c <- mxModel(name="MAAS52c",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Lack of somatic awareness", to=bodyunaware),
                        mxPath(from="lack of present awareness", to=lackpresfocus),
                        mxPath (from="lack of attention", to=lackattention),
                        mxPath (from="lack of awareness", to=lackaware),
                        mxPath (from="lack of present focus", to=lackpresfocus),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(maasitems2b)), type="cov", numObs=370)
                      )
maas5fit2c <- mxRun(Maas5model2c)
maas5summ2c <- summary(maas5fit2c)
@

<<maas2bsemcompare, echo=FALSE, results=tex>>=
maas2b.semcomp <- mxCompare(base=maas1fit2b, comparison=c( maas5fit2b))
print(xtable(maas2b.semcomp,label="tab:maas2bsemcompare", caption="Comparison of Factor Structures for MAAS 2B Solutions, Tested on Split C"), scalebox=0.9)
@

As can be seen from Table \ref{tab:maas2bsemcompare}, the one factor solution again performs best, further increasing our confidence in its adequacy.


<<maas2csemcompare, echo=FALSE, results=tex>>=
maas2c.semcomp <- mxCompare(base=maas1fit2c, comparison=c( maas5fit2c))
print(xtable(maas2c.semcomp,label="tab:maas2csemcompare", caption="Comparison of Factor Structures for MAAS 2C Solutions, Tested on Split B"), scalebox=0.9)
@

As can be seen from Table \ref{tab:maas2csemcompare}, the one factor model was again superior to the five factor model in Split C. 

\subsubsection{IRT Analyses}
\label{sec:irt-analyses}

Next, we examine the Mindful Attention Awareness Scale, using the same methodologies. 

First, the assumptions underlying item response theory modelling are checked. 

<<maas2bcheck, echo=FALSE, results=hide, cache=TRUE>>=
maas2b.iio <- check.iio(na.omit(maasitems2b))
maas2b.mono <- check.monotonicity(na.omit(maasitems2b))
@ 

<<maas2bitemord, echo=FALSE, results=hide>>=
print(xtable(maas2b.iio[["violations"]], caption="Item Ordering Assumption Check, MAAS Split B", label="tab:maas2bitemord"))
maas2b.s <- maasitems2b[,paste(maas, c(1:4, 7:15), sep="")]
@ 

Questions 5 and 6 failed the item ordering assumptions for Split B and so are removed from the scale before further analysis. The reduced scale had no violations of the monotonicity assumption. 

<<maas2ccheck, echo=FALSE, results=hide, cache=TRUE>>=
maas2c.iio <- check.iio(na.omit(maasitems2c))
maas2c.mono <- check.monotonicity(na.omit(maasitems2c))
maas2c.s <- maasitems2c[,paste(maas, c(1:4, 7:10, 12:14), sep="")]
@ 

For Split C, items 5 and 11 fail the item ordering check, and so is removed. The reduced scale had no failures of the monotonicity assumption, so modelling continues with the reduced scale. 

%% Firstly, three partial credit models are fit to Split B.







Three partial credit models were fit to Splits B and C, but were not analysed further, as they provided a poor fit to the data. 

Next, one and two parameter graded response models were fit to Split B. 

<<maas2bgrm, echo=FALSE, results=hide, cache=TRUE>>=
maas2b.grm.1pl <- grm(maas2b.s, constrained=TRUE)
maas2b.grm.2pl <- grm(maas2b.s, constrained=FALSE)
@ 


<<maas2bgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas2b.grm.1pl), caption="Coefficient Estimates for MAAS One Parameter Graded Response Model, Split B", label="tab:maas2bgrm1pl"))
@ 


It can be seen from Table \ref{tab:maas2bgrm1pl} that there were no problems with the parameter estimates for this model. 

<<maas2bgrm2plprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas2b.grm.2pl), caption="Coefficient Estimates for MAAS Two Parameter Graded Response Model, Split B", label="tab:maas2bgrm2pl"))
@ 

The two parameter GRM model (not shown)  had no problems with the coefficient estimates for this solution, and the estimates show the usual tradeoff between discrimination and ability estimate parameters. 

Finally, we examine the performance of these models on unseen data.

<<maas2bgrmtest, echo=FALSE, results=hide, cache=TRUE>>=
maas.irt.notb <- maasitems.notb[,paste(maas, c(1:4, 7:15), sep="")]
maas2b.grm.1pl.test <- testIRTModels(maas2b.grm.1pl, maas.irt.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
maas2b.grm.2pl.test <- testIRTModels(maas2b.grm.2pl, maas.irt.notb, gpcmconstraint=NULL, grmconstraint=FALSE)
maas2b.grm.test.all <- rbind(maas2b.grm.1pl.test, maas2b.grm.2pl.test)

@ 

<<maasgrmcompprint, echo=FALSE, results=tex>>=
print(xtable(maas2b.grm.test.all, caption="Performance of One and Two Parameter Graded Response Models for MAAS on Unseen Data", label="tab:maas2bgrmtest"))
@ 

As can be seen from Table \ref{tab:maas2bgrmtest}, the one parameter model provided the best fit to the unseen data. 

Next, one and two parameter Graded Response Models are fit to Split C.

<<maas2cgrm, echo=FALSE, results=hide, cache=TRUE>>=
maas2c.grm.1pl <- grm(maas2c.s, constrained=TRUE)
maas2c.grm.2pl <- grm(maas2c.s, constrained=FALSE)
@ 

<<maas2cgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(maas2c.grm.1pl), caption="Coefficient Estimates for MAAS One Parameter Graded Response Model, Split C", label="tab:maas2cgrm1pl"))
@ 


As can be seen from Table \ref{tab:maas2cgrm1pl}, the coefficient estimates appear reasonable. The discrimination parameter is relatively low, suggesting that this scale is good for all levels of abilities, even though the highest estimated difficulty parameter is only 2.048, for Q13 which is ``I often find myself occupied with the future or the past'', which is a relatively concise summary of the entire construct of mindfulness. 

Next, a two parameter model is fit to this data. 

<<maas2cgrm2plprint, echo=FALSE, results=hide>>=
print(xtable(coef(maas2c.grm.2pl), caption="Coefficient Estimates for MAAS, Two Parameter Graded Response Model, Split C", label="tab:maas2cgrm2pl"))
@ 

The two parameter model (not shown), is not that much different from the one parameter model. Of interest is that Q13 remains the most difficult question, but its discrimination parameter has come down, suggesting that it behaves similarly for participants of all ability levels. Q8 has the highest discrimination parameter of all the items and is ``I rush through activities without being really attentive to them'' and it appears that this question is the best at discrimination between those higher and lower on the construct of mindfulness. 


The final step in the analysis of the MAAS scale is to test the performance of the models on unseen  data. 

<<maas2cgrmtest, echo=FALSE, results=hide, cache=TRUE>>=
maas.irt.notc <- maasitems.notc[,paste(maas, c(1:4, 7:10, 12:14), sep="")]
maas2c.grm.1pl.test <- testIRTModels(maas2c.grm.1pl, maas.irt.notc, gpcmconstraint=NULL, grmconstraint=TRUE)
maas2c.grm.2pl.test <- testIRTModels(maas2c.grm.2pl, maas.irt.notc, gpcmconstraint=NULL, grmconstraint=FALSE)
maas2c.grm.test.all <- rbind(maas2c.grm.1pl.test, maas2c.grm.2pl.test)

@ 

<<maas2cgrmcompprint, echo=FALSE, results=tex>>=
print(xtable(maas2c.grm.test.all, caption="Performance of MAAS One and Two Parameter Graded Response Models on Unseen Data (Splits A and B)", label="tab:maas2cgrmtest"))
@ 

As can be seen from Table \ref{tab:maas2cgrmtest}, the one parameter model provided a better fit to the unseen data (though neither model was particularly good). 


\subsection{LOTR}
\label{sec:lotr-1}



<<lotr2bparallel, echo=FALSE, results=hide, cache=TRUE>>=
sink("tmp.txt")
lotr2b.parallel <- fa.parallel(na.omit(lotritems2b))
lotr2b.vss <- VSS(na.omit(lotritems2b))
sink(NULL)
@

<<lotr2cparallel, echo=FALSE, results=hide, cache=TRUE>>=
sink("tmp.txt")
lotr2c.parallel <- fa.parallel(na.omit(lotritems2c))
lotr2c.vss <- VSS(na.omit(lotritems2c))
sink(NULL)
@
<<lotrparallelplot2b, echo=FALSE, results=hide>>=
sink("tmp.txt")
par(mfrow=c(1,2))
print(lotr2b.parallel)
print(lotr2b.vss)
sink(NULL)
@ 

Again, the parallel analysis criterion suggests two factors, while the MAP criterion suggests one, so both solutions will be examined and interpreted for both splits, as the results were broadly similar. 

<<lotr2bfact1, echo=FALSE, results=tex>>=
lotr2b.fact1 <- fa(lotritems2b, 1, rotate="oblimin", fm="pa")
print(FactorXtab(lotr2b.fact1, names=c("Optimism"),label="tab:hom2blotr1", caption="One Factor Solution, LOT-R, Sample Two, Split B"))
@


<<lotr2cfact1, echo=FALSE, results=tex>>=
lotr2c.fact1 <- fa(lotritems2c, 1, rotate="oblimin", fm="pa")
print(FactorXtab(lotr2c.fact1,names=c("Optimism"),label="tab:hom2clotr1", caption="One Factor Solution, LOT-R, Sample Two, Split C"))
@

Tables \ref{tab:hom2blotr1} and \ref{tab:hom2clotr1} show the estimated coefficients for the one factor solutions in both splits. The two solutions are somewhat different. The communalities for Q1 are much higher in Split C than in Split B, while those for Q4 are much lower in Split C than in Split B. To some extent this probably represents sampling error, but it is quite strange that the two negatively worded items should show much of the variance across samples. 


<<lotr2bfact2, echo=FALSE, results=tex>>=
lotr2b.fact2 <- fa(lotritems2b, 2, rotate="oblimin", fm="pa")
print(FactorXtab(lotr2b.fact2, names=c("Pessimism", "Optimism"), label="tab:hom2blotr2", caption="Two Factor Solution, LOT-R, Split B"))
@

<<lotr2cfact2, echo=FALSE, results=tex>>=
lotr2c.fact2 <- fa(lotritems2c, 2, rotate="oblimin", fm="pa")
print(FactorXtab(lotr2c.fact2, names=c("Pessimism", "Optimism"), label="tab:hom2clotr2", caption="Two Factor Solution, LOT-R, Split C"))
@

Tables \ref{tab:hom2blotr2} and \ref{tab:hom2clotr2} show some differences. In general, though the major outline of the structure is the same, Split B's solution is much cleaner than that of Split C. For example, in Split C it is unclear whether or not Q10 belongs to the first or second factor. Therefore, it would seem advisable to prefer the structure from Split B, but this will be tested. 

<<lotr1sem2b, echo=FALSE, results=hide, cache=TRUE>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism")
optimism <- paste(lotr, c(1,3,4,7,9,10), sep="")
Lotr1model2b <- mxModel(name="LOTR1b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2c)), type="cov", numObs=370)
                      )
lotr1fit2b <- mxRun(Lotr1model2b)
lotr1summ2b <- summary(lotr1fit2b)
@


<<lotr1sem2c, echo=FALSE, results=hide, cache=TRUE>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism")
optimism <- paste(lotr, c(1,3,4,7,9,10), sep="")
Lotr1model2c <- mxModel(name="LOTR1c",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2b)), type="cov", numObs=370)
                      )
lotr1fit2c <- mxRun(Lotr1model2c)
lotr1summ2c <- summary(lotr1fit2c)
@

<<lotr2sem2b, echo=FALSE, results=hide, cache=TRUE>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism", "Pessimism")
optimism <- paste(lotr, c(1,4,10), sep="")
pessimism <- paste(lotr, c(3,7,9), sep="")
Lotr2model2b <- mxModel(name="LOTR2b",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from="Pessimism", to=pessimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2c)), type="cov", numObs=370)
                      )
lotr2fit2b <- mxRun(Lotr2model2b)
lotr2summ2b <- summary(lotr2fit2b)
@

<<lotr2sem2c, echo=FALSE, results=hide, cache=TRUE>>=
lotr <- "LOTRQ"
lotritems.cols <- paste(lotr, c(1,3,4,7,9,10), sep="")
manifests <- lotritems.cols
latents <- c("Optimism", "Pessimism")
optimism <- paste(lotr, c(1,4,10), sep="")
pessimism <- paste(lotr, c(3,7,9), sep="")
Lotr2model2c <- mxModel(name="LOTR2c",
                      type="RAM",
                      manifestVars=manifests,
                      latentVars=latents,
                      mxPath(from="Optimism", to=optimism),
                      mxPath(from="Pessimism", to=pessimism),
                      mxPath(from=manifests, arrows=2),
                      mxPath(from=latents, arrows=2, free=FALSE, values=0.5),
                      mxData(observed=cov(na.omit(lotritems2b)), type="cov", numObs=370)
                      )
lotr2fit2c <- mxRun(Lotr2model2c)
lotr2summ2c <- summary(lotr2fit2c)
@

<<lotrcompare2b, echo=FALSE, results=tex>>=
lotrcomp2b <- mxCompare(base=lotr1fit2b, comparison=lotr2fit2b)
lotrcomp.xtab2b <- xtable(lotrcomp2b,label="tab:hom2blotrcomp", caption="Comparison of LOT-R Split B Factor Solutions, Tested on Split C")
print(lotrcomp.xtab2b)
@


<<lotrcompare2c, echo=FALSE, results=tex>>=
lotrcomp2c <- mxCompare(base=lotr1fit2c, comparison=lotr2fit2c)
lotrcomp.xtab2c <- xtable(lotrcomp2c,label="tab:hom2clotrcomp", caption="Comparison of LOT-R Split C Factor Solutions, Tested on Split B")
print(lotrcomp.xtab2c)
@ 

Tables \ref{tab:hom2blotrcomp} and \ref{tab:hom2clotrcomp} show the performance of the one and two factor solutions on the respective splits. In both solutions, while the two factor solution had a lower negative log likelihood, when controlling for parameters the one factor solution performed much better. 



Next, we examine the LOT-R using IRT approaches. 

<<lotr2bcheck, echo=FALSE, results=hide, cache=TRUE>>=
lotr2b.iio <- check.iio(na.omit(lotritems2b))
lotr2b.mono <- check.monotonicity(na.omit(lotritems2b))
@ 

The LOT-R for Split B had no problems with either item ordering or monotonicity. 


All of the partial credit models had non-monotonically increasing parameter estimates and so are not reported further here. 
Next, one and two parameter Graded Response Models were fit to the data. 

<<lotr2bgrm, echo=FALSE, results=hide, cache=TRUE>>=
lotr2b.grm.1pl <- grm(lotritems2b, constrained=TRUE)
lotr2b.grm.2pl <- grm(lotritems2b, constrained=FALSE)
@ 


<<lotr2bgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(lotr2b.grm.1pl), caption="Coefficient Esitmates for One Parameter Graded Response Model, LOT-R, Split B", label="tab:lotr2bgrm1pl"))
@ 

Table \ref{tab:lotr2bgrm1pl} clearly shows that there were no obvious problems with this model. The parameter estimates are relatively low in comparison with other scales, suggesting that these items were easier to endorse. 

<<lotr2bgrm2plprint, echo=FALSE, results=hide>>=
print(xtable(coef(lotr2b.grm.2pl), caption="Coefficient Estimates for Two Parameter Graded Response Model, LOT-R, Split B", label="tab:lotr2bgrm2pl"))
@ 


The two parameter GRM (not shown) suggested that the ability estimates have risen while the discrimination parameters have fallen for the majority of the items (except for 7). 


Finally, we assess the performance of each of these models on unseen data. 

<<lotr2bgrmtest, echo=FALSE, results=tex>>=
lotr2b.grm.1pl.test <- testIRTModels(lotr2b.grm.1pl, lotritems.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
lotr2b.grm.2pl.test <- testIRTModels(lotr2b.grm.2pl, lotritems.notb, gpcmconstraint=NULL, grmconstraint=TRUE)
lotr2b.grm.test.all <- rbind(lotr2b.grm.1pl.test, lotr2b.grm.2pl.test)

@ 

<<lotr2bgrmcompprint, echo=FALSE, results=tex>>=
print(xtable(lotr2b.grm.test.all, caption="Performance of One and Two Parameter Graded Response Models on Unseen Data (Splits A and C)", label="tab:lotr2bgrmtest"))
@ 

As can be seen from Table \ref{tab:lotr2bgrmtest}, the one parameter model performed best on the unseen data. 

<<lotr2caisp, echo=FALSE, results=hide, cache=TRUE>>=
lotr2c.aisp <- aisp(na.omit(lotritems2c))
@ 

<<lotr2ccheck, echo=FALSE, results=hide, cache=TRUE>>=
lotr2c.iio <- check.iio(na.omit(lotritems2c))
lotr2c.mono <- check.monotonicity(na.omit(lotritems2c))
lotr2c.s <- lotritems2c[,paste(lotr, c(3,4,7,9,10), sep="")]
@ 

An examination of the item ordering assumption showed that Q1 did not fit this model, and so was removed from the scale. There were no failures of the monotonicity assumption, and thus the modelling could commence. 



All Partial Credit Models suffered from non-monotonically increasing parameter estimates, and so are not analysed further here. 

Next, one and two parameter Graded Response Models were fit to the scale.

<<lotr2cgrm, echo=FALSE, results=hide, cache=TRUE>>=
lotr2c.grm.1pl <- grm(lotr2c.s, constrained=TRUE)
lotr2c.grm.2pl <- grm(lotr2c.s, constrained=FALSE)
@ 

<<lotr2cgrm1plprint, echo=FALSE, results=tex>>=
print(xtable(coef(lotr2c.grm.1pl), caption="Coefficient Esitmates for LOT-R One Parameter Graded Response Model, Split C", label="tab:lotr2cgrm1pl"))
@ 

Table \ref{tab:lotr2cgrm1pl} shows the estimated difficulty parameters for the one parameter Graded Response Model. It can be seen that the discrimination parameter is quite high, and that the most difficult question is Q10 which is ``Overall, I expect more good things to happen to me than bad''. The ``easiest'' question is Q7, which is one of the negatively phrased questions, suggesting that the two of these questions would be enough to garner a rough estimate of ability from participants. 


<<lotr2cgrm2plprint, echo=FALSE, results=hide>>=
print(xtable(coef(lotr2c.grm.2pl), caption="Coefficient Estimates for LOT-R, Two Parameter Graded Response Model", label="tab:lotr2cgrm2pl"))
@ 

When the estimates for the two parameter GRM (not shown) were examined, it can be seen that Q7 is  the most discriminating question, while still having the lowest ability estimates, suggesting that it is a very good question for seperating out optimism and pessimism. Q10 is still the most difficult, but not as discriminating as Q1 and Q10 (which refer to hopes around the future and indeed are very similar questions). 

Finally, the performance of these two models is tested against unseen data. 

<<lotr2cgrmtest, echo=FALSE, results=tex>>=
lotr.irt.notc <- lotritems.notc[,paste(lotr, c(3,4,7,9,10), sep="")]
lotr2c.grm.1pl.test <- testIRTModels(lotr2c.grm.1pl, lotr.irt.notc, gpcmconstraint=NULL, grmconstraint=TRUE)
lotr2c.grm.2pl.test <- testIRTModels(lotr2c.grm.2pl, lotr.irt.notc, gpcmconstraint=NULL, grmconstraint=FALSE)
lotr2c.grm.test.all <- rbind(lotr2c.grm.1pl.test, lotr2c.grm.2pl.test)

@ 

<<lotr2cgrmcompprint, echo=FALSE, results=tex>>=
print(xtable(lotr2c.grm.test.all, caption="Performance of LOT-R Split C One and Two Parameter Graded Response Models on Unseen Data", label="tab:lotr2cgrmtest"))
@ 

As can be seen from Table \ref{tab:lotr2cgrmtest}, the one parameter model provides the best fit to the unseen data. 




\section{Backtesting}
\label{sec:backtesting}

The final step in the analysis was to take the most successful model for the LOT-R and MAAS from the Sample Two fits and test them on the data from Sample One. 

This process was begun with the CFA model for the LOT-R. Each of the splits showed that a one factor solution provided the best fit to the data, but the IRT approach from Split B showed that Q1 did not provide a good fit to the scale. Therefore, for the backtesting process the two models examined were both one factor models, but the second one had Q1 removed from the scale.

<<lotrbacktestsem1full, echo=FALSE, results=hide, cache=TRUE>>=
optimism <- paste(lotr, c(1, 3, 4, 7, 9, 10), sep="")
latents <- c("Optimism")
manifests <- optimism
lotr.full.bt.model <- mxModel(name="LOTRBacktestFull", 
                              type="RAM",
                              manifestVars=manifests,
                              latentVars=latents,
                             mxPath(from=latents, to=optimism),
                              mxPath(from=manifests, arrows=2),
                              mxPath(from=latents, arrows=2, free=FALSE, 
                                     values=1),
                              mxData(observed=cov(na.omit(lotritems)), type="cov", numObs=356))
lotr.bt.full.fit <- mxRun(lotr.full.bt.model)
lotr.mod.formative <- mxModel(name="LOTRFullBTFormative", model=lotr.full.bt.model, mxPath(from=optimism, to=latents))
lotr.bt.mod.form.fit <- mxRun(lotr.mod.formative) ##doesn't complete
@ 

<<lotrbacktestsem1reduced, echo=FALSE, results=hide, cache=TRUE>>=
manifests.s <- paste0(lotr, c(3, 4, 7,9,10))
lotritems.s <- lotritems[,paste0(lotr, c(3,4,7,9,10))]
lotr.bt.small <- mxModel(name="LOTRBackTestReduced",
    type="RAM", 
                         manifestVars=manifests.s,
                         latentVars=latents,
                            mxPath(from=latents, to=manifests.s),
                              mxPath(from=manifests.s, arrows=2),
                              mxPath(from=latents, arrows=2, free=FALSE, 
                                     values=0.01),
                              mxData(observed=cov(na.omit(lotritems.s)), type="cov", numObs=356)) 
lotr.bt.small.fit <- mxRun(lotr.bt.small)
@ 

<<lotrbacktestsemcompare, echo=FALSE, results=tex>>=
print(xtable(mxCompare(lotr.bt.small.fit, lotr.bt.full.fit), label="tab:lotrbtsemcompare", caption="Performance of Full and Reduced Model for LOT-R on Sample One Data"), scalebox=0.7)
omxGraphviz(lotr.bt.full.fit, dotFilename="lotrfullbacktest.dot")
omxGraphviz(lotr.bt.small.fit, dotFilename="lotrsmallbacktest.dot")
@ 

Table \ref{tab:lotrbtsemcompare} shows the results of the full and reduced one factor model on the Sample One Data. It can clearly be seen that the reduced model provides a better fit, even though the RMSEA is quite high (0.09). All other fit indices are quite good, however. Therefore, this reduced scale will be used in the experimental portion of the research. 

Next, the fit of the different IRT models was examined using the same approaches as before. 

<<lotrbacktestgrm, echo=FALSE, results=hide>>=
lotrbacktest2b.grm.1pl <- testIRTModels(lotr2b.grm.1pl, lotritems, gpcmconstraint=NULL, grmconstraint=TRUE)
lotrbacktest2c.grm.1pl <- testIRTModels(lotr2c.grm.1pl, lotritems.s, gpcmconstraint=NULL, grmconstraint=TRUE)
lotrbt.test.all <- rbind(lotrbacktest2b.grm.1pl, lotrbacktest2c.grm.1pl)


@ 

<<lotrbtprint, echo=FALSE, results=tex>>=
print(xtable(lotrbt.test.all, caption="Performance of LOT-R Split C One and Two Parameter Graded Response Models on Unseen Data", label="tab:lotrbtgrmtest"))
@ 

As can be seen from Table \ref{tab:lotrbtgrmtest}, the model developed on Split C with only five items fit the data from Sample One better than did the full model. This further shows both the utility of this approach, and its usefulness in selection of models for the experimental portion of the research. 

\subsection{MAAS}
\label{sec:maas-1}

Next,  a similar procedure was applied to the MAAS. Both splits showed that a one factor solution and a one parameter Graded Response Model provided the best performance on unseen data. 

<<maasbacktestsem1full, echo=FALSE, results=hide>>=
manifests.maas <- paste0(maas, 1:15)
latents.maas <- "Mindfulness"
maas.bt.full <- mxModel(name="MAASBackTestFull", 
                        type="RAM",
                        manifestVars=manifests.maas,
                        latentVars=latents.maas,
                        mxPath(from=latents.maas, to=manifests.maas),
                        mxPath(from=latents.maas, arrows=2, free=FALSE,
                               values=1),
                        mxPath(from=manifests.maas, arrows=2),
                        mxData(observed=cov(na.omit(maasitems)), type="cov", numObs=356))
maas.bt.full.fit <- mxRun(maas.bt.full)
@ 

<<maasbacktestreduced1, echo=FALSE, results=hide, cache=TRUE>>=

maasitems.red1 <- paste0(maas, c(1:4, 7:15))
maasitems.s1 <- maasitems[,maasitems.red1]
latents <- "Mindfulness"
maas.bt.reduced1 <- mxModel("MAASBackTestReduced1",
                            type="RAM",
                            manifestVars=maasitems.red1,
                            latentVars=latents,
                            mxPath(from=latents, to=maasitems.red1),
                            mxPath(from=latents, arrows=2, free=FALSE,
                                   values=1),
                            mxPath(from=maasitems.red1, arrows=2),
                            mxData(observed=cov(na.omit(maasitems.s1)), type="cov", numObs=356))
maas.bt.red1.fit <- mxRun(maas.bt.reduced1)
@ 


<<maasbacktestreduced2, echo=FALSE, results=hide, cache=TRUE>>=
maasitems.red2 <- paste0(maas, c(1:4, 7:10, 12:14))
maasitems.s2 <- maasitems[,maasitems.red2]
latents.maas <- "Mindfulness"
maas.bt.reduced2 <- mxModel(name="MAASBackTestReduced2", 
                            type="RAM",
                            manifestVars=maasitems.red2,
                            latentVars=latents.maas,
                            mxPath(from=latents.maas, to=maasitems.red2),
                            mxPath(from=maasitems.red2, arrows=2),
                            mxPath(from=latents.maas, arrows=2, free=FALSE,
                                   values=1),
                            mxData(observed=cov(na.omit(maasitems.s2)), type="cov",
                                   numObs=356))
maas.bt.reduced2.fit <- mxRun(maas.bt.reduced2)
@ 

<<maasbacktestsemcompare, echo=FALSE, results=tex>>=
print(xtable(mxCompare(base=maas.bt.full.fit, comp=c(maas.bt.red1.fit, maas.bt.reduced2.fit)), label="tab:maasbacktestsemcompare", caption="Full and Reduced Models for MAAS Tested on Sample One Data"), scalebox=0.7)
@ 

As can be seen from Table \ref{tab:maasbacktestsemcompare}, the reduced model without questions 5, 6 and 15  provided the best fit to the unseen data. 



\subsection{IRT Backtesting - MAAS}
\label{sec:irt-backtesting-maas}

The next step in the backtesting procedure was to test the IRT models on Sample One. 


<<maasbacktestirt, echo=FALSE, results=hide, cache=TRUE>>=

maas.bt.grm.1pl2b <- testIRTModels(maas2b.grm.1pl, maasitems.s1, gpcmconstraint=NULL, grmconstraint=TRUE)
maas.bt.grm.1pl2c <- testIRTModels(maas2c.grm.1pl, maasitems.s2, gpcmconstraint=NULL, grmconstraint=TRUE)
@ 

<<maasbacktestresprint, echo=FALSE, results=tex>>=
maas.bt.grm.test.all <- rbind(maas.bt.grm.1pl2b, maas.bt.grm.1pl2c)
print(xtable(maas.bt.grm.test.all, label="tab:maasbtgrmtest", caption="Backtesting of MAAS IRT Models on Sample One Data"))
@ 


It can be seen that the model developed from Split C (without items 5,6 and 15) provides the best fit to the unseen data, and this model will be used to generate scores for the experimental participants. 
\section{Discussion}

\subsection{Optimism and Health}
\label{sec:optimism-health-1}



There are a number of interesting findings which have emerged from
the study.  The most striking is the large negative correlation between
optimism and self reported health.  The sample in this study is quite
large, so the result is unlikely to be a statistical fluke.  That being
said, the result is problematic to explain, given the large amount
of evidence of beneficial effects of optimism on health\cite{rasmussen2009optimism}.

However, there have been some findings where higher optimism
has not has been associated with health outcomes.  There are two major
explanations for the curious and unexpected phenomenon,given the links
established by a recent meta-analysis \cite{rasmussen2009optimism}.
The first centers on the dimensionality of the optimism construct.
Many believe that these results are caused by optimism self report
measures being reflections of two interlinked constructs, optimism
and pessimism \cite{Herzberg2006} as established in a factor analytic
study in a sample of over 46,000 participants.  Some authors claim that
the apparently contradictory results suggest that the pessimism part
of the construct is the driver of the effects on health, and that
the correlations between the two constructs decline with age.  This
viewpoint was partially supported by the recent meta-analysis which
found that pessimism had a larger effect on health, though the difference
between optimism and pessimism was not significant \cite{rasmussen2009optimism}.
The other viewpoint argues that the effects of optimism on health
are mediated by negative and positive affect, and that high levels
of negative affect can either negate or reverse the optimism-health
link \cite{Baker2007}.  The aforementioned Baker study found that
the optimism health link was entirely mediated by negative affect.
Nonetheless, the balance of the evidence suggests that optimism has
beneficial consequences for health and healthy behaviours.

An explanation for this finding might be that it was the result
of high levels of negative affect in the population.  However, this
variable was not measured, so such an explanation can be regarded
as speculative at best. Given that the model mediating the optimism health link with   It is worth noting however, that the original
literature of the beneficial effects of optimism on health focused
on cellular immunity, which is obviously quite different from self
reported health.A problem with this explanation is that reports in the
literature indicate that the optimism-health link is larger when self
report methods are used \cite{rasmussen2009optimism}.  

Age may alsohave been a factor, as the regression weight for this vaiable was negative, which suggests that the relationship may have been different
if this research had been carried out in a sample with a broader distribution of ages.  We do not have a good explanation for this finding.
This issue could be resolved with a prospective study
measuring optimism and health at baseline, and having participants
report health problems and visits to medical professionals over the
course of a year.  Such a study could allow the casual chains of this
effect to be untangled.

The Structual Equation Model reported for Sample One is a good place to start in this regard.
The only models which converged were those which moderated the effect of health on optimism 
through either Emotional Well Being and Mindfulness. Although the fit of the emotional well being 
model was marginally superior, there are good reasons to believe that mindfulness is more 
important (c.f. the experimental portion of the research). 


\subsection{Mindfulness}
\label{sec:mindfulness}

Another interesting finding which arose from this research is the
impact of mindfulness scores on other health variables.  MAAS scores
correlated positively with all of the health sub-scales, very significantly
in the case of emotional well-being.  This may suggest that brief mindfulness
interventions may be of use for improving overall population health,
both physical and mental.  That being said, it is important to note  that the issues surrounding the mindfulness construct
itself and its relations with mindfulness meditation practice need
to be resolved before such strong conclusions can be drawn.

Another fascinating finding in this research is the strong negative correlation
between mindfulness and optimism.  Our study appears to have been the first to assess
these constructs using self-report measures, and this finding was not expected to occur.
MBSR programs have been found to increase optimism in a number of studies \cite{Carson2004},
but our results seem to show that mindfulness and optimism may be inversely related.


There are a number of reasons why this could be so.  Optimism is defined as generalised positive outcome expectancies about the future,
while mindfulness is defined as non-judgmental awareness of the content of thoughts.
It seems plausible that increased mindfulness could lead persons to become less optimistic,
 as their newfound awareness of their own thought patterns and behaviours makes them aware
that events have not always worked out well.  This increased awareness could temper future
assessments of the future, and decrease optimism as measured by the Life Orientation Test Revised. However, work in the experimental sample casts new light on this theory, and is discussed further in Chapter \ref{cha:primary-research}. 

\subsection{Psychometric Analysis}
\label{sec:psych-analys}



This study also confirms the proposed one factor structure for the MAAS, in line with previous research.
This sample also appears to show that the LOT-R can be modelled without loss of information with
just one factor.  We also demonstrated a replicable and parsimonous 4 factor structure for the RAND MOS,
and our results cast further doubt on the notion that these factors are uncorrelated.

It is worth noting that in all cases, parallel analysis did not provide a good measure of
the best number of factors to retain.  For all three measures, the MAP criterion provided
a more accurate metric.  This may have resulted as parallel analysis procedures tend
to sample from a normal distribution, and this condition was not met for any of our
variables.  We would argue that the use of multiple decision criteria on a regular basis
in factor analytic research would help us to understand which method suits a particular
application best.

Additionally, this study found that a combination of IRT analyses and CTT analyses provided the best fitting models for both the MAAS and the LOT-R. In both cases, a model fitted using the scales developed from the use of Mokken analysis and IRT fit indices provided better performance on unseen data than did the models developed exclusively through factor analysis. 

This provides evidence that the combination of these two methods is more useful than either of them alone, and it has allowed for the building of a useful model tailored to the population under study in this thesis which can be used in the experimental portion of the research. The success of these models is discussed further in Chapter \ref{cha:primary-research}.

%% A major limitation of this study was the exclusive use of self report
%% measures, and a student sample.  That being said, the sample was large
%% and representative of the general student population.  Given that over
%% 70\% of Irish people of this age group now attend college, it could
%% be argued that this sample is relatively representative of Irish young
%% people at large.  This, however, is somewhat speculative and further
%% research would need to investigate this proposition further.

%% In conclusion, this study points towards the importance of considering
%% psychosocial variables and their impact on health, and suggests that
%% further research is needed to examine how these psychological variables
%% are mediated by culture into differential biological outcomes.

In conclusion, this portion of the thesis aimed to develop tailored measures for important covariates of the relationship between placebo and implicit and explicit expectancies, and this has been achieved. Additionally, this portion of the research sheds a new perspective on the relationships between self-reported health, optimism and mindfulness. 


<<writefileforexperiment, echo=FALSE, results=hide, cache=TRUE>>=
save.image("healthforthesis.rda")
@ 


%%% Local Variables:
%%% TeX-master: "PlaceboMeasurementByMultipleMethods"
% %%% End:
