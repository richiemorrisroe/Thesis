
\section{Introduction}

This chapter describes the experimental portion of the research, which involved the assessment of treatment expectancies using implicit and explicit measures. This section further involved the testing of a number of proposed theoretical models around the relationship of explicit, implicit and physiological measures to the placebo response. The justification behind these theoretical models is described in Chapter~\ref{cha:methodology}. 

This chapter proceeds with the following sections:

\begin{enumerate}
  

\item A short rationale for this study

\item A look at some of the difficulties with measurement of placebo responses via self-report

%% \item A look at some broader measurement issues within placebo research more generally

\item A description of the methodology employed in this chapter to test the major hypotheses and mitigate some of the issues described above

%% \item A description of the major preliminary checks and tests to ensure that the data was as useful as it could be

\item A testing of the major hypotheses around the placebo and its relationship to implicit and explicit treatment expectancies. 

\item  A discussion outlining the major issues raised by this chapter.
\end{enumerate}

\subsection{Placebo Effects and Implicit Measures}
\label{sec:plac-effects-impl}

As discussed in Chapter \ref{cha:literature-review}, Section \ref{sec:sec:rese-base-comb} there are some reasons to believe that the placebo effect is impacted by implicit influences. These are the following:

\begin{itemize}
\item Research shows that priming can increase the size of placebo effects;
\item The results of Kirsch (1998) where he showed that informed conditioning was less effective than uninformed condition would seem to argue that the effectiveness of placebos can be reduced by conscious processing;

\item Some theorists have suggested that the IAT captures substantial state variance (as evidenced by low test-retest reliability) and it has also been argued that response to placebo is similar, thus suggesting that there may be a link~\cite{Whalley2008}

\item Research has shown that the awareness of priming manipulations can decrease the size of the effect, again suggesting that an implicit measure might prove useful in measuring this effect.
\end{itemize}


The implicit measure chosen for this purpose was the Implicit Association Test (IAT). This choice was made both because it is the most widespread, and as such more of the best practices and problems with the measure are available in prior research. 

These problems (discussed at length in Chapter \ref{cha:literature-review}, Part \ref{part:impl-assoc-test}), have included:

\begin{itemize}
\item Low test-retest reliability (approximately 0.49)

\item method variance affecting the results

\item Issues around the scoring and interpretation of the results
\end{itemize}

This chapter addresses these problems in the following manner. Firstly, the low test-retest reliability may be an advantage in that it represents state-based variance (and indeed the IAT has been shown to predict spontaneous behaviour better than explicit measures) and as such could be an (admittedly small) advantage in the prediction of placebo. The method variance problem was examined by assessing the correlations between the two IAT measures used in this study (Treatment Credibility and Optimism) and if these were two high (r>=.3) then controlling for them by including both IAT's in each regression model. 

\subsection{Placebo effects and deception}
\label{sec:plac-effects-decept}

The commonly accepted model of the placebo effect claims that deception is necessary for the effect to occur. The entire theory behind randomised controlled trials suggests that because participants respond to the mere administration of medicines, then a control (the placebo) is needed to allow the true effect of the drug to be established. A necessary part of this design is that the participants in the placebo group need to believe that there is a chance that they are getting the real treatment. 

Some theorists have argued against this~\cite{Deventer2008,Evans2002}, and a recent randomised controlled trial has shown that compared to no treatment, an open placebo can perform significantly better~\cite{kaptchuk2010placebos}.

The Kaptchuk \textit{et al} study, which was a three week randomised controlled trial using IBS patients showed that administration of an open-label placebo was associated with a significant improvement (d=0.79) at in the IBS Global Improvement Scale (the major outcome measure) the 21 day endpoint for the study. Similar trends were observed for other symptom severity ($d=0.53$) and quality of life ($d=0.40$)

However, this study did not include a deceptive placebo condition, and so is not a true test of the theory that deception is unnecessary for the placebo response. One secondary aim of this study was to examine the theory that placebo responses could be induced without deception.

\subsection{Explicit Measures and Placebo}
\label{sec:expl-meas-plac}

Another issue with current placebo research relates to expectancies. The first issue is that expectancies are typically measured with a simple one question scale. One of the aims of this thesis was to apply psychometric methods to the devlopment of a more sophisticated measure of treatment credibility and expectancies (see Chapter \ref{cha:tcq-thesis}). The other problem is that expectancies are assumed to be conscious, even though they appear to have far more in common with unconscious responses than they do with controlled processes. 

In brief, the measure adapted in Chapter \ref{cha:tcq-thesis} had six questions for each of the six different treatment modalities, which included conventional (Pills, Creams, Injections) and Alternative (Acupuncture, Homeopathy, Reiki) treatments. Three of the questions related to expectancy, while three related to credibility (factors found in the original development of the instrument). These 36 questions were reduced to 18 using factor analytic and IAT measures, to allow for easier administration (c.f. Section \ref{sec:reducing-size-tcq}).


%% \subsection{Detecting the existence of placebo effects}

%% The study of placebo effects is without consequence if it is impossible to establish that a placebo effect has or has not occurred in a controlled setting. One of the largest problems with establishing this is when the outcome is a subjective one, such as pain (which is the domain in which the placebo was examined in this chapter). Pain is a subjective experience, and equivalent stimulus levels may cause entirely different reactions  in two different people \cite{Kirsch1997}. 

%% This issue is often controlled for by calibration of the stimulus levels to the individual participant in an experiment. This allows the researcher to assess to what extent these pain levels are altered by the placebo treatment. Normally, pain intensity and unpleasantness levels are assessed on an eleven point Visual Analogue Scale (VAS)  and these ratings are used as the input data for statistical analysis of different groups. 

%% However, the use of these self report instruments carries with it a number of problems. Firstly, the issue of demand characteristics arises as a consequence of this. Demand characteristics refer to the subtle pressures faced by participants in experiments to live up to the expectations of the researcher \cite{weber1972subject} and were discussed in Section \ref{sec:plac-rand-trials} in Chapter \ref{cha:literature-review}. The idea is that if a researcher really wants to demonstrate something, then he or she will communicate this subtly to the participants in the study, and they will (theoretically) respond in the manner in which the researcher prefers. This factor is one of the major reasons why tests of new drugs must blind the experimenter as well as the participants. 

%% Response biases can often be an issue in controlled studies of placebo response. The problem is that if participants receive a treatment which they believe to be effective, then their thresholds of perception may be altered without any actual biochemical changes.

%% This is the theory put forward by Allan \& Siegel's \cite{Allan2002} analysis of the placebo phenomenon in terms of signal detection theory. These two factors were proposed to account for the entirety of the placebo effect in pain by Hrobjarrstsson \& Goetzche \cite{hrobjartsson2001}.

%% This interpretation of placebo seems less likely given that placebo effects have been observed on biochemical parameters, and also  given that placebo analgesia can be removed by the administration of naloxone~\cite{Levine1979,benedetti2003a} (c.f. Chapter \ref{cha:literature-review}, especially the section on physiological effects of placebo).

%% \subsection{Measurement Issues}
%% \label{sec:measurement-issues}

%% The measurement of placebo has improved greatly over the past fifty years since Beecher started examining the phenomenon. However, it still suffers from a number of problems.

%% The first problem is the consistent use of ANOVA and regression methods to examine differences between groups \cite{Colloca2008b,Pollo2001}. While there is nothing intrinsically wrong with this method, it is a waste of statistical power. The issue here is that  much placebo analgesia research typically only examines group differences in scores. The problem with this approach is that typically, pain scores are collected over time, and so the responses of one participant at time $t$ are not independent of the responses of this participant at $t+1$. This violates the assumptions of ANOVA (and of linear regression). While a simple binary responder/non responder classification is possible, this approach throws away large amounts of information. One study which did use more appropriate methods was \cite{Bausell2005} who examined the effects of expectancies and acupuncture using survival analysis. 

%% More appropriate methods for these analyses are time series analyses and survival analysis, both of which are discussed below. Additionally, in trials of functional bowel disorders, placebo effects correlate with the size of the trial, which suggests that regression to the mean effects are having an impact \cite{Enck2005a}. 

%% The next major problem with the current analytic strategies taken towards placebo is that most of the techniques used (ANOVA, linear models etc) assume that the mean is an appropriate measure of central tendency. However, given that placebo response can be modelled as binary (it either occurs or it does not) then the distribution would be expected to be bi-modal, in which case the mean can be very misleading. In reference to this point, a re-analysis of a number of placebo trials showed vast differences between the mean and median placebo responses \cite{McQuay1996}. 

%% Additionally, the additive model of drug and placebo interactions may be an issue. This is typically taken as true, which is a position which may have had validity before large scale computing resources were available, but this constraint does not apply any more \cite{Caspi2002}. The issue of additive placebo and drug interactions was discussed further in Chapter \ref{cha:methodology}. 







\subsection{Modeling Placebo by Multiple Methods}
\label{sec:modell-plac-mult}

This section will present the overall approach and rationale for the work carried out in the course of this chapter.

The placebo effect is a complex phenomenon. It has been established that it can be predicted by some variables which are typically measured using self report approaches. These variables include expectancies and optimism. Optimism was measured using the Life Orientation Test, Revised (LOT-R - see Chapter \ref{cha:health-for-thesis} for details on this instrument), and a new measure for treatment credibility expectancies was developed in  Chapter \ref{cha:tcq-thesis}. 

Additionally, these variables only explain a small proportion of the variance in the observed placebo response. It is the contention of this researcher that some of the residual variance can be predicted by the use of implicit measures (specifically the IAT). Therefore, two implicit measures, one of Treatment Credibility and the other of Optimism were developed (see Chapter \ref{cha:devel-impl-meas} for details). 

The relationship between the observed placebo response, and these explicit and implicit measures was of primary importance to this research, and so a measure of Mindfulness (Mindful Attention Awareness Scale) was also used. The rationale behind this choice of measure was explained in Chapters \ref{cha:literature-review} and \ref{cha:health-for-thesis}.

In order to make use of psychometric modelling, large samples of the self report instruments were collected in the same population from which the experimental participants were drawn. This allowed for factor score and IRT models to be built for each of the measures. This work was reported in Chapters \ref{cha:health-for-thesis} and \ref{cha:tcq-thesis}. 

In the experiment itself, physiological (GSR or skin conductance) measures were collected in order to examine the physiological characteristics of placebo and to allow for another form of measurement to be included in the final model.

The collection of these various forms of data, along with a behavioural criterion (the observed placebo response) allowed for psychometric models (Structural equation models) to be examined to seperate out the effects of the various predictor types. In essence, this thesis (and more specifically this chapter) seeks to marry the strengths of psychology in psychometric modelling to its complementary strengths in experimental design, with the aim of establishing these methods and measures relative usefulness in the prediction of placebo. Much more detailed descriptions of the major models applied are given in Chapter \ref{cha:methodology}. 


\section{Methodology}

\subsection{Experimental Procedure}
<<healthenv, echo=FALSE, results=hide>>=

## load("healthforthesis.rda")
## load("tcq2.rda")
load("homdata.rda")
load("credtotals.rda")
## load("tcqthesis.rda")
@ 

\subsubsection{Recruitment for the Experimental Procedure}

Following piloting, a random subset of students were emailed to ask if they would like to participate in the experiment. Given that the experiment took place in the Applied Psychology department, somewhat off campus and that the experiment involved suffering painful stimuli, an inducement of a smartphone was offered to one participant who completed the procedure on the basis of a draw following the completion of the research.  After this email was sent and participants recruited, another email was sent to Zoology, Ecology and Plant Science students, along with Psychology students, as all of these students had lectures in the Psychology building. In addition to this, an email was sent to all of the researchers acquaintances on popular social networking sites. Three more emails were sent to random samples from the all students mailing list, and the experiment ran from January 17th until April 14th inclusive.

\subsubsection{Measures used in the Experiment}
\label{sec:meas-used-exper}

The following measures were used in this experiment. Firstly, Age, Gender and course of study were collected for each participant. The MAAS and LOTR were also administered to each participant, as was a shortened version of the treatment credibility questionnaire (described in Chapter \ref{cha:tcq-thesis}, Section \ref{sec:reducing-size-tcq}). Following the completion of the explicit measures, participants completed a Treatment Credibility IAT and an Optimism IAT. The stimuli used in each of the IAT's were as follows:
\paragraph{Treatment Credibility IAT:}
\begin{itemize}
\item Conventional: Creams, Pills, Surgery, Injections 

\item Alternative: Homeopathy, Acupuncture, Reiki, Flower Essence

\item Real: Real, Accurate, True

\item Fake: Fake, Inaccurate, False
\end{itemize}



\paragraph{Optimism IAT}
\begin{itemize}
\item Optimism: Optimism, Happy, Improving, Succeeding

\item Pessimism: Pessimism, Unhappy, Disimproving, Failing

\item Self: Me, Mine, Myself

\item Other: You, Yours, Yourselves. 
\end{itemize}





Further details of the development and piloting of each IAT are in Chapter \ref{cha:devel-impl-meas}. 
In addition, participants gave verbal reports of their pain levels to the experimenter at one minute intervals, and these were recorded (along with condition and exact time of application of bandage and when the squeezing stopped) on a sheet of paper, along with the participants identification number. 


\subsubsection{Experimental Procedure}

All participants were met at the entrance to the building by the primary researcher. They were given the informed consent documentation, and after they signed it, they completed three questionnaires (the MAAS, the LOT-R and the TCQ). Following this, they completed both an Optimism IAT and the Treatment Credibility IAT, where order of administration was counterbalanced across participants.

Following this, the participants sat down next to the Biopac physiological monitoring data, and baseline data was recorded for five minutes.  Then, a blood pressure gauge was wrapped around the upper part of the non-dominant hand of the participant, and they were asked to squeeze a hand exerciser twenty times for two seconds each time. One minute after this, and every minute thereafter, participants were asked to rate their pain on a VAS from 1 to 10.

If the participant was in the Deceptive or Open Placebo group, then when they rated their pain as 7 or higher, the placebo cream was applied. The experiment continued until the participant either decided to withdraw, their pain rating reached 10 or 45 minutes elapsed  from when the bandage was applied. EDA recordings were taken one thousand times per second second using the Biopac equipment and VAS ratings were recorded on paper by the experimenter. The placebo cream consisted of moisturiser in a pharmacuetical container, and was unlabelled.

Participants in the treatment group were told that \textit{the cream was a potent painkiller, recently approved and proven to reduce pain, which would take effect almost immediately}. Participants in the placebo group were told that \textit{they were receiving a placebo and that placebos have been clinically proven to reduce pain, and that it would take effect almost immediately}.

\subsubsection{Analysis of Experimental Data}

The first step in the analysis of experimental data was to examine the comparability of each of the different groups. This procedure was carried out with t-tests (for numerical data) and ANOVA's (for categorical data). There were no differences in pre-treatment levels for any of the variables, and so analytical techniques did not need to include any variables to ensure comparability. This finding demonstrated that the randomisation was successful. 



\paragraph{Analysis of Pain Rating Data}
\label{sec:analysis-pain-rating}

The pain ratings were modelled as time series, and individual and gender, condition and group level ARIMA models were fitted to the data. In addition, their cross-correlations were examined with the physiological time series collected as part of the placebo analgesia experiment. 


%% \paragraph{Practical Model Building Strategy}

%% The following are general steps towards building an ARIMA model.

%% \begin{itemize}
%% \item Inspect the plots both of the series and the autocorrelations
%% \item Examine autocorrelations, differencing if necessary
%% \item Estimate parameters, ensuring that they are significantly different from zero, and within the bounds of invertibility
%% \item Examine residuals, if they are not white noise, repeat steps 1-4 until they are.
%% \end{itemize}

%% \paragraph{Event History Analysis}
%% \label{sec:event-hist-analys}
%% In this research, a particular form of time series analysis was used, which is known as event history analysis \cite{mccleary1980applied}. This type of analysis partitions the time series into two or more parts, based on whether or not a particular event has occurred. In the case of this research, there were either two or three parts to the analysis. In the Deceptive and Open Placebo groups (see Chapter \ref{cha:primary-research}), the first time series occcurred until the painful stimulus was applied, the second was the time from this point until the placebo was applied, and the third was this time point until the end. In the No Treatment group, there were two time series, one for the period before the pain was applied, and one after.

%% A major advantage of this method is that it allows us to examine changes in the parameters of the time series as a function of experimental stage and condition. This allowed us to estimate more precisely what changes occurred over time as a result of experimental procedure. The basic procedure is as above, except that parameters are estimated on a subset of the data, and cross-correlation functions are used to examine the changes between them.

\paragraph{Analysis of Placebo Response Data}
\label{sec:analys-plac-resp}

The first step was to classify participants as either placebo responders or non-responders. This was done simply by examining if their pain levels decreased following administration of placebo. If this happened, they were classified as placebo responders, and if not, they were classified as non-responders. %% Another approach taken was to examine the number of minutes spent with a pain rating of less than seven following administration of the placebo, and to model this as a Poisson variable using a generalised linear model. 

In addition, these models were compared against models using both factor scores and IRT ability estimates to determine the usefulness of this model based approach. 

%% Because of the autocorrelation inherent in the pain ratings, general linear models were not entirely appropriate for this data. Therefore, the autocorrelation structure was determined for the pain ratings, and a generalised linear mixed model was fitted to the data using the IAT, explicit and physiological data as predictors. These models were carried out using stepwise, lasso, ridge and least angle regression methods, and validated using ten-fold cross validation. 


\paragraph{Analysis of Physiological Data}
\label{sec:analys-phys-data}

Skin conductance recordings (GSR) were taken as part of this experiment. This source provided recordings at 1000Hz for the entire procedure. %% The ECG data was analysed to extract heart rate, heart variability  and QT and RR intervals.
%% In addition, both of these time series were examined using event history analysis (see Section \ref{sec:event-hist-analys}), which modelled the time series in three intervals (before administration of gauge, before administration of placebo, after administration till end) for the Deceptive and Open Placebo Groups and two series (before administration of gauge and afterwards) for the No Treatment group.

Finally, the relationships between physiological responses and the psychological (both implicit and explicit) variables collected, were examined. More details on the hypotheses regarding this can be found in  Chapter \ref{cha:methodology}.


\subsubsection{Analysis of Reaction Time Data}

Reaction time data has been studied by (mostly cognitive) psychologists for many years. The Implicit Association Test has been used in almost 300 published papers and reports. However, with a few exceptions, there has been almost no overflow from one area of study to the other.

%% Classic work in examining the distributions of reaction time data was carried out by Ratcliff\cite{ratcliff1979group,ratcliff1993methods}. In the 1979 paper he suggested that a quantile based approach should be used for individual reaction time scores. This involves ranking each of the latencies for each individual participant, and using certain percentages of these as quantiles, which can then be used to estimate group distributions. This approach will be utilised in this research, and four quantiles will be used, as given that some conditions (Blocks one, two and four) have only twelve observations, and quartiles divide each of the block sizes (12 and 36) equally. These quartiles were then used to estimate group, block and condition level distributions for the reaction time data. 

The typical approach to analysis of IAT data goes as follows  \cite{Greenwald1998}: firstly, the data is checked for outliers. Outliers, in this case are defined as responses less than 300ms and greater than 3000ms. Any such outliers are recoded to 300 or 3000ms respectively. Following this procedure, a mean is taken of the items in each condition. These means along with standard deviations are reported. The response latencies are then log transformed (to reduce positive skew) and the IAT score is calculated as follows. 

Given the participants mean latency in each condition, their IAT score is the mean for the incompatible condition (i.e. White + Unpleasant) less the mean from the compatible condition (i.e. White + Pleasant) divided by the average of the two within group standard deviations. This measure is typically called $D$, and was developed after analysis of an extremely large sample of IAT responses~\cite{Greenwald2003a}. In addition to the change of scoring procedure in the 2003 revisions, the threshold for outliers was also substantially widened to 10000 ms. Given that the sample used for this re-analysis was over one hundred thousand, this widening of the threshold was presumably based on experience with a much broader population than was used in many early IAT experiments.

%% There are a number of problems with this approach. Firstly, the approach throws away much information, more than once. In the first case, information regarding extreme responses is censored, in an unsystematic and theoretically unjustified manner. The breakpoints of 300 and 3000 milliseconds appear to have been chosen to allow for the use of the mean as a group value rather than for any principled reasons. 

%% It is arguable as to whether or not this censoring represents a good strategy, but certainly it is something which should have been examined in a principled fashion, which does not appear to have happened. The second issue relates again to the calculation of a mean.

While means are useful summary statistics, they are most optimal in situations where the distribution is unimodal and symmetric~\cite{venables2002modern}. Reaction latency data are neither, so the choice of mean seems to have been made from familiarity rather than principle. 

%% The choice of log transform (while slightly more justifiable) is decidedly inferior to a more data driven approach. Again, the issue here is not that such choices in analysis were made, but rather that they have been made once and repeated many times in the later literature. Even those who have criticised the IAT \cite{Klauer2005,Mierke2003,Blanton2006} on methodological grounds appear to have ignored this issue. 

Given the typical right skew observed in reaction time distributions, the median would seem to be a much better measure of central location than would the mean. This right skew typically occurs as there is a hard bound on how quickly a participant can respond, but no such bound (unless enforced by the procedure) in the maximum time taken to respond. Therefore, in this research, the efficacy of these different measures of central tendency will be reported.

The differences between the mean and median based approaches will be examined and reported, to determine if it makes any difference. As it turned out, in terms of the overall IAT scores, the mean and median were highly correlated and the use of one measure versus the other made no difference to the results. 

%% \paragraph{Analysis of IAT data}

%% Firstly,  an ex-Gaussian distribution was fit to the data. This distribution is a mixture distribution of a normal and an exponential, and can be used to account for the extremely long tails present in such data. If this distribution proved a reasonable fit to the data, then estimation of confidence intervals and predictions could be made more easily from the data.
%% In addition, as described previously, quantiles were determined for each participant in each Block, and these individual level quantiles were used to derive the group level distributions for each block and condition. 
%% Another approach was taken to the data in tandem with this. This approach involves ordinal test theory \cite{schulman1975test}. All of the reaction time data occurs on a common scale. The differences between the responses to a word ,$w$ in condition 1 and condition 2 are on the same scale, and a distance matrix can be constructed from this data. This distance matrix can either be ranked (which makes fewer assumptions and is less affected by outliers) or the Euclidean distance can be used in analysis. The advantage of the distance approach is that it discards less information, though at the cost of making more assumptions. A clustering approach was then used, to assess whether or not the stimuli fell into the same categories. This clustering approach was repeated 10 times with different seeds and the results averaged in order to reduce random variability inherent to the k-means approach. $k$ was also varied from 1 to 10, and the results averaged also.

%% The ordinal/euclidean matrix was then factor analysed and examined using item response theory to assess its psychometric structure, and assess fit or lack thereof.

%% Another approach which was applied to this data was to fit Samejina's continuous response model, which is a generalisation of the Graded Response Model % (described above in Section \ref{sec:polyt-item-resp})
%% to the reaction time data. This procedure aimed to produce ability estimates which were then applied to the prediction of placebo response data. 

\subsubsection{Structural Equation Models}
\label{sec:struct-equat-models}

Finally, the models described in Chapter \ref{cha:methodology} were fit to the data. These models allowed for the principled comparision of a number of different theoretical models on the relationship between expectancies, optimism and physiological variables on the placebo response. 


\section{Results}
\label{sec:results}




<<importdata, echo=FALSE, results=hide>>=
setwd("./ExperimentDataforR/FullStudy/")
expmeasures <- read.csv("explicitmeasuresfixed.csv")
expmeasures[,"PpNo"] <- with(expmeasures, sprintf("%04d", PpNo))
expfull <- read.csv("explicitmeasuresfull.csv")
expmeasures[,"LOTR"] <- with(expmeasures, LOTR/6)
vasscores <- read.csv("VASscores.csv"       )
optiat <- read.csv("optiatres.csv" )
tcqiat <- read.csv("tcqiatres.csv")
setwd("../..")
@ 



<<removeduplicates, echo=FALSE, results=hide>>=
dup.exp.small <- expmeasures[with(expmeasures, which(duplicated(PpNo))),]
dup.exp.full <- expfull[with(expfull, which(duplicated(Participant))),]
dup.all <- unique(c(dup.exp.small$PpNo, dup.exp.full$Participant))
dup.exp.vasscores <- vasscores[with(vasscores, which(duplicated(PPNo.))),]
vasscores2 <- vasscores[!(with(vasscores, PPNo.) %in% dup.all),]
expmeasures2 <- expmeasures[!(with(expmeasures, PpNo) %in% dup.all),]

@ 


<<loadpackages, echo=FALSE, results=hide>>=
require (randomForest)
require (cacheSweave)
require (gridExtra)
require(psych)
require(xtable)
require(arm)
require(ggplot2)
require(reshape2)
require(eRm)
require(ltm)
require(boot)
require(plyr)
require(caret)
require(survival)
require(partykit)
require(OpenMx)
require(lavaan)
source("func.R")
require(zoo)
require(forecast)
require(semPlot)
@ 

<<iatsort, echo=FALSE, results=hide>>=
rotate.labels <- theme(axis.text.x=element_text(angle=-45, hjust=1))
tcqiatsorted <- tcqiat[,c("Participant", "Date", "Time", "Block", "Accurate", "Actual", "Real", "Truth", "Fake", "Illusory", "Inaccurate", "Lies", "Cream", "Injections", "Pills", "Surgery", "Acupuncture", "FlowerEssence", "Homeopathy", "Reiki", "Correct", "BlockTime")]
optiatsorted <- optiat[, c("Participant", "Date", "Time", "Block", "Me", "Mine", "Myself", "Theirs", "Them", "Themselves", "Better", "Happy", "Improving", "Succeeding", "Disimproving", "Failing", "Sad", "Worse", "Correct", "BlockTime")]
@ 

<<optiatscore, echo=FALSE, results=hide>>=
optiatsorted[,"Block"] <- with(optiatsorted, gsub(":", "", x=Block))
optiatscore.mean <- calcIatScores(optiatsorted,Code="Participant", method="mean", words=c("Me", "Mine", "Myself", "Theirs", "Them", "Themselves", "Better", "Happy", "Improving", "Succeeding", "Disimproving", "Failing", "Sad", "Worse"))
names(optiatscore.mean)[1] <- "Participant"
names(optiatscore.mean)[6] <- "OptIAT.Mean"
optstimblock3 <- optiatscore.mean[,grep("Block3.", x=names(optiatscore.mean))]
optstimblock5 <- optiatscore.mean[,grep("Block5.", x=names(optiatscore.mean))]
optiatscore.median <- calcIatScores(optiatsorted,Code="Participant", method="median", words=c("Me", "Mine", "Myself", "Theirs", "Them", "Themselves", "Better", "Happy", "Improving", "Succeeding", "Disimproving", "Failing", "Sad", "Worse"))
names(optiatscore.median)[1] <- "Participant"
names(optiatscore.median)[6] <- "OptIAT.Median"
optiatscore <- merge(optiatscore.mean[c(1,6)], optiatscore.median[,c(1,6)], by="Participant")
part.opt <- unique(optiat[,"Participant"])
part.tcq <- unique(tcqiat[,"Participant"])
optstimblock3 <- as.data.frame(cbind(part.opt, optstimblock3))
optstimblock5 <- as.data.frame(cbind(part.opt, optstimblock5))
@ 

\subsection{Analysis of IAT data}
\label{sec:analysis-iat-data}
The first step in the analysis of IAT data is to examine the differential impact of using the mean versus the median as the measure of central tendency for the calculation of IAT scores (the $D$ measure). The results  showed that there were no major changes attributable to this difference (the correlation between the two scores was $r=0.91$). 

<<tcqtestblocks, echo=FALSE, results=hide>>=
tcqiatsorted[,"Block"] <- with(tcqiatsorted, gsub(":", "", x=Block))
tcqwords <- c("Accurate", "Actual", "Real", "Truth", "Fake", "Illusory", "Inaccurate", "Lies", "Cream", "Injections", "Pills", "Surgery", "Acupuncture", "FlowerEssence", "Homeopathy", "Reiki")
tcqiatscore.mean <- calcIatScores(tcqiatsorted, Code="Participant", method="mean", words=tcqwords)
tcqstimblock3 <- tcqiatscore.mean[,grep("Block3.", x=names(tcqiatscore.mean))]
tcqstimblock5 <- tcqiatscore.mean[,grep("Block5.", x=names(tcqiatscore.mean))]
names(tcqiatscore.mean)[1] <- "Participant"
names(tcqiatscore.mean)[6] <- "TCQIAT.Mean"
tcqiatscore.median <- calcIatScores(tcqiatsorted, Code="Participant", method="median", words=tcqwords)
names(tcqiatscore.median)[1] <- "Participant"
names(tcqiatscore.median)[6] <- "TCQIAT.Median"
tcqiatscore <- merge(tcqiatscore.mean[,c(1,6)], tcqiatscore.median[,c(1,6)], by="Participant")
part.tcq <- unique(tcqiat[,"Participant"])
tcqstimblock3 <- as.data.frame(cbind(part.tcq, tcqstimblock3))
tcqstimblock5 <- as.data.frame(cbind(part.tcq, tcqstimblock5))
IATscores <- merge(optiatscore, tcqiatscore, by="Participant")
@ 

%% \begin{figure}
<<meanmedtcqiat, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE, eval=FALSE, results=hide>>=
## meanmedoptiatplot <- ggplot(optiatscore, aes(x=OptIAT.Mean, y=OptIAT.Median))+geom_point()+geom_smooth(method="lm")
## meanmedtcqpl <- ggplot(tcqiatscore, aes(x=TCQIAT.Median, y=TCQIAT.Mean))+geom_point()+geom_smooth(method="lm")
## print(arrangeGrob(meanmedoptiatplot,meanmedtcqpl))
@   
%%   \caption{Scatterplot of TCQIAT Median Scores against TCQ IAT mean scores (bottom) and Optimism IAT Mean vs Median (top) with a linear regression smooth line}
%%   \label{fig:meanmediat}
%% \end{figure}






Next, we examine the difference between the mean and median scores for the TCQ IAT. %% As can be seen from Figure \ref{fig:meanmediat},
There was little difference between the two measures of central tendency ($r=0.89$). 


The next question is whether or not the IATs have been contaminated by method variance. This can be assessed in a preliminary fashion by examining the correlations between the Treatment Credibility and Optimism IAT. The correlation between the two mean scored IAT measures was ($r=0.003$), while the correlation between the two median scored IAT measures was \Sepxr{round(with(Iatandexpmeasures2, cor(TCQIAT.Median, OptIAT.Median)), 3)}, thus showing that method variance does not appear to have contaminated the results. 




%% \begin{figure}
<<tcqstimblock3plot, echo=FALSE, eps=TRUE, pdf=TRUE, png=TRUE>>=
tcqstimblock3.real <- tcqstimblock3[,2:9]
tcqstimreal3.m <- melt(tcqstimblock3.real)
tcqstimblock5.real <- tcqstimblock5[,2:9]
tcqstimreal5.m <- melt(tcqstimblock5.real)
tcqstimreal3.m[,"variable"] <- with(tcqstimreal3.m, gsub("Block3\\.","",x=variable))
tcqstimreal5.m[,"variable"] <- with(tcqstimreal5.m, gsub("Block5\\.","",x=variable))
names(tcqstimreal3.m)[1] <- "Block3"
names(tcqstimreal5.m)[1] <- "Block5"

stimreal3.pl <- ggplot(tcqstimreal3.m, aes(x=Block3, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
stimreal5.pl <- ggplot(tcqstimreal5.m, aes(x=Block5, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
print(arrangeGrob(stimreal3.pl, stimreal5.pl))
@   
%%   \caption{Real versus Fake Stimuli, TCQ IAT. Boxplots of response times to each stimuli, Block 3 (top) and Block 5 (bottom)}
%%   \label{fig:tcqstimrealpl}
%% \end{figure}

An analysis showed that the majority of items were responded to relatively quickly in both categories. Of note, however, are the outliers which were words which were associated with fake treatments in Block 3 (where they were paired with conventional treatments) and words which were associated with real treatments (where they were paired with alternative treatments). This would seem to suggest that the words were in fact serving their intended purpose. 


%% As can be seen from Figure \ref{fig:tcqstimblock3} above, the distribution of participant response times was clearly not normal, being far too skewed to the right and heavy tailed. The log transformation helps matters, but the tails are still extremely long (a point further reinforced by the outliers seen in Figure \ref{fig:tcqstimrealpl} also). 
%% \begin{figure}
<<tcqstimconvaltpl, echo=FALSE, eps=TRUE, png=TRUE, pdf=TRUE>>=
tcqstim3convalt <- tcqstimblock3[,10:17]
tcqstim5convalt <- tcqstimblock5[,10:17]
tcqstim3conv.m <- melt(tcqstim3convalt)
tcqstim5conv.m <- melt(tcqstim5convalt)
tcqstim3conv.m[,"variable"] <- with(tcqstim3conv.m, gsub("Block3\\.", "", x=variable))
tcqstim5conv.m[,"variable"] <- with(tcqstim5conv.m, gsub("Block5\\.", "", x=variable))
names(tcqstim3conv.m)[1] <- "Block3"
names(tcqstim5conv.m)[1] <- "Block5"
tcqstimconv3pl <- ggplot(tcqstim3conv.m, aes(x=Block3, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
tcqstimconv5pl <- ggplot(tcqstim5conv.m, aes(x=Block5, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
print(arrangeGrob(tcqstimconv3pl, tcqstimconv5pl))
@   
%%   \caption{Boxplots for Conventional and Alternative Stimuli, TCQ IAT, Block 3 (top) Block 5 (bottom)}
%%   \label{fig:stimblockconvalt}
%% \end{figure}


A similar pattern emerged from the conventional and alternative stimuli. Interestingly, it appears that response times were slower overall in Block 5, which may represent fatigue. However, the order of IAT's was counterbalanced, so one would expect to see the same pattern in the Optimism IAT's if this was the case. 

%% \begin{figure}
<<optstimblock3, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE, eval=FALSE>>=
optstimblock3.m <- melt(optstimblock3, id.vars="part.opt")
optstimblock3.m[,"part.opt"] <- with(optstimblock3.m, as.factor(part.opt))
names (optstimblock3.m) [3] <- "ResponseTime"
partstim3opt <- ggplot(optstimblock3.m, aes(x=log(ResponseTime), colour=as.factor(part.opt)))+geom_density()+theme(legend.position="none")
optstimblock5.m <- melt(optstimblock5, id.vars="part.opt")
optstimblock5.m[,"part.opt"] <- with(optstimblock5.m, as.factor(part.opt))
names (optstimblock5.m) [3] <- "ResponseTime"
partstim5opt <- ggplot(optstimblock5.m, aes(x=log(ResponseTime), colour=as.factor(part.opt)))+geom_density()+theme(legend.position="none")
print (arrangeGrob (partstim3opt, partstim5opt))
@   
%%   \caption{Density Plots for Response Times of Each Participant, Block 3 (top) and Block 5 (bottom) All response times are plotted on a log scale}
%%   \label{fig:optstimblock3part}
%% \end{figure}

%% As can be seen from Figure \ref{fig:optstimblock3part}, the distributions for the optimism IAT were relatively similar, except that they were overall shifted towards the right, indicating that response times were generally slower to each of these words. Note that one participant has almost the entirety of their distribution beyond the tails of most of the other participants, which given that this is a log-scale plot, indicates that something is very wrong here. Note additionally that the Block 5 results are much more variable than those from Block 3, which again may represent fatigue. 
\begin{figure}
<<optstimposneg, echo=FALSE,pdf=TRUE, eps=TRUE, png=TRUE>>=
optstimblock3.real <- optstimblock3[,2:8]
optstimreal3.m <- melt(optstimblock3.real)
optstimblock5.real <- optstimblock5[,2:8]
optstimreal5.m <- melt(optstimblock5.real)
optstimreal3.m[,"variable"] <- with(optstimreal3.m, gsub("Block3\\.","",x=variable))
optstimreal5.m[,"variable"] <- with(optstimreal5.m, gsub("Block5\\.","",x=variable))
names(optstimreal3.m)[1] <- "Block3"
names(optstimreal5.m)[1] <- "Block5"

stimreal3.pl <- ggplot(optstimreal3.m, aes(x=Block3, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
stimreal5.pl <- ggplot(optstimreal5.m, aes(x=Block5, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
print(arrangeGrob(stimreal3.pl, stimreal5.pl))
@   
%%   \caption{Boxplots for Self/Other words in Optimism IAT, Block 3 (top) and Block 5 (bottom).}
%%   \label{fig:optstimmeyou}
%% \end{figure}

The same pattern emerged for the optimism IAT in that the Block 5 scores were much more variable and overall participants responded slower to this block. 


%% \begin{figure}
<<optiatplotself, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
optstimblock3.posneg <- optstimblock3[,9:15]
optstimpos3.m <- melt(optstimblock3.posneg)
optstimblock5.posneg <- optstimblock5[,9:15]
optstimpos5.m <- melt(optstimblock5.posneg)
optstimpos3.m[,"variable"] <- with(optstimpos3.m, gsub("Block3\\.","",x=variable))
optstimpos5.m[,"variable"] <- with(optstimpos5.m, gsub("Block5\\.","",x=variable))
names(optstimpos3.m)[1] <- "Block3"
names(optstimpos5.m)[1] <- "Block5"

stimpos3.pl <- ggplot(optstimpos3.m, aes(x=Block3, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
stimpos5.pl <- ggplot(optstimpos5.m, aes(x=Block5, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
print(arrangeGrob(stimpos3.pl, stimpos5.pl))
@   
%%  \caption{Boxplots of Response Times to Positive and Negative Words, Optimism IAT Block 3 (top), Block 5 (bottom).}
%%  \label{fig:optiatplotpos}
%% \end{figure}



The pattern of Block 5 responses tending to be slower was repeated in the mean response latencies for the positive and negative words in the Optimism IAT.  Note that disimproving appears to be the word with the highest mean latency, which is not surprising given its relatively unfamiliarity (compared to the other words, at least). 
 
 Next, the correlation between the different block times is assessed.
 
 %% \begin{figure}
<<tcqmeanresp, echo=FALSE,pdf=TRUE, eps=TRUE, png=TRUE>>=
tcqiat.mean.resp <- ddply(tcqiatsorted, .(Block, Participant), summarise, Correlations=mean(BlockTime, na.rm=TRUE))
## tcqmeanresp.pl <- ggplot(tcqiat.mean.resp, aes(x=MeanResponseTime))+geom_histogram+facet_grid(.~Block)
## print(tcqmeanresp.pl)
tcqiat.mean.m <- melt(tcqiat.mean.resp, id.vars=c("Participant", "Block"))
tcq.iat.c <- dcast(tcqiat.mean.m, Participant+...~Block)
names(tcq.iat.c) <- c("Participant", "variable", "TCQBlock1", "TCQBlock2", "TCQBlock3", "TCQBlock4", "TCQBlock5")
tcq.block.corr.pl <- plotmatrix(tcq.iat.c[,3:length(tcq.iat.c)])
print(tcq.block.corr.pl)
@    
 %%   \caption{Correlations between Block Scores for Treatment Credibility IAT with linear regression smooth line}
 %%   \label{fig:tcqblockcorr}
 %% \end{figure}

%% As can be seen from Figure \ref{fig:tcqblockcorr}, the correlations are relatively low between most of the blocks, though somewhat higher between blocks 3 and 5. 
Table \ref{tab:tcqcormat} gives the exact Kendalls $\tau$ between each of the blocks. As can be seen the correlations hover between 0.3 and 0.4, which is in line with expectations prior to the experiment. 

<<cormatrixtcqiat, echo=FALSE, results=tex>>=
cormat <- corr.test(tcq.iat.c[,3:length(tcq.iat.c)], method="kendall")[["r"]]
print(xtable(cormat, label="tab:tcqcormat", caption="Correlations between the blocks of the treatment credibility IAT (Kendalls tau. All correlations are significant at the p<0.001 level"))
@ 
 

Next, the same process is repeated for the Optimism IAT. 
%% \begin{figure}
<<tcqmeanresp, echo=FALSE,pdf=TRUE, eps=TRUE, png=TRUE>>=
optiat.mean.resp <- ddply(optiatsorted, .(Block, Participant), summarise, Correlations=mean(BlockTime, na.rm=TRUE))
## optmeanresp.pl <- ggplot(optiat.mean.resp, aes(x=MeanResponseTime))+geom_histogram+facet_grid(.~Block)
## print(optmeanresp.pl)
optiat.mean.m <- melt(optiat.mean.resp, id.vars=c("Participant", "Block"))
opt.iat.c <- dcast(optiat.mean.m, Participant+...~Block)
opt.block.corr.pl <- plotmatrix(opt.iat.c[,3:length(opt.iat.c)])+geom_smooth(method="lm")
print(opt.block.corr.pl)
@    
 %%   \caption{Correlations between Block Scores for Optimism IAT with linear regression smooth line}
 %%   \label{fig:optblockcorr}
 %% \end{figure}

%% As shown in Figure \ref{fig:optblockcorr}, the correlations between blocks are moderate, though highest in blocks 3 and 5, as was seen for the Treatment Credibility IAT. 
Table \ref{tab:optcormat} shows that the correlations are a little higher than for the Treatment Credibility IAT, but still within an acceptable range. Its interesting to note that (with the exception of Block 5), the correlations are strongest between adjacent blocks, and drop off as the blocks move further apart, suggesting that there may be correlations between adjacent blocks (which would not be surprising given the nature of the IAT task). 

<<cormatrixtcqiat, echo=FALSE, results=tex>>=
opt.cormat <- corr.test(opt.iat.c[,3:length(tcq.iat.c)], method="kendall")[["r"]]
print(xtable(opt.cormat, label="tab:optcormat", caption="Correlations between the blocks of the Optimism IAT (Kendalls tau. All correlations are significant at the p<  0.001 level"))
@ 

The next question with regard to the IAT's is whether or not the non-critical blocks (that is, Blocks 1, 2 and 4) will be correlated. Given that these were administered in counterbalanced order and there was a small gap between them one would expect there to be much lower correlations between these blocks of the IAT's. These correlations (if present) should provide an index of general processing speed, and may be useful as predictor variables for some of the other measures. 

<<opttcqiatcorr, echo=FALSE, results=hide>>=
curnames <- names(opt.iat.c)
curnames.bl <- curnames[3:length(curnames)]
curnames.bl2 <- paste("Opt", curnames.bl, sep="")
curnames.d <- c(curnames[1:2], curnames.bl2)
names(opt.iat.c) <- curnames.d
curnames.tcq <- names(tcq.iat.c)
curnames.bl.tcq <- curnames[3:length(curnames)]
curnames.bl2.tcq <- paste("TCQ", curnames.bl.tcq, sep="")
curnames.d.tcq <- c(curnames.tcq[1:2], curnames.bl2.tcq)
names(tcq.iat.c) <- curnames.d.tcq
iat.block.merge <- merge(tcq.iat.c, opt.iat.c, by="Participant")
iat.block.merge2 <- iat.block.merge[, -1*c(2, 5,7,8,11,13)]
@ 

%% \begin{figure}
<<corriattcqopt, echo=FALSE,pdf=TRUE, eps=TRUE, png=TRUE, eval=FALSE>>=
corr.iat.tcq.opt.pl <- plotmatrix(iat.block.merge2[,2:length(iat.block.merge2)])+geom_smooth(method="lm")+geom_smooth(method="lm")
print(corr.iat.tcq.opt.pl)
@   
%%   \caption{Correlations between Non Critical Blocks of Optimism and Treatment Credibility IAT}
%%   \label{fig:corriattcqopt}
%% \end{figure}

%% There were correlations between the two IAT's. These correlations, while significant, were quite low ($r=\bar0.20$) which equates to about 4\% of the variance. Therefore the two implicit measures can be safely be regarded as not being contaminated by method variance . 

<<survivaldata, echo=FALSE, results=hide, eval=FALSE>>=
painratings <- vasscores2[,c(9:53)]
painratings2 <- vasscores2[,c(1,9:53)]
names(painratings2) <- gsub("X", "T", x=names(painratings2))
napainratings <- apply(painratings,1, function (x)  sum(is.na(x)))
napainratings2 <- data.frame(Participant=vasscores2$PPNo., napainratings)
napainratings2[,"length.surv"] <- with(napainratings2, 46-napainratings)
Iatandexpmeasures.surv <- merge(Iatandexpmeasures, napainratings2, by="Participant")
censor <- Iatandexpmeasures[,"Censored"]
censor2 <- ifelse(censor==c("No", "Left"), 1, 0)
## Surv.data <- Surv(lengthsurv,censor2)
@ 

<<selfreportcleanup, echo=FALSE, results=hide>>=
names(vasscores2)[1] <-  "Participant"
names(expmeasures2)[1] <- "Participant"
vasscores2.test <- vasscores2[,1:8]
expmeasures2comp <- merge(vasscores2.test, expmeasures2)
Iatandexpmeasures <- merge(expmeasures2comp, IATscores, by="Participant")
iatexp <- Iatandexpmeasures[,c(14:22, 24)]
Iatandexpmeasures[,"meanconv"] <- with(Iatandexpmeasures, (Pill+Cream+Inj)/3)
Iatandexpmeasures[,"meanalt"] <- with(Iatandexpmeasures, (Acu+Hom+Rei)/3)
Iatandexpmeasures[,"convaltcomp"] <- with(Iatandexpmeasures, meanconv -meanalt)
## Iatandexpmeasures[,"Date"] <- with(Iatandexpmeasures, dmy(Date))
@
 
%% Next, the relationship between overall response time in each block (total time to complete the block, including interstimulus intervals) was examined in terms of the demographic variables. 

<<optiatdemo, echo=FALSE, results=hide, eval=FALSE>>=
optiatdemographics <- merge(expmeasures2, optiatsorted, by="Participant")
opt.demo <- xtable(summary(lm(BlockTime~Block+Age+Gender, data=optiatdemographics)), label="tab:optblocktimedemo", caption="Summary of Linear Regression of Block Time by Block, Age and Gender")
print(opt.demo)
@  
%% As can be seen from Table \ref{tab:optblocktimedemo}, the major influence on Block Time comes from Block, which is as expected given that Blocks 3 and 5 had three times as many trials as the other blocks. However, there is also an effect of gender, with males tending to respond somewhat quicker than females. This is interesting, as there are typically no gender based effects on IAT's (except for those which measure gender attitudes). Additionally, there was a significant effect of Age on the results, but this was in the typical direction, with greater Age being associated with greater block time. 

<<tcqiatdemo, echo=FALSE, results=hide, eval=FALSE>>=
tcqiatdemographics <- merge(expmeasures2, tcqiatsorted, by="Participant")
tcq.demo <- xtable(summary(lm(BlockTime~Block+Age+Gender, data=optiatdemographics)), caption="Summary of Linear Regression on Age, Gender and Individual Block Times.", label="tab:tcqblocktimedemo" )
print(tcq.demo)
@ 

\begin{figure}
<<tcqiatgender, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE, eval=FALSE>>=
tcqiatgender <- ggplot(na.omit(Iatandexpmeasures), aes(x=Gender, y=TCQIAT.Mean))+geom_boxplot()
optiatgender <- ggplot(na.omit(Iatandexpmeasures), aes(x=Gender, y=OptIAT.Mean))+geom_boxplot()
print(arrangeGrob(optiatgender,tcqiatgender))
@   
  \caption{Optimism (top) and Treatment Credibility (bottom) IAT Scores by Gender}
  \label{fig:tcqiatgend}
\end{figure}



As can be seen from Figure \ref{fig:tcqiatgend} above, there were no significant differences ($t=-0.4973, p=0.6211$) between men and women in  the sample with regards to their scores on the Treatment Credibility Questionnaire. However, the variance was much higher for men, which was a pattern replicated in previous research into Treatment Credibility (using a self report instrument described in Chapter \ref{cha:tcq-thesis}).
The same pattern was repeated for thje Optimism IAT, in that there were no significant differences ($t=-0.8234, df=49.761, p=0.4142$) between males and females in this sample. 

<<ordtestiatprep, echo=FALSE, results=hide>>=
part.opt <- unique(optiat[,"Participant"])
part.tcq <- unique(tcqiat[,"Participant"])
optblock3 <-  optstimblock3 
optblock5 <- optstimblock5
tcqblock3 <- tcqstimblock3
tcqblock5 <- tcqstimblock5
optiat.diff <- iatDiff(optblock3, optblock5)
tcqiat.diff <- iatDiff(tcqblock3, tcqblock5)

optdiffs.dich <- apply(optiat.diff, c(1,2), function (x) ifelse(x>0, 1, 0))
tcqdiffs.dich <- apply(tcqiat.diff, c(1,2), function (x) ifelse(x>0, 1, 0))
@ 
<<optiatirt, echo=FALSE, results=hide, cache=TRUE>>=
optiat.rasch <- RM(na.omit(optdiffs.dich))
optiat.ppar <- person.parameter(optiat.rasch)
optiat.elim <- stepwiseIt(optiat.rasch)
@ 

%% \begin{figure}
<<optpimap, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE, eval=FALSE, results=hide>>=
plotPImap(optiat.rasch)
@   
%%   \caption{Person Item Map of Difficulty parameters for Optimism IAT}
%%   \label{fig:optpimap}
%% \end{figure}


<<optiatrachprint, echo=FALSE, results=hide>>=
optrasch.df <- with(optiat.rasch, as.data.frame(cbind(etapar, se.eta, betapar, se.beta)))
print(xtable(optrasch.df, label="tab:optrasch", caption="Ability Estimates for Rasch Model Analysis of Optimism IAT"))
@ 

%% Table \ref{tab:optrasch} shows the estimated parameters for a Rasch model of the optimism IAT stimuli. Figure \ref{fig:optpimap} shows the Person Item Map for this model. This map shows that most of the stimuli were equivalent in difficulty, which indicates that this IAT is suitable for measuring implicit optimism in the general population, however, more discriminating stimuli would be necessary if the instrument was to be used in a clinical sample. Additionally, the map shows that the majority of participants showed latent traits of less than zero, suggesting that implicit optimism is rarer than explicit optimism. 


%% A process of stepwise elimination was carried out to eliminate items which did not fit the model. In this case, the stimulus ``Myself'' was the only one which had significant model misfit. 

<<tcqiatirt, echo=FALSE, results=hide, cache=TRUE>>=
tcqiat.rasch <- RM(tcqdiffs.dich)

tcqiat.elim <- stepwiseIt(tcqiat.rasch)
tcqiat.ppar <- person.parameter(tcqiat.rasch)
@ 

<<tcqiatability, echo=FALSE, results=hide>>=
tcqrasch.df <- with(tcqiat.rasch, as.data.frame(cbind(etapar, se.eta, betapar, se.beta)))
print(xtable(tcqrasch.df, label="tab:tcqiatrasch", caption="Ability Estimates for Rasch Model of Treatment Credibility IAT"))
@ 

<<tcqiatpiplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE, eval=FALSE>>=
sink("tmp.txt")
plotPImap(tcqiat.rasch)
sink(NULL)
## print(tcqiat.piplot)
@   


%% Table \ref{tab:tcqiatrasch}  shows the estimated abilities and their associated standard errors for the Treatment Credibility IAT. %% Figure \ref{fig:tcqpimap} the estimates of person and item abilities can be seen in graphical form. 


\subsection{Implicit-Explicit Relationships}
\label{sec:impl-expl-relat}

Next, the relationships between the explicit and implicit measures were examined. 

<<corrtestimplexpl, echo=FALSE, results=tex>>=
corr.imp.exp <- corr.test(Iatandexpmeasures[,14:25], method="kendall")[["r"]]
print(xtable(corr.imp.exp, label="tab:corrimpexp", caption="Correlations between Implicit and Explicit Measures"), scalebox=0.6)
@ 

It can be seen from Table \ref{tab:corrimpexp} that the LOTR was only really correlated with the Acupuncture items and with the MAAS, the Conventional Treatment scales correlated within themselves, as did the Alternative treatment scales, while the two IAT measures showed no appreciable correlations with each other. The relationships between the IAT's and explicit measures were small, and surprisingly in the unpredicted direction (negative) %% \footnote{in contrast to the pilot study reported in Chapter \ref{cha:devel-impl-meas}}
. Another surprise was that the direction of the correlation between the LOT-R and the MAAS was opposite to that observed in prior research. Possible reasons for these results are considered in the Discussion.  


\subsection{Explicit Measures}
\label{sec:explicit-measures}


%% \begin{figure}[ht]
<<lotrmaas, echo=FALSE,pdf=TRUE, eps=TRUE, png=TRUE>>=
lotrmaas <- ggplot(Iatandexpmeasures, aes(x=LOTR, y=MAAS))+geom_point()+geom_smooth(method="lm")+facet_grid(.~Condition)
print(lotrmaas)
@   
%%   \caption{Scatterplot of LOT-R Scores by Condition, with a linear regression smooth}
%%   \label{fig:lotrmaas}
%% \end{figure}


Optimism and mindfulness were positively correlated with one another. Additionally this correlation appeared to be relatively stable across condition, though it appeared a little weaker in the Deceptive Placebo Group.  This is in contrast to the results found in a much larger scale study carried out earlier in the research. Note that one plausible explanation for this effect is that, in the experiment, the measures were administered in the opposite order - Optimism, followed by Mindfulness. It is possible that the completion of the mindfulness measure affected the way in which participants approached the Optimism measure. This theory is more fully discussed in Chapter \ref{cha:general-discussion}. 


\subsection{Relationships between Experimental Samples and Survey Samples}
\label{sec:relat-betw-exper}

Given the focus of this thesis on the integration of survey and experimental research, the next step was to examine the differences and similarities between the samples collected from the general population via survey and the experimental sample.


\begin{figure}
<<surveyoptimism, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
surv.exp.opt.pl <- ggplot()+geom_density(data=hom.data, aes(x=optimism), fill="red")+geom_density(data=expmeasures2, aes(x=LOTR), fill="blue")
print(surv.exp.opt.pl)
@   
  \caption{Density Plot of Optimism Scores in the Survey samples (red), and the experimental sample (blue) }
  \label{fig:compoptimism}
\end{figure}


In Figure \ref{fig:compoptimism}  can be seen that the two distributions are extremely different, with a much higher average optimism score in the experimental sample. To some extent, this is not unexpected given that the study was described as an investigation of painkilling drugs and there was an opportunity to win a smart-phone, so perhaps students with higher levels of optimism were more likely to agree to participate. 



Next, the differences in mindfulness levels between the two samples were assessed. 

\begin{figure} 
<<surveymindfulness, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
surv.exp.mind.pl <- ggplot()+geom_density(data=hom.data, aes(x=mindfulness), fill="blue")+geom_density(data=Iatandexpmeasures, aes(x=MAAS), fill="red")
print(surv.exp.mind.pl)
@  
  \caption{Density Plot for Mindfulness Scores, Survey Sample (red), and Experimental Sample (blue)}
  \label{fig:compmind}
\end{figure}

As can be seen from Figure \ref{fig:compmind}, the pattern was quite different for mindfulness levels (as measured by the MAAS) as the levels of mindfulness were less long-tailed and more concentrated around a central peak in the survey sample. Again, this may be due to the association of mindfulness with introversion, as introverts may have been less likely to respond to the email invitiation(s) to take part in the study, while an online study might not have suffered from the same problem. 


Finally, the treatment credibility questionnaire scores were examined to assess the differences between the survey and experimental samples. 

First, the differences between the two samples in terms of Pill credibility were examined. 

\begin{figure}
<<surveypill, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
exp.pill.pl <- ggplot(Iatandexpmeasures, aes(x=Pill))+geom_density()
surv.exp.pill.pl <- ggplot()+geom_density(data=credtotals, aes(x=Pilltot), fill="red")+geom_density(data=Iatandexpmeasures, aes(x=Pill), fill="blue")
print(surv.exp.pill.pl)
@   
  \caption{Pill Credibility Density Plot, Survey Sample (sample 2) (red), and Experimental Sample (blue)}
  \label{fig:comppill}
\end{figure}


As can be seen from Figure \ref{fig:comppill}, the general population sample was higher peaked, with less variation around the peak than was the experimental sample. In fact, the experimental sample seemed to be more variable than the survey sample, which could either be due to a true difference in the distributions or due to a greater uncertainty in the experimental sample due to the smaller sample size. 

Next, the difference between Cream Credibility scores was assessed. 

\begin{figure}
<<surveycream, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=

surv.exp.cream.pl <- ggplot()+geom_density(data=credtotals, aes(x=Creamtot), fill="red")+geom_density(data=Iatandexpmeasures, aes(x=Cream), fill="blue")
print(surv.exp.cream.pl)
@   
  \caption{Density Plot for Distribution of Cream Credibility Scores, Survey Sample (sample 2) (top) and Experimental Cream Credibility Totals (bottom)}
  \label{fig:compcream}
\end{figure}



As shown in Figure \ref{fig:compcream}, the survey group tended to have a more positive view of painkilling creams. While the survey group is strongly peaked at the right of the plot, the experimental group were more evenly distributed, with a peak at the centre of the plot. This is interesting, as one might expect the experimental group to be more positive towards painkilling treatments in general, given that they had agreed to take part in  a study which examined the effects of a new analgesic.

Next, the credibility scores for injection painkilling treatments were examined between the survey and experimental groups. 

\begin{figure}
<<surveyinj, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
surv.exp.inj.pl <- ggplot()+geom_density(data=credtotals, aes(x=Injtot), fill="blue")+geom_density(data=Iatandexpmeasures, aes(x=Inj), fill="red")
print(surv.exp.inj.pl)
@   
  \caption{Injection Credibility Density Plot, Survey Sample (sample two) (top), Experimental Sample (bottom)}
  \label{fig:compinj}
\end{figure}


Figure \ref{fig:compinj}  show that the Injection credibility scores were almost identical in their distributions between the two samples. 

Next, the Alternative treatment scores were examined between the two samples. Acupuncture levels in the experimental sample were a little lower than those in the survey sample. In contrast ,the credibility scores for Homeopathy were slightly higher in the experimental sample than in the survey sample.  While Reiki credibility totals were quite low in both samples, they were a little lower in the survey sample. 

%% Figure \ref{fig:comphom} shows the levels of homeopathy credibility in both survey and experimental samples. It can be seen that
%% Figure \ref{fig:comprei} shows the credibility totals for Reiki in the survey and experimental samples.






%% \begin{figure}
  
<<alttreatmentssurvey, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE, eval=FALSE, results=hide>>=
surv.acu.pl <- ggplot(credtotals, aes(x=Acutot))+geom_density()
exp.acu.pl <- ggplot(Iatandexpmeasures, aes(x=Acu))+geom_density()
print(arrangeGrob ( surv.acu.pl, exp.acu.pl))
@   
%% \caption{Density Plots for Acupuncture Scores, Survey sample (top) and Experimental Sample (bottom)}
%%   \label{fig:compacu}
%% \end{figure}



%% \begin{figure}
  
<<alttreatmentssurvey, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE, eval=FALSE, results=hide>>=
surv.hom.pl <- ggplot(credtotals, aes(x=Homtot))+geom_density()
exp.hom.pl <- ggplot(Iatandexpmeasures, aes(x=Hom))+geom_density()
print(arrangeGrob ( surv.hom.pl, exp.hom.pl))
@  
%% \caption{Density Plots for Hompuncture Scores, Survey sample (top) and Experimental Sample (bottom)}
%%   \label{fig:comphom}
%% \end{figure}



%% \begin{figure}
  
<<alttreatmentssurvey, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE, eval=FALSE, results=hide>>=
surv.rei.pl <- ggplot(credtotals, aes(x=Reitot))+geom_density()
exp.rei.pl <- ggplot(Iatandexpmeasures, aes(x=Rei))+geom_density()
print(arrangeGrob ( surv.rei.pl, exp.rei.pl))
@   
%% \caption{Density Plots for Reipuncture Scores, Survey sample (top) and Experimental Sample (bottom)}
%%   \label{fig:comprei}
%% \end{figure}


%% \section{Using Psychometric Models from Survey Samples on the Experimental Participants}
%% \label{sec:using-psych-models}


%% \subsection{Treatment Credibility Questionnaire}
%% \label{sec:treatm-cred-quest}

<<grm1pltcqsmall, echo=FALSE, results=hide>>=
load("tcqexp110813.rda")
load("healthdata170813.rda")
exp.tcq <- expfull[,with(expfull, grep("^Pi|^Cr|^I|^Ac|^Ho|^R", x=names(expfull)))]
exp.tcq.conv <- exp.tcq[,with(exp.tcq, grep("Pill|Cream|Inj", x=names(exp.tcq)))]
exp.tcq.alt <- exp.tcq[,with(exp.tcq, grep("Acu|Hom|Rei", x=names(exp.tcq)))]
tcq2a.scale <- tcq2a[,17:52]
tcq2a.s <- tcq2a.scale[,with(tcq2a.scale, grep("[246]$", x=names(tcq2a.scale)))]
tcq2a.s.conv <- tcq2a.s[,with(tcq2a.s, grep("Pill|Cream|Inj", x=names(tcq2a.s)))]
tcq2a.s.alt <- tcq2a.s[,with(tcq2a.s, grep("Acu|Hom|Rei", x=names(tcq2a.s)))]
tcq2a.grm1pl.conv.s <- grm(tcq2a.s.conv, constrained=TRUE)
tcq2a.grm1pl.alt.s <- grm(tcq2a.s.alt, constrained=TRUE)
fscore.exp.grm1pl.tcq.conv <- factor.scores(tcq2a.grm1pl.conv.s, resp.patterns=exp.tcq.conv)
fscore.exp.grm1pl.tcq.alt <- factor.scores(tcq2a.grm1pl.alt.s, resp.patterns=exp.tcq.alt)
fscores.part <- data.frame(Participant=expfull$Participant, AbilityConv=fscore.exp.grm1pl.tcq.conv$score.dat[["z1"]],AbilityAlt=fscore.exp.grm1pl.tcq.alt$score.dat[["z1"]])
Iatandexpmeasures.grmtcq <- merge(Iatandexpmeasures, fscores.part, by="Participant")
@ 

<<grm1pllotr, echo=FALSE, results=hide>>=
lotr.exp <- expfull[,with(expfull, grep("LOTR.*[34790]$", x=names(expfull)))]
lotr.fscores <- factor.scores(lotr.grm.1pl, resp.patterns=lotr.exp)
fscores.part.lotr <- data.frame(Participant=expfull[["Participant"]], AbilityLOTR=lotr.fscores[["score.dat"]][["z1"]])
Iatandexpmeasures.abest <- merge(Iatandexpmeasures.grmtcq, fscores.part.lotr, by="Participant")
@ 


\subsection{Randomisation Checks}
\label{sec:randomisation-checks}

Next, the comparability of the groups were assessed to ensure that the randomisation process had proved effective. 

<<randcheck1, echo=FALSE, results=tex>>=
tcq.xtab <- xtable(summary(aov(TCQIAT.Mean~Condition, data=Iatandexpmeasures)), label="tab:tcqiatcheck", caption="Summary of Anova of Treatment Credibility Scores by Condition")
opt.xtab <- xtable(summary(aov(OptIAT.Mean~Condition, data=Iatandexpmeasures)), label="tab:optiatcheck", caption="Summary of ANOVA of Optimism IAT scores by Condition")
conv.xtab <- xtable(summary(aov(meanconv~Condition, data=Iatandexpmeasures)))
alt.xtab <- xtable(summary(aov(meanalt~Condition, data=Iatandexpmeasures)))
chisq.gend <- with(Iatandexpmeasures, chisq.test(table(Gender, Condition)))
chisq.prime <- with(Iatandexpmeasures, chisq.test(table(Prime, Condition)))
@ 

<<tcqcheckprint, echo=FALSE, results=tex>>=
print(tcq.xtab)
@ 

<<optcheckprint, echo=FALSE, results=tex>>=
print(opt.xtab)
@ 

As can be seen from Table \ref{tab:tcqiatcheck} and Table \ref{tab:optiatcheck} the scores on the treatment credibility IAT or the optimism IAT did not differ by Condition. 

The results of a chi-square test showed that the gender of participants in each condition were equivalent, (p=\Sexpr{round ( chisq.gend[["p.value"]], 3)}).
In addition, the priming manipulation was not significantly different across groups.



<<chisqcondition, echo=FALSE, results=hide>>=
Subsetiatandexp <- Iatandexpmeasures[with(Iatandexpmeasures,Condition!="No Treatment"),]
Subsetiatandexp <- droplevels(Subsetiatandexp)
chisq2 <- with(Subsetiatandexp, chisq.test(table(PlacResp, Condition)))
chisqtest2 <- as.data.frame(cbind(chisq2[["statistic"]], chisq2[["parameter"]], chisq2[["p.value"]], chisq2[["observed"]]))
names(chisqtest2)[1] <- "Chi Square"
names(chisqtest2)[2] <- "df"
names(chisqtest2)[3] <- "p Value"
chisq2.xtab <- xtable(chisqtest2)
print(chisq2.xtab)
@ 

As can be seen, there is no significant effect of condition on placebo response, which would suggest that this experiment was not successful at inducing a response reliably. This is damaging, but not devestating to the main point of the research.  

<<chisqtestprimeplac, echo=FALSE, results=hide>>=
chisq1 <- with(Iatandexpmeasures, chisq.test(table(Prime, Condition, PlacResp)))
chisqtest1 <- as.data.frame(cbind(chisq1[["statistic"]], chisq1[["parameter"]], chisq1[["p.value"]], chisq1[["observed"]]))
names(chisqtest1)[1] <- "Chi Square"
names(chisqtest1)[2] <- "df"
names(chisqtest1)[3] <- "p Value"
chisq1.xtab <- xtable(chisqtest1, label="tab:primeplacchi", caption="Chi Square for Relationship between Priming, Condition and Placebo Response")
print(chisq1.xtab)
@ 

The results of the Chi-Square test( $\chi^2(11)=64.0, p\le 0.0001$) show that priming appears to be the driver of this effect, as the Placebo Response by Condition is extremely significant given the priming manipulation.

This finding is much more clearly conveyed in Figure \ref{fig:placprimeplot}, where it can be seen that participants were much more likely to respond to placebo following a priming intervention. Given that priming interventions typically take place outside conscious awareness, this suggests that there is at least some part of the placebo response which is amenable to non-conscious (or implicit) influences. 

\begin{figure}
<<placprimeplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
primedata <- Iatandexpmeasures[with(Iatandexpmeasures, grep("^Placebo|^Treatment", x=Condition)),]
plac.prime.pl <- ggplot(primedata, aes(x=PlacResp, fill=Prime))+geom_histogram()
print(plac.prime.pl)
@   
  \caption{Proportion of Placebo Response in Primed and Non-Primed Conditions}
  \label{fig:placprimeplot}
\end{figure}

\begin{figure}
<<pairsplot, echo=FALSE, fig=TRUE>>=
iatexp2 <- iatexp
names(iatexp2)[9:10] <- c("TCQIAT", "OptIAT")
print(plotmatrix(iatexp2)+geom_smooth(method="lm")+geom_jitter())
@   
  \caption{Scatterplots and Density Plots for Self Report and Implicit Measures}
  \label{fig:pairsplotimpexp}
\end{figure}

In Figure \ref{fig:pairsplotimpexp} the relationships between the self report and implicit measures are shown. It can be seen that the credibility scores break into conventional and alternative groups, while the LOT-R and MAAS do not correlate hugely with any of the other measures (but do correlate quite well with themselves, as noted above).  


\subsection{Pain Ratings}
\label{sec:pain-ratings}






<<paints, echo=FALSE, results=hide>>=
painratings <- vasscores2[,c(1,2,8:53)]
painratings.temp <- painratings[,2:length(painratings)]
painratings.trans <- t(painratings.temp)
colnames(painratings.trans) <- as.character(t(painratings[,1]))
painratings.trans <- as.data.frame(painratings.trans)
painratings.trans[,"Time"] <- 1:nrow(painratings.trans)
pain.cond <- ddply(painratings, .(Condition), summarise, PainRatings=apply(painratings[,4:48], 2, mean, na.rm=TRUE))
pain.cond.m <- melt(pain.cond, id.vars="Condition")
pain.cond.m2 <- pain.cond.m[,c("Condition", "value")]
## pain.cond.rs <- dcast(pain.cond.m2, ...~Condition)
@ 




\subsection{Analysis of Pain Ratings}

<<painmetadata, echo=FALSE, results=hide>>=
pain.metadata <- read.csv("PainScoresMetaData.csv")
pain.metadata[,"PPNo."] <- with(pain.metadata, sprintf("%04d", PPNo.))
pain.metadata[3:6] <- apply(pain.metadata[,3:6], c(1,2), function(x) x/1000)
pain.pad <- pain.metadata[,c("PPNo.", "SqueezStop")]
pain.pad[,"FirstPainRating"] <- with(pain.pad, SqueezStop+60)
pain.pad <- pain.pad[with(pain.pad, !duplicated(PPNo.)),]
pain.ip <- interpolate.pain(vasscores2, padding=pain.pad)
@ 

<<meanpaingroup, echo=FALSE, results=hide>>=
pain.metadata.uniq <- pain.metadata[with(pain.metadata, !duplicated(PPNo.)),]

painratings[,"Participant"] <- with(painratings, sprintf("%04d", as.numeric(Participant)))
pain.ip2 <- interpolate2(painratings, pain.metadata.uniq)
pain.ip2 <- as.data.frame(pain.ip2)
pain.ip2[,"Participant"] <- with(pain.metadata.uniq, PPNo.)
pain.ip.cond <- merge(pain.ip2, painratings[,c(1:3)], by="Participant")
deceptivepain <- pain.ip.cond[with(pain.ip.cond,Condition=="Treatment"),]
placebopain <- pain.ip.cond[with(pain.ip.cond,Condition=="Placebo"),]
notreatpain <- pain.ip.cond[with(pain.ip.cond,Condition=="No Treatment"),]
meandeceptivepain <- apply(deceptivepain[,4:48], 2, mean, na.rm=TRUE)
meanplacebopain <- apply(placebopain[,4:48], 2, mean, na.rm=TRUE)
meannotreatpain <- apply(notreatpain[,4:48], 2, mean, na.rm=TRUE)
meangrouppainratings <- cbind(meandeceptivepain, meanplacebopain, meannotreatpain)
meangrouppainratings <- as.data.frame(meangrouppainratings)
meangrouppainratings[,"Time"] <- 1:45
meddeceptivepain <- apply(deceptivepain[,4:48], 2, median, na.rm=TRUE)
medplacebopain <- apply(placebopain[,4:48], 2, median, na.rm=TRUE)
mednotreatpain <- apply(notreatpain[,4:48], 2, median, na.rm=TRUE)
medpainratings <- as.data.frame(cbind(meddeceptivepain, medplacebopain, mednotreatpain))
medpainratings[,"Time"] <- 1:45
dec.pain.mean <- apply(deceptivepain[,4:48], 2, mean, na.rm=TRUE)
open.pain.mean <- apply(placebopain[,4:48], 2, mean, na.rm=TRUE)
notreat.pain.mean <- apply(notreatpain[,4:48], 2, mean, na.rm=TRUE)

painbycond <- data.frame(Deceptive=dec.pain.mean, Open=open.pain.mean, NoTreat=notreat.pain.mean)
painbycond[,"Time"] <- 1:45
painbycond.m <- melt(painbycond, id.vars="Time")
painbycond2 <- painbycond[,1:3]
painbycond2 <- zoo(painbycond, order.by=as.integer(seq(1, length.out=nrow(painbycond), by=60)))
@ 

<<arimapaincond, echo=FALSE, results=hide>>=
arima.decept <- with(painbycond, auto.arima(Deceptive))
arima.open <- with(painbycond, auto.arima(Open))
arima.notreat <- with(painbycond, auto.arima(NoTreat))
ets.decept <- with(painbycond, ets(na.omit(Deceptive)))
ets.open <- with(painbycond, ets(na.omit(Open)))
ets.notreat <- with(painbycond, ets(na.omit(NoTreat)))
@ 

\begin{figure}
<<tsmeanpainplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
meanpain.melt <- melt(meangrouppainratings, id="Time")
tspainplot <- ggplot(meanpain.melt, aes(x=Time, y=value, label=variable, colour=variable))+geom_line()+geom_smooth(span=0.2)
print(tspainplot)
@   
  \caption{Plot of Pain Responses by Condition Over Time}
  \label{fig:tsmeanpainplot}
\end{figure}



As can be seen from Figure \ref{fig:tsmeanpainplot}, the Deceptive Placebo group appeared to report lower pain ratings than the Open Placebo group, but from a cursory investigation of the plot, did not appear to be significantly different to the No Treatment group.  The curves shown in the figure used a locally weighted smoother (loess, span=0.2) to create the lines, given the substantial non linearity of the results. However, this plot does show that there was a significant difference between the placebo group and the two other conditions. 

\begin{figure}
<<medtspainplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
medpain.melt <- melt(medpainratings, id="Time")
medtspainplot <- ggplot(medpain.melt, aes(x=Time, y=value, group=variable, colour=variable))+geom_smooth(span=0.5)
print(medtspainplot)
@   
  \caption{Median Pain Ratings Over Time by Condition}
  \label{fig:medpainplot}
\end{figure}


As can be seen from Figure \ref{fig:medpainplot}, the results of the median pain ratings by group show a somewhat different pattern, in that the drop in the pain ratings for the Deceptive Condition is much less apparent, suggesting that the majority of the decrease in pain ratings was driven by a small number of extreme ratings in this group.
Again, a locally weighted smoother (loess, span=0.5) was used to fit the curves. 


%% \begin{figure}
<<medianpaintscorrelationplots, echo=FALSE,pdf=TRUE, eps=TRUE, png=TRUE, results=hide>>=
dev.new()
par(mfrow=c(2,2))
plot(acf(na.omit(with(medpainratings,meddeceptivepain)), main="Deceptive Placebo Group"))
plot(acf(na.omit(with(medpainratings,medplacebopain)), main="Open Placebo Group"))
plot(acf(na.omit(with(medpainratings,mednotreatpain)), main="No Treatment Group"))
@   
%%   \caption{Autocorrelation Plots for Median Pain Responses over Time by Condition}
%%   \label{fig:autocorrpainplot}
%% \end{figure}


The autocorrelation plots for the three groups appeared to be similar, which means that a the same ARIMA model can be fit to them. The first three differences are significant, and a process of ARIMA model fitting indicates that an ARIMA(1,3,1) model has the best AIC and likelihood. This is information that needs to be incorportated into an overall model which will be fit to the data.

%% \begin{figure}
<<meanpaintscorrelationplots, echo=FALSE,pdf=TRUE, eps=TRUE, png=TRUE, results=hide, eval=FALSE>>=
plot.new()
par (mfrow=c (2,2))
sink ("tmp.txt")
print(acf(na.omit(with(meangrouppainratings, meannotreatpain)), main="Deceptive Placebo Group"))
print(acf(na.omit(with(meangrouppainratings,  meandeceptivepain)), main=" Open Placebo Group"))
print(acf(na.omit(with(meangrouppainratings,meanplacebopain)), main="No Treatment Group"))
sink (NULL)
@   
%%   \caption{Mean Autocorrelation plots for pain ratings over time}
%%   \label{fig:meanautocorrplot}
%% \end{figure}


The mean pain ratings were also examined for autocorrelations and the results were exactly the same as for the median pain ratings, indicating an ARIMA(1,3,1) model was the best fit for the data.



\subsection{Analysis of Physiological Data}


\subsection{Examining the Effect of Pain on GSR}
\label{sec:exam-effect-pain}

The next step in the analysis was to examine the impact of the particular painful stimuli administered on skin conductance. Some preliminary research has suggested that this may be a useful predictor of painful stimuli, and this question will be covered in the following section. However, in this section, the impact of the painful stimuli on skin conductance will be assessed. Note that due to experimenter error, not all participants had the band to induce pain at 300s into the experiment. This time of administration was recorded, and additionally as participants squeezed either more slowly or more quickly, there was some variability in these times. 

The first step in this analysis was to examine the size of these disparities. 

<<bandagequeeze, echo=FALSE, results=hide>>=
bandage.squeeze <- Iatandexpmeasures [,c ("Participant", "BandageOn", "SqueezStop")]
bandage.squeeze [,"SqueezingTime"] <- with (bandage.squeeze, SqueezStop-BandageOn)
@ 
 

%% \begin{figure}
<<bandonoff, echo=FALSE, results=hide>>=
band.m <- melt (bandage.squeeze [,1:3], id.vars="Participant")
print (ggplot (band.m, aes (x=value, fill=variable))+geom_density (alpha=0.5))
@ 
%%   \caption{Density Plots for Time of Application of Bandage and Stopping Squeezing}
%%   \label{fig:onoffplot}
%% \end{figure}

%% Density Plots for the time of application of bandage and when the participants stopped squeezing are shown in Figure \ref{fig:onoffplot}. Note that the shape of the time of application is far more regular than that for the  stopping squeezing, suggesting that inter-participant variability was responsible for the majority of observed differences. 

%% \begin{figure}
<<squeezeplot, echo=FALSE, fig=TRUE>>=
squeeez.pl <- ggplot (bandage.squeeze, aes (x=SqueezingTime))+geom_density ()
print (squeeez.pl)
@ 
%%   \caption{Density Plot of Time Spent Squeezing the Hand Exerciser}
%%   \label{fig:squeezplot}
%% \end{figure}


The majority of participants spent approximately between 30 and 50 seconds squeezing the hand exercisor, which seems reasonable given that they were asked to squeeze it for 40 seconds. 



<<physlen, echo=FALSE, results=hide, eval=FALSE>>=
physlen <- read.table ("./ExperimentDataforR/FullStudy/PhysMeasures/Richieoutput/FullTimePhys2.txt", header=FALSE)
names (physlen) <- c ("Time", "Participant")
physlen [,"Participant"] <- with (physlen, as.factor (Participant))
survlength.ratings <- Iatandexpmeasures [,c ("Participant","BandageOn", "lengthsurv")]
len.test <- merge (physlen, survlength.ratings, by="Participant")
len.test [,"Time"] <- with (len.test, Time/1000)
len.test [,"Diff"] <- with (len.test, Time-lengthsurv)
@ 

Another sanity check was then performed on the datasets to ensure that all data had been recorded correctly. This was the examination of the lengths of the physiological recordings versus the length of the recordings of pain ratings. While the physiological data should be more accurate here (as it was recorded automatically and at a high resolution) any major differences would be cause for elimination of the affected records. 


<<physpaindiffplot, echo=FALSE, fig=TRUE, eval=FALSE>>=
physpaindiff.pl <- ggplot (len.test, aes (x=Diff))+geom_density ()
print (physpaindiff.pl)
@ 


Before the analysis of GSR data was conducted, the mean GSR per group were scaled to ensure that they were directly comparable. Scaling was performed using a z-score method, where each observations value was subtracted from the mean and divided by the standard deviation of all the observations. 



<<tsimport, echo=FALSE, results=hide>>=
gsr <- read.csv("GSRdataDownSample.csv")
gsr[,"index"] <- 1:nrow(gsr)
gsr.melt <- melt(gsr, id.vars="index")
gsr.melt[,"variable"] <- with(gsr.melt, gsub("X", "", x=variable))
gsr.mean <- ddply(gsr.melt, .(variable), summarise, gsr.mean=mean(value, na.rm=TRUE))
names(gsr.mean)[1] <- "Participant"
Iatandexpmeasures[,"Participant"] <- with(Iatandexpmeasures, sprintf("%04s", Participant))
Iatandexpmeasures.phys <- merge(Iatandexpmeasures2, gsr.mean, by="Participant")
## Iatandexpmeasures.phys <- Iatandexpmeasures.phys[,1:30]
## names(Iatandexpmeasures.phys)[30] <- "gsr.mean"
gsr.rs <- dcast(gsr.melt, variable~index+...)
@



<<gsrcond, echo=FALSE, results=hide>>=
gsrcond <- merge(gsr.rs, Iatandexpmeasures, by.x="variable", by.y="Participant")
gsrcond.scale.ind <- with(gsrcond, grep("^[0-9]+$", x=names(gsrcond)))
gsrcond[,gsrcond.scale.ind] <- scale(gsrcond[,gsrcond.scale.ind])
gsrcond2 <- gsrcond[,c(1:3202)]
## gsrcond.done <- with(gsrcond2, tapply(gsrcond[,2:3201], Condition, function (x) apply(x, 2, mean, na.rm=TRUE)))
gsr.decept <- gsrcond2[with(gsrcond2, Condition=="Treatment"),]
gsr.plac <- gsrcond2[with(gsrcond2, Condition=="Placebo"),]
gsr.nt <- gsrcond2[with(gsrcond2, Condition=="No Treatment"),]
mean.decept <- apply(gsr.decept[,2:3201], 2, mean, na.rm=TRUE)
decept.m <- melt(mean.decept)
decept.m[,"index"] <- 1:nrow(decept.m)
mean.plac <- apply(gsr.plac[,2:3201], 2, mean, na.rm=TRUE)
plac.m <- melt(mean.plac)
plac.m[,"index"] <- 1:nrow(plac.m)
mean.nt <- apply(gsr.nt[,2:3201], 2, mean, na.rm=TRUE)
nt.m <- melt(mean.nt)
nt.m[,"index"] <- 1:nrow(nt.m)
gsr.mean.cond <- Reduce(function(x, y) merge(x, y, by="index"), list(decept.m, plac.m, nt.m))
names(gsr.mean.cond)[2:4] <- c("Deceptive", "Placebo", "NoTreat")
cond.m <- melt(gsr.mean.cond, id.vars="index")
## gsrcond.m <- melt(gsrcond, id.vars=c("variable", names(gsrcond)[1:3200]))
gsr.mean.cond2 <- zoo(gsr.mean.cond, order.by=gsr.mean.cond$index)
gsr.pain.merge <- na.locf(merge(painbycond2, gsr.mean.cond2))
names(gsr.pain.merge) <- c("DeceptivePain", "OpenPain", "NoTreatPain", "Time", "index", "DeceptiveGSR", "OpenGSR", "NoTreatGSR")
gsr.pain.merge.gg <- fortify(gsr.pain.merge)
gsr.pain.merge.gg2 <- gsr.pain.merge.gg[,c(1:4, 7:9)]
gsr.pain.merge.gg2[,2:6] <- scale(gsr.pain.merge.gg2[,2:6])
gsr.decept.merge <- gsr.pain.merge.gg2[,c("Index", "DeceptivePain", "DeceptiveGSR")]
gsr.open.merge <- gsr.pain.merge.gg2[,c("Index", "OpenPain", "OpenGSR")]
gsr.decept.merge.m <- melt(gsr.decept.merge, id.vars="Index")
gsr.open.merge.m <- melt(gsr.open.merge, id.vars="Index")
gsr.notreat.merge <- gsr.pain.merge.gg2[,c("Index", "NoTreatPain", "NoTreatGSR")]
gsr.notreat.merge.m <- melt(gsr.notreat.merge, id.vars="Index")
gsr.decept.pl <- ggplot(gsr.decept.merge.m, aes(x=Index, y=value, color=variable))+geom_line()+xlim(c=0,1700)
gsr.open.pl <- ggplot(gsr.open.merge.m, aes(x=Index, y=value, colour=variable))+geom_line()
gsr.notreat.pl <- ggplot(gsr.notreat.merge.m, aes(x=Index, y=value, colour=variable))+geom_line()
@ 

<<gsrgend, echo=FALSE, results=hide>>=
gsr.male <- gsrcond[with(gsrcond, Gender=="male"),]
gsr.female <- gsrcond[with(gsrcond, Gender=="female"),]
gsr.male2 <- gsr.male[,with(gsr.male, grep("^[0-9]+$", x=names(gsr.male) ))]
gsr.male.mean <- apply(gsr.male2, 2, mean, na.rm=TRUE)
gsr.female2 <- gsr.female[,with(gsr.female, grep("^[0-9]+$", x=names(gsr.female) ))]
gsr.female.mean <- apply(gsr.female2, 2, mean, na.rm=TRUE)
gsr.gend <- data.frame(index=1:3200, Males=gsr.male.mean, Females=gsr.female.mean)
gsr.gend.m <- melt(gsr.gend, id.vars="index")
@ 
\begin{figure}
<<gsrgendplot, echo=FALSE, fig=TRUE>>=
print(ggplot(na.omit(gsr.gend.m), aes(x=index, y=value, colour=variable))+geom_line()+geom_smooth()+xlab("Time"))
@   
  \caption{Scaled GSR by Gender over time (in seconds)}
  \label{fig:gendgsrplot}
\end{figure}

Figure \ref{fig:gendgsrplot} shows that the pattern of GSR responses was quite different for males and females. Males showed somewhat decreasing GSR over time (but note that the number of participants in each group fell off as time progressed, so the pattern is less than certain), but an overall higher GSR throughout the entire experiment, while females showed the opposite pattern.  

<<gsrplacresp, echo=FALSE, results=hide>>=
gsr.placyes <- gsrcond[with(gsrcond, PlacResp=="Yes"),]
gsr.placno <- gsrcond[with(gsrcond, PlacResp=="No"),]
gsr.placyes.mean <- apply(gsr.placyes[,gsrcond.scale.ind], 2, mean, na.rm=TRUE)
gsr.placno.mean <- apply(gsr.placno[,gsrcond.scale.ind], 2, mean, na.rm=TRUE)
gsr.placresp <- data.frame(index=1:3200,PlaceboResp=gsr.placyes.mean, NoPlaceboResp=gsr.placno.mean)
gsr.placresp.m <- melt(gsr.placresp, id.vars="index")
@ 
\begin{figure}
<<gsrplacresplot, echo=FALSE, fig=TRUE, pdf=TRUE, png=TRUE, eps=TRUE>>=
print(ggplot(gsr.placresp.m, aes(x=index, y=value, colour=variable))+geom_line()+geom_smooth())
@   
  \caption{GSR over Time by Response to Placebo}
  \label{fig:gsrplacresplot}
\end{figure}

As can be seen from Figure \ref{fig:gsrplacresplot}, the GSR levels of those participants who responded to placebo increased over time, while the GSR levels of those who did not decreased over time. 

This seems intuitively odd, as one might have expected the placebo effect to have lowered skin conductance in the group which responded to placebo. 

<<gsrcond2, echo=FALSE, results=hide>>=
gsr.decept <- gsrcond[with(gsrcond, Condition=="Treatment"),]
gsr.open <- gsrcond[with(gsrcond, Condition=="Placebo"),]
gsr.notreat <- gsrcond[with(gsrcond, Condition=="No Treatment"),]
gsr.decept.mean <- apply(gsr.decept[,gsrcond.scale.ind], 2, mean, na.rm=TRUE)
gsr.open.mean <- apply(gsr.open[,gsrcond.scale.ind], 2, mean, na.rm=TRUE)
gsr.notreat.mean <- apply(gsr.notreat[,gsrcond.scale.ind], 2, mean, na.rm=TRUE)
gsr.conditions <- data.frame(index=1:3200, Deceptive=gsr.decept.mean, Open=gsr.open.mean, NoTreatment=gsr.notreat.mean)
gsr.conditions.m <- melt(gsr.conditions, id.vars="index")
@ 
\begin{figure}
<<gsrplotcond, echo=FALSE, results=hide>>=
print(ggplot(gsr.conditions.m, aes(x=index, y=value, colour=variable))+geom_line()+geom_smooth()+xlab("Time"))
@   
  \caption{GSR by Condition over Time}
  \label{fig:gsrcondplot}
\end{figure}

Figure \ref{fig:gsrcondplot} shows the GSR levels by condition over time. It can be seen that the Deceptive Placebo Group show a large increase in skin conductance across the study, while the Open Placebo Group show a parallel decrease in GSR levels. The No Treatment group show  a steady increase from low levels. It can be seen that even when the GSR scores are scaled and centred, the groups were not comparable at baseline, which is surprising, given that the groups were balanced on all other measures. 

\begin{figure}
<<meanplotgsr, echo=FALSE, eval=FALSE,pdf=TRUE, eps=TRUE, png=TRUE>>=
ggplot(cond.m, aes(x=index, y=value, colour=variable))+geom_line()
@   
  \caption{Mean GSR Levels by Condition over Time}
  \label{fig:meangsr}
\end{figure}


The results shown above in Figure \ref{fig:meangsr} are quite unexpected, but fit with the strange pain ratings observed in this experiment. It can be seen that the Open Placebo group had the lowest GSR over time (middle plot), and that the deceptive pain group seemed to have the highest GSR throughout the experiment. The No Treatment group chart is perhaps the strangest, showing a steady upward trend until about 25 minutes into the experiment, and then falling steadily from there to reach (at 30000, or 45 minutes after the pain was induced) approximately the level which it began. This may have occurred due to habituation to the experimental environment, as they had no experimental manipulation during the course of the study which might have changed their GSR. In contrast, the Open Placebo group show a low GSR starting off which then rises slowly, dips and then rises again. The course of GSR in the Deceptive Placebo group looks like what would have been expected from the No Treatment group; i.e. a slow and steady rise throughout the experiment.

<<semdata, echo=FALSE, results=hide>>=
Iatandexpmeasures2 <- Iatandexpmeasures.phys
names(Iatandexpmeasures2)[c(22,24,29)] <- c("OptIATMean", "TCQIATMean", "gsrmean")
exp.sem <- Iatandexpmeasures2[, c("Age", "Gender", "LOTR", "OptIATMean", "TCQIATMean", "PlacResp", "meanconv", "meanalt", "convaltcomp", "Prime","gsrmean" )]
@ 


@ 
%% \begin{figure}
<<gsrpainplt, echo=FALSE, eps=TRUE, png=TRUE, pdf=TRUE, jpg=TRUE>>=
print(arrangeGrob(gsr.decept.pl, gsr.open.pl, gsr.notreat.pl))
@ 
%% \caption{GSR versus Pain Ratings over Time by Condition}
%% \label{fig:gsrpainplot}
%% \end{figure}


There was a  sizeable difference by condition when the cross-correlations between GSR ratings and pain ratings were examined. 
For the Deceptive Placebo Condition, the mean correlation was 0.63, for the Deceptive Placebo condition it was -0.53, and for the No Treatment Condition it was -0.22. As can be seen the magnitudes for the two active conditions were similar, though the directions were reversed, while for the no treatment group, the correlations were much smaller, suggesting that there was less relationship between the physiological measures and the pain ratings in this condition. This subject is interesting, and is explored more below and in the Discussion. 

<<gsrmodel, echo=FALSE, results=tex>>=
Iatandexpmeasures.phys <- Iatandexpmeasures.phys[,1:30]
gsr.cond <- lm(gsrmean~Condition, data=Iatandexpmeasures.phys)
print(xtable(summary(gsr.cond), caption="Regression of Condition on Mean GSR", label="tab:gsrcondreg"))
@ 








\section{Variables impacting the placebo response}
\label{sec:vari-impact-plac}
In this section, the covariates associated with response to placebo in this sample are examined, first graphically and then through a process of formal model fitting. 




A regression which is shown in Table \ref{tab:gsrcondreg} showed that there was a significant effect of the Deceptive Placebo Condition on mean GSR scores, suggesting that the mean GSR scores tended to be higher in this condition as compared to the others. This would seem to match the descriptions of the cross-correlation functions above. 

<<ccfgsr, echo=FALSE, results=hide>>=
ccf.decept <- with(gsr.pain.merge, ccf(DeceptivePain, DeceptiveGSR, lag.max=600))
ccf.open <- with(gsr.pain.merge, ccf(OpenPain, OpenGSR, lag.max=600))
ccf.notreat <- with(gsr.pain.merge, ccf(NoTreatPain, NoTreatGSR, lag.max=600))
@ 

<<gsrts, echo=FALSE, results=hide>>=
gsr.mean.cond[,"decept60"] <- with(gsr.mean.cond, ts(Deceptive, deltat=1/60))
gsr.mean.cond["decept360"] <- with(gsr.mean.cond, ts(Deceptive, deltat=1/360))
gsr.mean.cond[,"open60"] <- with(gsr.mean.cond, ts(Placebo, deltat=1/60))
gsr.mean.cond[,"open360"] <- with(gsr.mean.cond, ts(Placebo, deltat=1/360))
gsr.mean.cond[,"notreat60"] <- with(gsr.mean.cond, ts(NoTreat, deltat=1/60))
gsr.mean.cond[,"notreat360"] <- with(gsr.mean.cond, ts(NoTreat, deltat=1/360))
@ 


%% \begin{figure}
<<ggplothistcond, echo=FALSE, eval=FALSE,pdf=TRUE, eps=TRUE, png=TRUE>>=
plac.resp.hist.pl <- ggplot(Iatandexpmeasures, aes(x=TCQIAT.Mean))+geom_density()+facet_grid(.~PlacResp)
print(plac.resp.hist.pl)
@   
%%   \caption{TCQIAT.Mean against Pain Responses Over Time}
%%   \label{fig:histresp}
%% \end{figure}


%% From Figure \ref{fig:histresp}, it can be seen that the participants who did respond to placebo had marginally higher Treatment Credibility IAT scores than those who did not. 






%% \begin{figure}
<<optiatcorrsurv, echo=FALSE, eval=FALSE,pdf=TRUE, eps=TRUE, png=TRUE>>=
optiat.histcond <- ggplot(Iatandexpmeasures, aes(x=OptIAT.Mean))+geom_density()+facet_grid(.~PlacResp)
print(optiat.histcond)
@   
%%   \caption{Density Plots of Optimism IAT Scores by Condition}
%%   \label{fig:opthistcond}
%% \end{figure}


%% Again, it can be seen from Figure \ref{fig:opthistcond} that those who responded to placebo had higher optimism IAT scores than  those who did not, suggesting that something about the IAT is predictive of placebo response. 

%% \begin{figure}
<<creamhistcond, echo=FALSE, eval=FALSE,pdf=TRUE, eps=TRUE, png=TRUE>>=
cream.hist.cond <- ggplot(Iatandexpmeasures, aes(x=Cream))+geom_density()+facet_grid(.~PlacResp)
print(cream.hist.cond)
@   
%%   \caption{Density Plot for Mean Cream Credibility Scores by Condition}
%%   \label{fig:creamhistcond}
%% \end{figure}


There was a difference in the mean cream credibiliuty scores by whether or not a participant responded to placebo, but the difference was not significant ($t=-0.9545, df=53.766, p=0.3441$). 

\begin{figure}
<<ggplotplacyesno, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
placrespyes <- painratings[with(painratings,PlacResp=="Yes"),]
placrespno <- painratings[with(painratings,PlacResp=="No"),]
placresyesmean <- apply(placrespyes[4:48], 2, mean, na.rm=TRUE)
placresnomean <- apply(placrespno[4:48], 2, mean, na.rm=TRUE)
placresyesnopain <- as.data.frame(cbind(placresyesmean, placresnomean))
placresyesnopain$Time <- 1:45
placres.melt <- melt(placresyesnopain, id="Time")
placresplot <- ggplot(placres.melt, aes(x=Time, y=value, group=variable, colour=variable))+geom_line() +geom_smooth(method="loess")
print(placresplot)
@   
  \caption{Pain Ratings of Participants by Response to Placebo Across Time. Straight line is a loess smoother, the jagged line represents the actual pain levels}
  \label{fig:placyesno}
\end{figure}


A number of findings are apparent from the plot above in Figure \ref{fig:placyesno}. The placebo effect was approximately equivalent to a 15\% decrease in pain (read from the graph at the point the no response participants pain reached seven). This is a relatively large effect, and adds confidence to the significant results for modelling reported below. In addition, the participants who responded to placebo tended to remain in the experiment for a longer period of time (which is intuitively obvious). Below, formal model testing for the major hypotheses takes place. 

\subsection{Logistic Regressions on Placebo Response}
\label{sec:logist-regr-plac}
In order to examine whether or not the IAT scores were predictive of placebo response, a logistic regression model was used. 


<<placmod1, echo=FALSE, results=tex>>=
placmod1 <- glm(PlacResp~TCQIAT.Mean, data=Iatandexpmeasures, family=binomial(link="logit"))
placmod1.xtab <- xtable(summary(placmod1),label="tab:placmod1", caption="Logistic Regression of Treatment Credibility IAT on Placebo Response")
print(placmod1.xtab)
@ 

As can be seen above in Table \ref{tab:placmod1}, the treatment credibility IAT was not a significant independent predictor of placebo response.

<<placmod2, echo=FALSE, results=tex>>=
placmod2 <- glm(PlacResp~OptIAT.Mean, data=Iatandexpmeasures, family=binomial(link="logit"))
placmod2.xtab <- xtable(summary(placmod2), label="tab:placmod2", caption="Regression of Optimism IAT on Placebo Response")
print(placmod2.xtab)
@ 

As shown in Table \ref{tab:placmod2} the Optimism IAT is not an independent significant predictor of placebo response either. 

<<placebolotr, echo=FALSE, results=tex>>=
placmod.lotr<- glm(PlacResp~LOTR, data=Iatandexpmeasures, family=binomial(link="logit"))
placmod.lotr.xtab <- xtable(summary(placmod.lotr), label="tab:placebolotr", caption="Regression of LOT-R Scores on Placebo Response")
print(placmod.lotr.xtab)
@ 

In addition, the Life Orientation scores are not a significant predictor of placebo response either, as shown in Table \ref{tab:placebolotr}.

<<placmodlotrirt, echo=FALSE, results=tex>>=
placmod.lotr.irt <- glm(PlacResp~AbilityLOTR, data=Iatandexpmeasures.abest, family=binomial(link="logit"))
print(xtable(summary(placmod.lotr.irt), caption="Regression of Placebo Response on IRT Estimated Optimism Scores", label="tab:placmodlotrirt"))
@ 

Even when the IRT estimated Optimism scores are included in place of the sum-scores, the result does still not achieve the conventional (or indeed any) threshold of significance. These results are shown in Table \ref{tab:placmodlotrirt}. 


The next variable to be tested in terms of univariate regressions are the treatment credibility scores. These were tested in both their raw forms and as estimates from the successful IRT models described in Chapter \ref{cha:tcq-thesis}. 

<<placmodconv, echo=FALSE, results=tex>>=
print(xtable(summary(glm(PlacResp~Pill+Cream+Inj, data=Iatandexpmeasures, family=binomial(link="logit"))), caption="Regression of Conventional TCQ Summary Scores on Placebo Response", label="tab:placmodconvsumm"))
@ 

Table \ref{tab:placmodconvsumm} shows the estimated coefficients for the logistic regression of all three conventional sum scores on Placebo Response. As can be seen, the results were not significant. 

Next, the ability estimates generated through the best fitting IRT model were used as predictor variables in a regression on the placebo response. 

<<placmodirtabsest, echo=FALSE, results=tex>>=
print(xtable(summary(glm(PlacResp~AbilityConv+AbilityAlt, data=Iatandexpmeasures.grmtcq, family=binomial(link="logit"))), caption="Regression of Conventional TCQ Summary Scores on Placebo Response", label="tab:placmodirtest"))
@ 

Table \ref{tab:placmodirtest} shows the results of fitting the model using the IRT estimated abilities for the Trpeatment Credibility Questionnaire. As can be seen, the model is still not significant. However, the AIC was slightly lower for the model fitted using the ability estimates, which suggests that though they are not particularly good predictors, they are at least somewhat better than the raw sum-scores. 

<<placmodgsr, echo=FALSE, results=tex>>=
placmod10 <- glm(PlacResp~gsrmean, data=Iatandexpmeasures.phys, family=binomial(link="logit")) #extremely significant!
print(xtable(summary(placmod10), label="tab:placmodgsr", caption="Logistic Regression for the Impact of Mean GSR measurements on placebo response"))
@ 

As can be seen from Table \ref{tab:placmodgsr}, the mean level of skin response was not significantly associated with the response to placebo. 


Next, the three major variables (IAT's and Optimism) were placed into one model and allowed to interact. 

<<placeboglm, echo=FALSE, results=tex>>=
placmod6<- glm(PlacResp~TCQIAT.Mean*OptIAT.Mean*LOTR, data=Iatandexpmeasures, family=binomial(link="logit"))
placmod.xtab <- xtable(summary(placmod6), label="tab:placmodinter", caption="Regression of TCQIAT, OptIAT and LOT-R on Placebo Response")
print(placmod.xtab)
@ 

As can be seen from Table \ref{tab:placmodinter} when interactions were allowed and the Life Orientation Test was added to the model, both IAT measures were significantly associated with placebo response, and all of the three variables interactions were also significant.  The same relationship occurred when using the IRT estimated Optimism scores. Interestingly enough, this relationship only held with the three way interaction, which suggests that it is not particularly robut, and may be an artifact of multi-collinearity. 

In addition, this model explains approximately 27\% of the variance in placebo response (using a pseudo R-square of 1-(residual deviance/null deviance)).



\begin{figure}
  
<<ggplot3wayinter, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
iatexpno <- Iatandexpmeasures[with(Iatandexpmeasures,PlacResp=="No"),]
iatexpyes <- Iatandexpmeasures[with(Iatandexpmeasures,PlacResp=="Yes"),]
iatexpno <- na.omit(iatexpno)
iatexpyes <- na.omit(iatexpyes)
iatexpyesno <- rbind(iatexpyes, iatexpno)
lotriatplot <- ggplot(iatexpyesno, aes(x=LOTR, y=TCQIAT.Mean , colour=PlacResp, size=OptIAT.Mean))+geom_point()
print(lotriatplot)
@   
  \caption{Scatterplot of Optimism IAT, Treatment Credibility and LOT-R scores against mean survival time by Condition}
  \label{fig:3wayinter}
\end{figure}

 The plot above in Figure \ref{fig:3wayinter} indicates that there appears to be a non linear interaction between the three variables included in our final model. It can be seen that extremely high scores on both the treatment credibility questionnaire and the Life Orientation test appear to be associated with not responding to placebo, while moderate levels of all three variables appear to provide the greatest likelihood of placebo response. 
 %% \begin{figure}
<<lotrtcqsmoothplot, echo=FALSE, eval=FALSE,pdf=TRUE, eps=TRUE, png=TRUE>>=
lotrtcqplot <- ggplot(iatexpyesno, aes(x=LOTR, y=TCQIAT.Mean , colour=PlacResp))+geom_point()
print(lotrtcqplot)
@     
%%    \caption{Scatterplot of LOT-R Scores against TCQ IAT Scores, Placebo Response is denoted using colour}
%%    \label{fig:lotrtcqplot}
%%  \end{figure}


%% This is perhaps clearer in Figure \ref{fig:lotrtcqplot} where it can be seen that the majority of placebo response occurs at median levels of both self reported optimism and implicit treatment credibility. 

%% \begin{figure}
<<lotroptplot, echo=FALSE,pdf=TRUE, eps=TRUE, png=TRUE>>=
lotroptplot <- ggplot(iatexpyesno, aes(x=LOTR, y=OptIAT.Mean , colour=PlacResp))+geom_point()
print(lotroptplot)
@   
%%   \caption{LOTR Scores against Optimism Scores, colour denotes placebo response}
%%   \label{fig:lotroptplot}
%% \end{figure}


Scores of 0 or above on the optimism IAT (reflecting no to small difference in favour of positive stimuli) and high self reported optimism are associated with a greater likelihood of placebo response. 

\subsection{Machine Learning and the Placebo Response}
\label{sec:mach-learn-plac}

Given the confusing results obtained from linear statistical modelling of the placebo response, it was decided to fit a more flexible model, treating the placebo response as a two valued (Yes, No) classification task. 

Firstly, the data was split into two random sections, with 80\% of the data utilised to train the models, and 20\% held aside for a final validation of any model's predictive power. Additionally, the training set was split into ten sections, of which nine were used to train the model and one was used to test the model. This allowed for the accuracy of each model to be ranked in a preliminary fashion, though the ultimate arbiter was performance on the training set. 

<<randomforestCV, echo=FALSE, results=hide, cache=TRUE>>=
train.ind <- sample(1:54, 40, replace=FALSE)
iatandexpfull <- Iatandexpmeasures[,c("PlacResp", "Age", "Gender", "Prime", "LOTR", "MAAS", "Pill", "Cream", "Inj", "Acu", "Hom", "Rei", "OptIAT.Mean", "OptIAT.Median", "TCQIAT.Mean", "TCQIAT.Median", "meanconv", "meanalt", "convaltcomp")]
iatandexpfull <- na.omit(iatandexpfull)
iatandexpfull.train.ind <- with(iatandexpfull,createDataPartition(PlacResp, p=0.8, list=FALSE))
iatandexpfull.train <- iatandexpfull[iatandexpfull.train.ind,]
iatandexpfull.test <- iatandexpfull[-iatandexpfull.train.ind,]
rf.train <- train(PlacResp~TCQIAT.Mean+meanconv+OptIAT.Mean+LOTR, data=iatandexpfull, method="rf", preProcess=c("center", "scale"))
rf.pred <- predict(rf.train, iatandexpfull.test)

var.used.count <- varUsed(rf.train[["finalModel"]])
var.used.count <- as.data.frame(var.used.count)
rownames(var.used.count) <- c("TCQIAT.Mean", "OptIAT.Mean", "meanconv", "LOTR")

## var.used.count.m[,"variable"] <- names(iatandexpfull)
## ggplot(var.used.count.m, aes(x=variable, y=value))+geom_histogram()+coord_flip()

@ 

<<rf2, echo=FALSE, results=hide, cache=TRUE>>=
iatandexpfull.train.ind2 <- with(iatandexpfull,createDataPartition(PlacResp, p=0.8, list=FALSE))
iatandexpfull.train2 <- iatandexpfull[iatandexpfull.train.ind2,]
iatandexpfull.test2 <- iatandexpfull[-iatandexpfull.train.ind2,]
rf.train2 <- train(PlacResp~., data=iatandexpfull.train2, method="rf", metric="Kappa", maximise=TRUE)
rf.pred2 <- predict(rf.train2, iatandexpfull.test2)
@ 

<<rfTestResults, echo=FALSE, results=hide>>=
test.xtab <- xtable(confusionMatrix(rf.pred, iatandexpfull.test$PlacResp)$table, label="tab:rfplac", caption="Random Forest Accuracy on Placebo Response (Test set)")
print(test.xtab)
@ 

%% As can be seen from Table \ref{tab:rfplac}, the random forest predicted the unseen data for the placebo response exceptionally well, with an accuracy of 1 ($95CI 0.7684-1$), and was significantly better than a random prediction ($p=0.002059$). This suggests that the major hypothesis of the thesis, that implicit measures would be a useful predictor of the placebo response was accurate. This subject is covered further in the discussion. 

%% Models which were fit to the data included a naive bayes classifier ($Acc=0.5714$), a linear support vector machine ($Acc=0.6439$), an an evolutionary tree model ($Acc=0.7857$). Of these, only the evolutionary tree performed above chance when predicting positive placebo response (all models predicted no placebo response correctly). This may be, as the plots above in section \ref{sec:vari-impact-plac} showed, the relationship appeared to be nonlinear, in that moderate levels of the three predictor variables appeared to be more highly associated with positive response to placebo. Tree models allow the data to be split on particular levels of a predictor variable, and this flexibility allows placebos to be predicted more accurately. 

%% \begin{figure}
<<varImpPlot, echo=FALSE, pdf=TRUE, eps=TRUE, png=TRUE, eval=FALSE>>=
par(mfrow=c (1,1))
print(varImpPlot(rf.train[["finalModel"]]))
@   
%%   \caption{Variable Importance Plot for Random Forest Model on Placebo Response}
%%   \label{fig:varimprf}
%% \end{figure}

%% As can be seen from Figure \ref{fig:varimprf}, all three variables were important to the fit, however the TCQ IAT appeared to be the most important, followed by the mean credibility score for conventional treatments, followed by the Optimism IAT, followed by scores on the LOT-R.


<<evtree, echo=FALSE, results=hide>>=
evtree.train <- train(PlacResp~OptIAT.Mean+TCQIAT.Mean+meanconv, data=iatandexpfull.train, method="evtree", preProcess=c("center", "scale"))
evtree.pred <- predict(evtree.train, iatandexpfull.test)
evtree.confusionmat <- confusionMatrix(evtree.pred, iatandexpfull.test$PlacResp)
@ 

%% To have some kind of graphical representation of how the tree works, the evolutionary tree final model on the training set was plotted. As can be seen from Figure \ref{fig:evtreeplot}, 
%% The Treatment Credibility IAT was responsible for most of the predictive power of the model. As hypothesised earlier, it appears that a particular level of score on the TCQIAT was associated with a positive response to placebo. Note that the variables on the plot are scaled and centered, so this cannot be read off as the particular scores on the TCQ-IAT associated with placebo response. 

%% This model was so accurate that it made the researcher suspicious that it was an artefact of the particular random sub-sample chosen as a test set. In fact, when the entire model-fitting procedure was repeated ten times with a different random test set each time, the accuracy of the model dropped dramatically ($Acc=\bar 0.5$). Next, the model fitting process was repeated using this same process. 

%% \begin{figure}
<<evtreeplot, echo=FALSE,pdf=TRUE, eps=TRUE, png=TRUE, eval=FALSE>>=
print(plot(evtree.train$finalModel))
@   
%%   \caption{Evolutionary Tree plot for Prediction of Placebo Response}
%%   \label{fig:evtreeplot}
%% \end{figure}


<<testandtrain, echo=FALSE, results=hide>>=
require(doMC)
registerDoMC(3)
iatandexpfull <- Iatandexpmeasures[,c("PlacResp", "Age", "Gender", "Prime", "LOTR", "MAAS", "Pill", "Cream", "Inj", "Acu", "Hom", "Rei", "OptIAT.Mean", "OptIAT.Median", "TCQIAT.Mean", "TCQIAT.Median", "meanconv", "meanalt", "convaltcomp")]
iatandexpfull <- na.omit(iatandexpfull)
iatandexpfull.train.ind <- with(iatandexpfull,createDataPartition(PlacResp, p=0.8, list=FALSE))
iatandexpfull.train <- iatandexpfull[iatandexpfull.train.ind,]
iatandexpfull.test <- iatandexpfull[-iatandexpfull.train.ind,]
mytrain <- trainControl(method="repeatedcv", number=10, repeats=25)
@ 


<<rftrain, echo=FALSE, results=hide, eval=FALSE>>=
randf.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv, data=iatandexpfull.train, method="rf", trControl=mytrain, tuneGrid=data.frame(.mtry=1:10), preProcess=c("center", "scale"))
randf.pred <- predict(randf.train, iatandexpfull.test)
@ 

<<nbtrain, echo=FALSE, results=hide, eval=FALSE>>=
nb.train <- train(PlacResp~., data=iatandexpfull.train, method="nb", trControl=mytrain, tuneGrid=data.frame(.usekernel=c(TRUE, FALSE), .fL=0:1), preProcess=c("center", "scale"))
nb.train.pc <- train(PlacResp~., data=iatandexpfull.train, method="nb", trControl=mytrain, tuneGrid=data.frame(.usekernel=c(TRUE, FALSE), .fL=0:1), preProcess=c("center", "scale", "pca"))
@

<<knntrain, echo=FALSE, results=hide, eval=FALSE>>=
knn.train.pc <- train(PlacResp~. , data=iatandexpfull.train, method="knn", trControl=mytrain, tuneGrid=data.frame(.k=1:10), 
                   preProcess=c("center", "scale", "pca"))
knn.train <- train(PlacResp~. , data=iatandexpfull.train, method="knn", trControl=mytrain, tuneGrid=data.frame(.k=1:10), 
                   preProcess=c("center", "scale"))
@ 

<<svmtrain, echo=FALSE, results=hide, eval=FALSE>>=
svm.train.pc <- train(PlacResp~., data=iatandexpfull.train, method="svmLinear", trControl=mytrain, tuneGrid=data.frame(.C=1:10), preprocess=c("center", "scale", "pca"))
svm.train <- train(PlacResp~., data=iatandexpfull.train, method="svmLinear", trControl=mytrain, tuneGrid=data.frame(.C=1:10), preprocess=c("center", "scale"))
@ 

<<svmradialtrain, echo=FALSE, results=hide, eval=FALSE>>=
svmradial.train <- train(PlacResp~., data=iatandexpfull.train, method="lssvmRadial", trControl=mytrain, preProcess=c("center", "scale", "pca"))
@ 

<<gbmtrain, echo=FALSE, results=hide, eval=FALSE>>=
gbm.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+LOTR+meanconv, data=iatandexpfull.train, method="gbm", trControl=mytrain, tuneGrid=data.frame(.shrinkage=(c(0.1, 0.01, 0.001)), .n.trees=c(50,100,150), .interaction.depth=3))
@ 

<<gamboost, echo=FALSE, results=hide>>=
gamboost.train <- train(PlacResp~., data=iatandexpfull.train, method="gamboost", trControl=mytrain, tuneGrid=data.frame(.mstop=1:100, .prune=1:100))
@ 

<<glmboost, echo=FALSE, results=hide>>=
glmboost.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv, data=iatandexpfull.train, method="glmboost", trControl=mytrain, tuneGrid=data.frame(.mstop=1:100, .prune=1:100))
@ 

<<logitboost, echo=FALSE, results=hide, eval=FALSE>>=
logitboost.train <-  train(PlacResp~., data=iatandexpfull.train, method="logitBoost", trControl=mytrain, tuneGrid=data.frame(.nIter=seq(10,500, by=10)))
@ 

<<glmnettrain, echo=FALSE, results=hide>>=
glmnet.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv, data=iatandexpfull.train, method="glmnet", trControl=mytrain, tuneGrid=data.frame(.alpha=seq(0.1, 1, by=0.1), .lambda=seq(0.1, 1.0, by=0.1)))
@ 

<<blackboost, echo=FALSE, results=hide, eval=FALSE>>=
blackboost.train <- train(PlacResp~., data=iatandexpfull.train, method="blackboost", trControl=mytrain, tuneGrid=data.frame(.mstop=c(50,100,150), .maxdepth=1:3))
@ 

<<glmstep, echo=FALSE, results=hide>>=
glmstep.train <- train(PlacResp~., data=iatandexpfull.train, method="glmStepAIC", trControl=mytrain)
@ 

<<rpart, echo=FALSE, results=hide>>=
rpart.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv, data=iatandexpfull.train, method="rpart", trControl=mytrain, tuneGrid=data.frame(.cp=seq(0.01, 1, by=0.01)))
@ 

<<rpart2, echo=FALSE, results=hide, eval=FALSE>>=
rpart2.train <- train(PlacResp~., data=iatandexpfull.train, method="rpart2", trControl=mytrain, tuneGrid=data.frame(.maxdepth=1:10), preProcess=c("center", "scale"))
@ 

<<svmRadialCost, echo=FALSE, results=hide, eval=FALSE>>=
svmradialcost.train.pc <- train(PlacResp~., data=iatandexpfull.train, method="svmRadialCost", trControl=mytrain, tuneGrid=data.frame(.C=1:10), preProcess=c("center", "scale", "pca"))
svmradialcost.train <- train(PlacResp~., data=iatandexpfull.train, method="svmRadialCost", trControl=mytrain, tuneGrid=data.frame(.C=1:10), preProcess=c("center", "scale"))
@ 

<<ctree, echo=FALSE, results=hide, eval=FALSE>>=
ctree2.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv, data=iatandexpfull.train, method="ctree2", trControl=mytrain, tuneGrid=data.frame(.maxdepth=seq(0,10, by=1)), preProcess=c("center", "scale"))
@ 

<<evtree, echo=FALSE, results=hide>>=
evtree.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv, data=iatandexpfull.train, method="evtree", trControl=mytrain, tuneGrid=data.frame(.alpha=1:10), preProcess=c("center", "scale"))
@ 

<<cforest, echo=FALSE, results=hide, eval=FALSE>>=
cforest.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv, data=iatandexpfull.train, method="cforest", trControl=mytrain, tuneGrid=data.frame(.mtry=seq(1, 101, by=10)))
@ 

<<rferns, echo=FALSE, results=hide, eval=FALSE>>=
rferns.train <- train(PlacResp~., data=iatandexpfull.train, method="rFerns", trControl=mytrain, tuneGrid=data.frame(.depth=1:10) )
@ 

<<pred, echo=FALSE, results=hide>>=
## nb.pc.pred <- predict(nb.train.pc, iatandexpfull.test)
## nb.pred <- predict(nb.train, iatandexpfull.test)
## knn.pc.pred <- predict(knn.train.pc, iatandexpfull.test)
## knn.pred <- predict(knn.train, iatandexpfull.test)
## svm.pc.pred <- predict(svm.train.pc, iatandexpfull.test)
## svm.pred <- predict(svm.train, iatandexpfull.test)
gamboost.pred <- predict(gamboost.train, iatandexpfull.test)
glmboost.pred <- predict(glmboost.train, iatandexpfull.test)
logitboost.pred <- predict(logitboost.train, iatandexpfull.test)
glmnet.pred <- predict(glmnet.train, iatandexpfull.test)
## blackboost.pred <- predict(blackboost.train, iatandexpfull.test)
glmstep.pred <- predict(glmstep.train, iatandexpfull.test)
rpart.pred <- predict(rpart.train, iatandexpfull.test)
rpart2.pred <- predict(rpart2.train, iatandexpfull.test)
## svmradialcost.pred <- predict(svmradialcost.train, iatandexpfull.test)
## ctree.pred <- predict(ctree2.train, iatandexpfull.test)
## evtree.pred <- predict(evtree.train, iatandexpfull.test)
cforest.pred <- predict(cforest.train, iatandexpfull.test)
## rferns.pred <- predict(rferns.train, iatandexpfull.test)
@ 

<<confusionmats, echo=FALSE, results=hide>>=
randf.acc <-confusionMatrix(randf.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
## nb.acc <-confusionMatrix(nb.pc.pred, iatandexpfull.test[,"PlacResp"])
## knn.pc.acc <- confusionMatrix(knn.pc.pred, iatandexpfull.test[,"PlacResp"])
## knn.acc <- confusionMatrix(knn.pred, iatandexpfull.test[,"PlacResp"])
## svm.pc.acc <-confusionMatrix(svm.pc.pred, iatandexpfull.test[,"PlacResp"])
## svm.acc <- confusionMatrix(svm.pred, iatandexpfull.test[,"PlacResp"])
gamboost.acc <- confusionMatrix(gamboost.pred, iatandexpfull.test[,"PlacResp"])
glmboost.acc <- confusionMatrix(glmboost.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
glmnet.acc <- confusionMatrix(glmnet.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
## blackboost.acc <- confusionMatrix(blackboost.pred, iatandexpfull.test[,"PlacResp"])
glmstep.acc <- confusionMatrix(glmstep.pred, iatandexpfull.test[,"PlacResp"])
rpart.acc <- confusionMatrix(rpart.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
rpart2.acc <- confusionMatrix(rpart2.pred, iatandexpfull.test[,"PlacResp"])
## svmradial.acc <- confusionMatrix(svmradialcost.pred, iatandexpfull.test[,"PlacResp"])
## ctree.acc <- confusionMatrix(ctree.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
## evtree.acc <- confusionMatrix(evtree.pred, iatandexpfull.test[,"PlacResp"])
cforest.acc <- confusionMatrix(cforest.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
## rferns.acc <- confusionMatrix(rferns.pred, iatandexpfull.test[,"PlacResp"])
@ 


After an extensive model fitting process, only three models reached acceptable levels of accuracy. One algorithm which achieved acceptable accuracy was the conditional random forest model. These models form ensembles of trees to reduce variability and increase predictive power. This model achieved an accuracy of 0.73 ($95CI 0.4281 - 0.9451$), with a sensitivity of 0.8750 and a specificity of 0.5. 

The other model which was selected as being acceptably accurate was a glmboost algorithm which averages the results of many generalised linear models (logistic regression, in this case) and weights observations which were categorised incorrectly more highly in the next iteration of the algorithm. This model achieved an accuracy of 0.875, ($95CI 0.4822 -- 0.9772$), with a sensitivity of 1 and a specificity of 0.5. 

This would suggest that while the model was excellent at predicting those who would not respond to placebo, it was much worse (chance level) at predicting those who would. Again, note that the positive examples of placebo response are harder to predict than are the negative ones. Note that these results should be regarded as tentative at best, given the extremely small sample size of the dataset. 





%% \subsection{Machine Learning and the Placebo Response}
%% \label{sec:mach-learn-plac}-


\subsection{Testing the Theoretical Models}
\label{sec:test-theor-models}

The final step in the analysis was to test the theoretical models which were proposed for the relationship between the Placebo Response and implicit and explicit expectancies. The models  outlined in Chapter \ref{cha:methodology} are briefly recapped below:
\begin{itemize}
\item The direct expectancy model of Kirsch, in which the effects of all other variables are mediated through expectancies (both implicit and explicit)

\item The conscious expectancy model of Stewart-Williams \textit{et al} whereby implicit measures have no effect

\item A model whereby optimism mediates the effects of all other variables on placebo response

\item Two models which allow only implicit or explicit expectancy measures to be the mediators of the placebo response. 
\end{itemize}




<<mod1sem, echo=FALSE, results=hide, eval=FALSE>>=
pred <- c("Gender", "LOTR", "OptIATMean", "TCQIATMean", "convaltcomp")
resp <- "PlacResp"
variables <- c(pred, resp)
plac.mod.direct <- mxModel(name="PlacModDirect",
                           type="RAM",
                           mxData(observed=na.omit(exp.sem),
                                  type="raw",
                                  means=NA,
                                  numObs=NA),
                           manifestVars=variables,
                           mxPath(from=variables, arrows=2, free=FALSE, values=1),
          
                           mxPath(from="Gender", to=c( "LOTR", "OptIATMean", "TCQIATMean", "convaltcomp"), connect="unique.pairs", arrows=2),
                           mxPath(from="LOTR", to=c( "Gender", "OptIATMean", "TCQIATMean", "convaltcomp" ), connect="unique.pairs", arrows=2),
                           mxPath(from="OptIATMean", to=c("Gender", "LOTR", "TCQIATMean", "convaltcomp"), connect="unique.pairs", arrows=2),
                           mxPath(from="TCQIATMean", to=c( "Gender", "LOTR", "OptIATMean", "convaltcomp"), connect="unique.pairs", arrows=2),
                           mxPath(from="convaltcomp", to=c( "Gender", "LOTR", "OptIATMean"), connect="unique.pairs", arrows=2),
                           mxPath(from="one", to=variables, arrows=1, free=FALSE, values=1),
                           ## mxPath(from="Age", to="PlacResp", arrows=1),
                           mxPath(from="Gender", to="PlacResp", arrows=1),
                           mxPath(from="LOTR", to="PlacResp", arrows=1),
                           mxPath(from="OptIATMean", to="PlacResp", arrows=1),
                           mxPath(from="TCQIATMean", to="PlacResp", arrows=1),

                           mxPath(from="convaltcomp", to="PlacResp", arrows=1)
                           )
                                  
@ 

<<placmod1semrun, echo=FALSE, results=hide, eval=FALSE>>=
placmod1.fit <- mxRun(plac.mod.direct)
@



<<impexpdata, echo=FALSE, results=hide>>=
impexpdata <- Iatandexpmeasures2[,c("TCQIATMean", "OptIATMean", "MAAS", "meanconv", "meanalt", "LOTR")]
@ 

<<modimpexp, echo=FALSE, results=hide, eval=FALSE>>=

latents <- "Expectancies"
observed <- c("TCQIATMean", "OptIATMean", "MAAS", "meanconv", "meanalt")
imp.exp.direct <- mxModel(name="ImpExpDirect",
                          type="RAM",
                          manifestVars=observed,
                          latentVars=latents,
                          mxPath(from=observed, connect="unique.pairs"),
                          mxPath(from=observed, to=latents, connect="unique.pairs", values=0.5),
 ##                          mxPath(from="TCQIATMean", to=c("OptIATMean", "MAAS", "meanconv", "meanalt", "MAAS"), connect="unique.pairs", arrows=2),
 ##                          mxPath(from="OptIATMean", to=c("MAAS", "TCQIATMean",  "meanconv", "meanalt", "MAAS"), connect="unique.pairs", arrows=2),
 ##                          mxPath(from="MAAS", to=c("TCQIATMean", "OptIATMean", "meanconv", "meanalt"), connect="unique.pairs", arrows=2),
 ##                          mxPath(from="meanconv", to=c("TCQIATMean", "OptIATMean", "MAAS", "meanalt"), arrows=2, connect="unique.pairs"),
 ##                          mxPath(from="meanalt", to=c("TCQIATMean",
 ## "OptIATMean", "MAAS", "meanconv"), connect="unique.pairs", arrows=2),
                          mxPath(from="one", to=c("TCQIATMean", "OptIATMean", "MAAS", "meanconv", "meanalt"), arrows=1, values=0.5),
                          mxData(observed=na.omit(impexpdata), type="raw"))
                                 
@ 
<<impexpmediated, echo=FALSE, results=hide, eval=FALSE>>=
latents2 <- c("ImplicitExp", "ExplicitExp")
imp.exp.mediated1 <- mxModel(imp.exp.direct, mxPath(from=observed, to=latents, arrows=1), remove=TRUE) 
imp.exp.mediated2 <- mxModel(
                             name="TwoFacMod",
    type="RAM",
                             latentVars=latents2,
    manifestVars=observed,
    
                             mxPath(from=c("TCQIATMean", "OptIATMean"), to="ImplicitExp", connect="unique.pairs", arrows=1),
                             mxPath(from="ImplicitExp",  arrows=1, free=FALSE, values=0.5),
                             mxPath(from="ExplicitExp", arrows=1, free=FALSE, values=0.5),
                             mxPath(from=c("meanconv", "meanalt", "MAAS"), to="ExplicitExp", connect="unique.pairs", arrows=1, values=0.5),
    mxPath(from="TCQIATMean", to=c("OptIATMean", "MAAS", "meanconv", "meanalt", "MAAS"), connect="unique.pairs", arrows=2, values=0.5),
                          mxPath(from="OptIATMean", to=c("MAAS", "TCQIATMean",  "meanconv", "meanalt", "MAAS"), connect="unique.pairs", arrows=2, values=0.5),
                          mxPath(from="MAAS", to=c("TCQIATMean", "OptIATMean", "meanconv", "meanalt"), connect="unique.pairs", arrows=2, values=0.5),
                          mxPath(from="meanconv", to=c("TCQIATMean", "OptIATMean", "MAAS", "meanalt"), arrows=2, connect="unique.pairs", values=0.5),
                          mxPath(from="meanalt", to=c("TCQIATMean",
 "OptIATMean", "MAAS", "meanconv"), connect="unique.pairs", arrows=2, values=0.5),
    mxPath(from="one", to=observed, free=FALSE, values=0.5),
    mxData(observed=cov(na.omit(impexpdata)), type="cov", numObs=95, means=sapply(impexpdata, mean, na.rm=TRUE))
    )
@ 

<<runimpexpmod, echo=FALSE, results=hide, eval=FALSE>>=
imp.exp.direct.fit <- mxRun(imp.exp.direct)
imp.exp.twofac.fit <- mxRun(imp.exp.mediated2)
@ 

<<testimpexpmod, echo=FALSE, results=hide, eval=FALSE>>=
imp.exp.comp <- mxCompare(base=imp.exp.direct, comp=c(imp.exp.mediated2))
@ 

<<placsemlavaan, echo=FALSE, results=hide>>=
readLines("lavaanPlac.R")
exp.sem[,"PlacResp"] <- with(exp.sem, ordered(PlacResp))
exp.sem2 <- na.omit(exp.sem)
placmod.first.mod <- lavaanify(placmod.first, fixed.x=FALSE, std.lv=TRUE)
placmod.kirsch.mod <- lavaanify(placmod.kirsch, fixed.x=FALSE)
placmod.cred.opt.mod <- lavaanify(placmod.cred.opt, std.lv=TRUE, fixed.x=FALSE)
placmod.opt.mod <- lavaanify(placmod.opt)

@ 


<<placsemlavaanfit, echo=FALSE, results=hide>>=
placmod.direct.fit <- sem(placmod.direct, data=exp.sem2)
placmod.first.fit <- sem(placmod.first, data=exp.sem2,  orthogonal=TRUE)
placmod.kirsch.fit <- sem(placmod.kirsch, data=exp.sem2)
placmod.cred.opt.fit <- sem(placmod.cred.opt, data=exp.sem2)
placmod.opt.fit <- sem(placmod.opt, data=exp.sem2)
placmod.cred.opt.twofac.fit <- sem(placmod.cred.opt.twofac, data=exp.sem2, std.lv=TRUE,orthogonal=TRUE)
## placmod.phys.fit <- sem(placmod.phys, data=exp.sem, std.lv=TRUE, estimator="GLS")


placmod.kirsch.twofac.fit <- sem(placmod.kirsch.twofac, data=exp.sem)
@ 


\begin{figure}
<<firstfitplot, echo=FALSE, fig=TRUE>>=
semPaths(placmod.first.fit)
@ 
  \caption{Direct Placebo Response Model (Model 1)}
  \label{fig:placsem1}
\end{figure}


Figure \ref{fig:placsem1} shows the direct placebo model, where implicit and explicit expectancy factors have direct effects on the placebo response. Unfortunately, this model did not converge, rendering the likelihood of its truth open to debate \footnote{given a small data set such as this, the possibility exists that even the true model could be rejected}. Nonetheless, this model will not be discussed further. 


The next model fit is the model of Kirsch, where all other parameters are mediated by (explicit) expectancies and implicit expectancies have no impact. 

\begin{figure}
<<kirschfitplot, echo=FALSE, fig=TRUE>>=
semPaths(placmod.kirsch.fit)
@ 
  \caption{SEM Model of Kirsch (all variables mediated by explicit expectancies)}
  \label{fig:placsemkirsch}
\end{figure}

This model is shown in Figure \ref{fig:placsemkirsch}. This model fits, but it has only two degrees of freedom remaining, so is almost certainly overfitting the data. 

Next, a model where expectancies and optimism are jointly responsible for the observed placebo response, and implicit expectancies play no role. 


This model is shown in Figure \ref{fig:placsemcredopt}
  
\begin{figure}
<<credoptplot, echo=FALSE, fig=TRUE>>=
semPaths(placmod.cred.opt.fit)
@   
  \caption{Placebo Response SEM mediated by Explicit Credibility and Optimism}
  \label{fig:placsemcredopt}
\end{figure}


Next,  a model where optimism was the sole mediator of the placebo response was fitted, and this model is shown in Figure \ref{fig:placsemopt}. 


\begin{figure}
<<optsemplot, echo=FALSE, fig=TRUE>>=
semPaths(placmod.opt.fit)
@   
  \caption{Placebo SEM mediated by Optimism}
  \label{fig:placsemopt}
\end{figure}


\begin{figure}
<<kirschtwofacplot, echo=FALSE, fig=TRUE>>=
semPaths(placmod.kirsch.twofac.fit, what="par")
@   
  \caption{Placebo SEM Kirsch with implicit measures}
  \label{fig:placsemkirschtwofac}
\end{figure}

Finally, a model like that of Kirsch, but including the implicit measures was fitted. This model is shown in Figure \ref{fig:placsemkirschtwofac}. 

Another model was fitted with an independent effect of GSR on the placebo response, and all other variables mediated through a general expectancy factor. Unfortunately, this model did not converge, and so is not discussed further. 

<<placmodfitall, echo=FALSE, results=tex>>=
## placmod.direct.fit.measures <- fitMeasures(placmod.direct.fit, c("chisq", "df", "pvalue", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"))
placmod.kirsch.fit.measures <- fitMeasures(placmod.kirsch.fit, c("chisq", "df", "pvalue", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"))
placmod.cred.opt.fit.measures <- fitMeasures(placmod.cred.opt.fit, c("chisq", "df", "pvalue", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"))
placmod.opt.fit.measures <- fitMeasures(placmod.opt.fit, c("chisq", "df", "pvalue", "cfa",  "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"))
placmod.opt.fit.measures <- fitMeasures(placmod.opt.fit, c("chisq", "df", "pvalue", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"))
placmod.kirsch.twofac.fit.measures <- fitMeasures(placmod.kirsch.twofac.fit, c("chisq", "df", "pvalue", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"))
placmod.fit.all <- rbind( placmod.kirsch.fit.measures, placmod.cred.opt.fit.measures, placmod.opt.fit.measures, placmod.kirsch.twofac.fit.measures)
rownames(placmod.fit.all) <- c( "Kirsch", "Optimism", "Credibility", "Kirsch Two Factor")
print(xtable(placmod.fit.all, label="tab:placmodsemfit", caption="Fit Measures for Placebo Response Structural Equation Models"))
@ 

From examining the fit measures shown in Table \ref{tab:placmodsemfit} it would appear that the best fitting model is that of Kirsch when implicit expectancies are included in the model. Implications are elaborated upon in the discussion. 

\subsection{Relationships between Explicit and Implicit Measures}
\label{sec:relat-betw-expl}

Next, three models were fit to examine the relationship between the implicit and explicit measures themselves. Of interest here was whether or not mindfulness (operationalised using the MAAS) would moderate the relationships between explicit and implicit measures. 

The three models were as follows:

\begin{enumerate}

\item A one factor model for all implicit and explicit measures
  
\item A two factor correlated model between implicit and explicit measures

\item A two factor uncorrelated model with implicit and explicit measures

\item A two factor model where the covariances between implicit and explicit measures were moderated by mindfulness. 
\end{enumerate}

<<impexpmod, echo=FALSE, results=hide>>=
expimp1.mod <- lavaanify(expimpdirect)
twofac.impexp.mod <- lavaanify(expimptwofac)
expimp1.fit <- sem(expimp1.mod, data=na.omit(impexpdata))
twofac.impexp.fit <- sem(twofac.impexp.mod, data=na.omit(impexpdata))
@ 

<<impexpmodels, echo=FALSE, results=hide>>=
expimpsep.fit <- sem(expimpsep, data=na.omit(impexpdata))
expimp.moderated.fit <- sem(expimp.moderated, data=na.omit(impexpdata))
## expimp.nocorr.fit <- sem(expimptwofac, data=na.omit(impexpdata), orthogonal=TRUE)
@ 

<<expimpfit, echo=FALSE, results=tex>>=
expimp1.fit.meas <- fitMeasures(expimp1.fit, c("chisq", "df", "pvalue", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"))
twofac.fit.meas <- fitMeasures(twofac.impexp.fit, c("chisq", "df", "pvalue", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"))
## expimpsep.fit.meas <- fitMeasures(expimpsep.fit, c("chisq", "df", "pvalue", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"))
expimpmoderated.fit.meas <- fitMeasures(expimp.moderated.fit, c("chisq", "df", "pvalue", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"))
## expimp.nocorr.fit.meas <- fitMeasures(expimp.nocorr.fit, c("chisq", "df", "pvalue", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"))
expimpall <- rbind(expimp1.fit.meas, twofac.fit.meas, expimpmoderated.fit.meas)
rownames(expimpall) <- c("Direct", "Two Factor", "Moderated")
print(xtable(expimpall, label="tab:expimplavaanfit", caption="Fit Statistics for the SEM Models on the Relationship between Implicit and Explicit Expectancies"))
@ 

The next step in the use of SEM models was to examine whether a two factor or one factor model best fitted the relationship between implicit and explicit models. Comparing a model where both implicit and explicit measures loaded on one factor to a model in which they loaded on seperate factors showed that the two factor solution provided a better fit to the data. Given the lack of data for a test sample in this portion of research, these results should be regarded as provisional until replicated. 

\section{Discussion} 

%% \subsection{Open Placebos}
%% \label{sec:open-placebos}



%% This study has revealed a number of findings. Firstly, and most unexpectedly, the group who were administered an Open Placebo showed greater reductions of pain than did a group who were told that they would receive a real treatment. This is a partial replication of Kaptchuk et al 2010. However note that there were slight differences in the suggestions given to participants, with the use of the term ``clinically proven'' appearing in the open placebo suggestion, while the term ``recently approved'' appearing in the deceptive placebo condition. This difference may have accounted for the unexpected results in this part of the study. 

\subsection{Relationship between Implicit Measures and Placebo}
\label{sec:relat-betw-impl}


Some evidence did suggest that the IAT measures were related to placebo response. Indeed, all of the machine learning methods were able to predict the absence of placebo response, while only one (the recursive partitioning algorithm) was able to learn positive examples of placebo (but performed much worse on the task of predicting a negative placebo response).  Additionally the logistic regressions showed a three way interaction between the two IAT's and Optimism. 

A number of caveats are in order here. Firstly, neither the optimism IAT or the treatment credibility IAT were independently predictive of placebo response. However, when the model included an interaction between them, all three of these variables were significant (as in the model shown above). This may indicate that there may be some irrelevant (from the conventional scoring perspective) feature of the IAT measures which was predictive of placebo response in this sample. This is a matter which can be teased out by future research. 

Secondly, given the number of models fitted, some were almost certain to come up as significant, and the three way interaction between the two IAT measures and the Life Orientation Test was not a hypothesis of the research. 

\subsection{Scoring Methods}
\label{sec:scoring-methods}



A secondary aim of this study was to examine if IRT scoring methods might prove better when compared to sum-scores in the experiment. This prediction was not borne out by the data, although there were some indicators that it might have some merit. Nonetheless, the methodology applied in this research might prove more useful in other research domains. 

The relationship between LOT-R and MAAS scores was also of some interest, as it was in the opposite direction from that observed in the previous studies. The notion that the previous results were an artefact of the online scoring method does not hold up to scrutiny, given that the same results were observed in the paper-based scoring method from Study 1 (see Chapter \ref{cha:health-for-thesis}). However, one difference between the prior use of these measures with that in the current study was the order in which they were administered. In the previous work, the MAAS questions were answered before the LOT-R, while in this chapter, this order was reversed (as an artefact of the printing process). Given that the MAAS represents mindfulness as a lack of mindlessness, it seems somewhat plausible that answering these questions made the lack of their mindfulness salient to those participants who would typically have rated their optimism relatively highly, and thus moved their scores downwards, accounting for the observed results. This theory is discussed more in Chapter \ref{cha:general-discussion}. 

\subsection{Modelling the Placebo Response with SEM}
\label{sec:modell-plac-resp}

This section threw up a number of interesting results, with the caveat that these results should be regarded as very preliminary. There were some difficulties with fitting these models in general, given the small sample size, and a number of models of theoretical interest did not converge. However, it does appear that based on the data collected here, that both implicit and explicit measures provide a useful fit to the data. Furthermore, these results would appear to support the theoretical model of Kirsch, whereby all other variables are mediated by expectancies, which exerts the change upon the placebo response. 


\subsection{Physiological Analyses}
\label{sec:phys-analys}

This section was perhaps the most interesting, in that some observations were made that do not appear to have been reported previously in the literature. Firstly, the GSR responses appeared to vary systematically as a result of Condition, more specifically as a result of the suggestions given for the Deceptive and Open Placebo conditions. This can be seen by examining the cross-correlations between pain ratings and GSR responses. These were systematically correlated in the two ``active'' conditions, where suggestions were given, while not correlated at all in the No Treatment condition. Given that the only difference in the experimental conditions was the suggestions, this would seem to suggest that the suggestion can affect the physiological responses of the participants. More interesting was that the two suggestions appeared to have directionally different influences, in that GSR appeared to go down in the Open Placebo condition, while it increased in the Deceptive Placebo condition. 

Finally, it appeared that participants who responded to placebo had a qualitively different GSR responses to those who did not. Again, this is something which has not been reported in the literature before, and as such deserves further investigation. 

\subsection{Priming and Placebo Response}
\label{sec:prim-plac-resp}

The final contribution of this research is that it provides further evidence that priming manipulations can be used to induce placebo effects in a pain paradigm. This is important for the future development of the field, as priming is much less resource-intensive than current procedures of conditioning. This will assist future research as it will allow for more powerful studies to be performed with fewer participants. This may also prove useful as an adjunct to standard clinical practice, subject to ethical concerns being mitigated. 



\subsection{Conclusions}
\label{sec:conclusions}

To recap, the following has been learned from this research. 

\begin{enumerate}
\item There appears to be a relationship between priming and the placebo response

\item There appears to be no direct relationship between the implicit measures used in this research and the placebo response

\item There may be an interaction effect between the placebo response and a combination of explicit and implicit measures

\item There appears to be a noticeable effect of placebo adminstration/suggestion on electrodermal activity. 
  

\item A model involving both implicit and explicit measures of Optimism and Expectancies mediated by a general expectancy factor appears to provide a much better fit to the data than do alternative models. 
\end{enumerate}


It really needs to be clarified that given the amount of leeway taken with the modeling process in this research that these findings must be regarded as tentative until borne out by future research. 

The contributions of this research are as follows. Firstly, a relationship between priming and the placebo response was demonstrated in a placebo analgesia paradigm. This is important for both experimental and clinical reasons. From an experimental perspective, priming has the advantage that it does not require special equipment or a two or three administration design. This will allow for larger placebo responses to be demonstrated under experimental conditions, which will help the field to progress in its aims. From a clinical perspective, this finding also seems to suggest that simple priming of patients by word games (or potentially cues in their environment) could aid in the healing process. Such a practice would need to be squared with the need to achieve informed consent, but given that the risk of harm from priming seems quite low, this might be acceptable to the general medical community. 

Next, the relationship between implicit and explicit expectancies was assessed. No real significant or robust relationship was found here, but given that the effect sizes were rather small, this does not entirely rule out the possibility of an impact, but it does suggest that any effect would not explain large portions of the variance. 

The use of machine learning models to predict the placebo response threw up some interesting results. It appears from this sample that it is easier for a learning algorithm to predict the lack of placebo response relatively accurately, but predicting a positive response is much harder. This could have occurred in this sample either because either it represents a veridical fact regarding the response to placebo, or because there were more examples of negative placebo response in the sample, and thus with more data, the procedure predicted better. 

Additionally, there was an interesting relationship noted between skin conductance and experimental condition, as well as Gender. While the groups were comparable at baseline, both the Deceptive and Open Placebo groups showed significant changes in GSR after administration of the treatment. Given that the treatment was the same in both cases, this would appear to suggest that skin conductance was affected by the suggestions given as part of the experiment. The fact that these changes were in opposite directions may indicate that either GSR was tracking pain responses, or the effect of certain and uncertain expectancies. This would seem to be a prime area for future research on the placebo. 



%%% Local Variables:
%%% TeX-master: "PlaceboMeasurementByMultipleMethods"
%%% End:
