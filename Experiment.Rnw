
\section{Introduction}

This chapter describes the experimental portion of the research, which involved the assessment of treatment expectancies using implicit and explicit measures. This section further involved the testing of a number of proposed theoretical models around the relationship of explicit, implicit and physiological measures to the placebo response. The justification behind these theoretical models are reported in Chapter \ref{cha:methodology}. 

This chapter proceeds with the following sections:

\begin{enumerate}
\item A look at some of the difficulties with measurement of placebo responses via self-report


\item A look at some broader measurement issues within placebo research more generally

\item A description of the methodology employed in this chapter to test the major hypotheses and mitigate some of the issues described above

\item A description of the major preliminary checks and tests to ensure that the data was as useful as it could be

\item A testing of the major hypotheses around the placebo and its relationship to implicit and explicit treatment expectancies. 

\item  A discussion outlining the major issues raised by this chapter.
\end{enumerate}

\subsection{Detecting the existence of placebo effects}

The definition of placebo effects is without consequence if it is impossible to establish that a placebo effect has or has not occurred in a controlled setting. One of the largest problems with establishing this is when the outcome is a subjective one, such as pain (which is the domain in which the placebo was examined in this thesis). Pain is a subjective experience, and equivalent stimulus levels may cause entirely different reactions  in two different people \cite{Kirsch1997}. 

This issue is often controlled for by calibration of the stimulus levels to the individual participant in an experiment. This allows the researcher to assess to what extent these pain levels are altered by the placebo treatment. Normally, pain intensity and unpleasantness levels are assessed on an eleven point Visual Analogue Scale (VAS)  and these ratings are used as the input data for statistical analysis of different groups. 

However, the use of these self report instruments carries with it a number of problems. Firstly, the issue of demand characteristics arises as a consequence of this. Demand characteristics refer to the subtle pressures faced by participants in experiments to live up to the expectations of the researcher \cite{weber1972subject} and were discussed in Section \ref{sec:plac-rand-trials}. The idea is that if a researcher really wants to demonstrate something, then he or she will communicate this subtly to the participants in the study, and they will (theoretically) respond in the manner in which the researcher prefers. This factor is one of the major reasons why tests of new drugs must blind the experimenter as well as the participants. 

Another large factor which affects placebo analgesia research is the issue of response biases. The problem is that if participants receive a treatment which they believe to be effective, then their thresholds of perception may be altered without any actual biochemical changes.

This is the theory put forward by Allan \& Siegel's \cite{Allan2002} analysis of the placebo phenomenon in terms of signal detection theory. These two factors were proposed to account for the entirety of the placebo effect in pain by Hrobjarrstsson \& Goetzche \cite{hrobjartsson2001}.

This interpretation of placebo seems less likely given that placebo effects have been observed on biochemical parameters, and also  given that placebo analgesia can be removed by the administration of naloxone~\cite{Levine1979,benedetti2003a} (c.f. Chapter \ref{cha:literature-review}, especially the section on physiological effecs of placebo.

\subsection{Measurement Issues}
\label{sec:measurement-issues}

The measurement of placebo has improved greatly over the past fifty years since Beecher started examining the phenomenon. However, it still suffers from a number of problems.

The first problem is the consistent use of ANOVA and regression methods to examine differences between groups \cite{Colloca2008b,Pollo2001}. While there is nothing intrinsically wrong with this method (as long as it is not within person differences which are examined), it is a waste of statistical power. The issue here is that  much placebo analgesia research typically only examines group differences in scores. The problem with this approach is that typically, pain scores are collected over time, and so the responses of one participant at time $t$ are not independent of the responses of this participant at $t+1$. This violates the assumptions of ANOVA (and of linear regression). While a simple binary responder/non responder classification is possible, this approach throws away large amounts of information. One study which did use more appropriate methods was \cite{Bausell2005} who examined the effects of expectancies and acupuncture using survival analysis. 

More appropriate methods for these analyses are time series analyses and survival analysis, both of which are discussed below. Additionally, in trials of functional bowel disorders, placebo effects correlate with the size of the trial, which suggests that regression to the mean effects are having an impact \cite{Enck2005a}. 

The next major problem with the current analytic strategies taken towards placebo is that most of the techniques used (ANOVA, linear models etc) assume that the mean is an appropriate measure of central tendency. However, given that placebo response can be modelled as binary (it either occurs or it does not) then the distribution would be expected to be bi-modal, in which case the mean can be very misleading. In reference to this point, a re-analysis of a number of placebo trials showed vast differences between the mean and median placebo responses \cite{McQuay1996}. 

Additionally, the additive model of drug and placebo interactions may be an issue. This is typically taken as true, which is a position which may have had validity before large scale computing resources were available, but this constraint does not apply any more \cite{Caspi2002}. The issue of additive placebo and drug interactions was discussed further in Chapter \ref{cha:methodology}. 


One final issue with current placebo research relates to expectancies. The first issue is that expectancies are typically measured with a simple one question scale. One of the aims of this thesis was to apply psychometric methods to the devlopment of a more sophisticated measure of treatment credibility and expectancies (see Chapter \ref{cha:tcq-thesis}). The other problem is that expectancies are assummed to be conscious, even though they appear to have far more in common with unconscious responses than they do with controlled processes. The limitations of this approach have been addressed earlier, and will not be revisited here. 




\subsection{Modelling Placebo by Multiple Methods}
\label{sec:modell-plac-mult}

Finally, this section will present the overall approach and rationale for the work carried out in the course of this thesis.

The placebo effect is a complex phenomenon. It has been established that it can be predicted by some variables which are typically measured using self report approaches. These variables include expectancies and optimism. Optimism was measured using the Life Orientation Test, Revised (LOT-R - see Chapter \ref{cha:health-for-thesis} for details on this instrument), and a new measure of expectancies and treatment credibility was developed (Chapter \ref{cha:tcq-thesis}). 

Additionally, these variables only explain a small proportion of the variance in the observed placebo response. It is the contention of this researcher that some of the residual variance can be predicted by the use of implicit measures (specifically the IAT). Therefore, two implicit measures, one of Treatment Credibility and the other of Optimism were developed (see Chapter \ref{cha:devel-impl-meas} for details). 

The relationship between the observed placebo response, and these explicit and implicit measures was of primary importance to this research, and so a measure of Mindfulness (Mindful Attention Awareness Scale) was also used. The rationale behind this choice of measure was explained in Chapters \ref{cha:literature-review} and \ref{cha:health-for-thesis}.

In order to make use of psychometric modelling, large samples of the self report instruments were collected in the same population from which the experimental participants were drawn. This allowed for factor score and IRT models to be built for each of the measures. 

In the experiment itself, (see Chapter \ref{cha:primary-research}) physiological (ECG and GSR) was collected in order to examine the physiological characteristics of placebo and to allow for another form of measurement to be included in the final model.

The collection of these various forms of data, along with a behavioural criterion (the observed placebo response) allowed for psychometric models (Structural equation models) to be examined to seperate out the effects of the various predictor type. In essence, this thesis seeks to marry the strengths of psychology in psychometric modelling to its complementary strengths in experimental design, with the aim of establishing these methods and measures relative usefulness in the prediction of placebo. Much more detailed descrptions of the major models applied are given in Chapter \ref{cha:methodology}. 


\section{Methodology}

\subsection{Experimental Procedure}
<<healthenv, echo=FALSE, results=hide>>=
## load("healthforthesis.rda")
## load("tcq2.rda")
load("homdata.rda")
load("credtotals.rda")
## load("tcqthesis.rda")
@ 

\subsection{Recruitment for the Experimental Procedure}

Following piloting, a random subset of students were emailed to ask if they would like to participate in the experiment. Given that the experiment took place in the Applied Psychology department, somewhat off campus and that the experiment involved suffering painful stimuli, an inducement of a smartphone was offered to one participant who completed the procedure on the basis of a draw following the completion of the research.  After this email was sent and participants recruited, another email was sent to Zoology, Ecology and Plant Science students, along with Psychology students, as all of these students had lectures in the Psychology building. In addition to this, an email was sent to all of the researchers acquaintances on popular social networking sites. Three more emails were sent to random samples from the all students mailing list, and the experiment ran from January 17th until April 14th inclusive.

\subsection{Measures used in the Experiment}
\label{sec:meas-used-exper}

The following measures were used in this experiment. Firstly, Age, Gender and course of study were collected for each participant. The MAAS and LOTR were also administered to each participant, as was a shortened version of the treatment credibility questionnaire (described in Chapter \ref{cha:tcq-thesis}, Section \ref{find-something-to-put-here}). Following the completion of the explicit measures, participants completed a Treatment Credibility IAT and an Optimism IAT. The stimuli used in each of the IAT's were as follows:
\textup{Treatment Credibility IAT:}
Conventional: Creams, Pills, Surgery, Injections
Alternative: Homeopathy, Acupuncture, Reiki, Flower Essence
Real: Real, Accurate, True
Fake: Fake, inaccurate, False
Optimism IAT
Optimism: Optimism, Happy, Improving, Succeeding
Pessimism: Pessimism, Unhappy, Disimproving, Failing
Self: Me, Mine, Myself
Other: You, Yours, Yourselves. 

Further details of the development and piloting of each IAT are in Chapter \ref{cha:devel-impl-meas}. 
In addition, participants gave verbal reports of their pain levels to the experimenter at one minute intervals, and these were recorded (along with condition and exact time of application of bandage and when the squeezing stopped) on a sheet of paper, along with the participants identification number. 


\subsection{Experimental Procedure}

All participants were met at the entrance to the building by the primary researcher. They were given the informed consent documentation, and after they signed it, they completed three questionnaires (the MAAS, the LOT-R and the TCQ). Following this, they completed both an Optimism IAT and the Treatment Credibility IAT, where order of administration was counterbalanced across participants.

Following this, the participants sat down next to the Biopac physiological monitoring data, and baseline data was recorded for five minutes.  Then, a blood pressure gauge was wrapped around the upper part of the non-dominant hand of the participant, and they were asked to squeeze a hand exerciser twenty times for two seconds each time. One minute after this, and every minute thereafter, participants were asked to rate their pain on a VAS from 1 to 10.

If the participant was in the treatment or placebo group, then when they rated their pain as 7 or higher, the placebo cream was applied. The experiment continued until the participant either decided to withdraw, their pain rating reached 10 or 45 minutes elapsed  from when the bandage was applied. ECG and EDA recordings were taken one thousand times per second second using the Biopac equipment and VAS ratings were recorded on paper by the experimenter. The placebo cream consisted of moisturiser in a pharmacuetical container, and was unlabelled.

Participants in the treatment group were told that \textit{the cream was a potent painkiller, recently approved and proven to reduce pain, which would take effect almost immediately}. Participants in the placebo group were told that \textit{they were receiving a placebo and that placebos have been clinically proven to reduce pain, and that it would take effect almost immediately}.

\subsubsection{Analysis of Experimental Data}

The first step in the analysis of experimental data was to examine the comparability of each of the different groups. This procedure was carried out with t-tests (for numerical data) and ANOVA's (for categorical data). There were no differences in pre-treatment levels for any of the variables, and so analytical techniques did not need to include any variables to ensure comparability. This finding demonstrated that the randomisation was successful. 

\paragraph{Analysis of IAT data}

Firstly,  an ex-Gaussian distribution was fit to the data. This distribution is a mixture distribution of a normal and an exponential, and can be used to account for the extremely long tails present in such data. If this distribution proved a reasonable fit to the data, then estimation of confidence intervals and predictions could be made more easily from the data.
%% In addition, as described previously, quantiles were determined for each participant in each Block, and these individual level quantiles were used to derive the group level distributions for each block and condition. 
Another approach was taken to the data in tandem with this. This approach involves ordinal test theory \cite{schulman1975test}. All of the reaction time data occurs on a common scale. The differences between the responses to a word ,$w$ in condition 1 and condition 2 are on the same scale, and a distance matrix can be constructed from this data. This distance matrix can either be ranked (which makes fewer assumptions and is less affected by outliers) or the Euclidean distance can be used in analysis. The advantage of the distance approach is that it discards less information, though at the cost of making more assumptions. A clustering approach was then used, to assess whether or not the stimuli fell into the same categories. This clustering approach was repeated 10 times with different seeds and the results averaged in order to reduce random variability inherent to the k-means approach. $k$ was also varied from 1 to 10, and the results averaged also.

The ordinal/euclidean matrix was then factor analysed and examined using item response theory to assess its psychometric structure, and assess fit or lack thereof.

%% Another approach which was applied to this data was to fit Samejina's continuous response model, which is a generalisation of the Graded Response Model % (described above in Section \ref{sec:polyt-item-resp})
%% to the reaction time data. This procedure aimed to produce ability estimates which were then applied to the prediction of placebo response data. 

\paragraph{Analysis of Pain Rating Data}
\label{sec:analysis-pain-rating}

The pain ratings were modelled as time series, and individual and gender, condition and group level ARIMA models were fitted to the data. In addition, their cross-correlations were examined with the physiological time series collected as part of the placebo analgesia experiment. The model fitting procedure was as follows:


\paragraph{Practical Model Building Strategy}

The following are general steps towards building an ARIMA model.

\begin{itemize}
\item Inspect the plots both of the series and the autocorrelations
\item Examine autocorrelations, differencing if necessary
\item Estimate parameters, ensuring that they are significantly different from zero, and within the bounds of invertibility
\item Examine residuals, if they are not white noise, repeat steps 1-4 until they are.
\end{itemize}

\paragraph{Event History Analysis}
\label{sec:event-hist-analys}
In this research, a particular form of time series analysis was used, which is known as event history analysis \cite{mccleary1980applied}. This type of analysis partitions the time series into two or more parts, based on whether or not a particular event has occurred. In the case of this research, there were either two or three parts to the analysis. In the Deceptive and Open Placebo groups (see Chapter \ref{cha:primary-research}), the first time series occcurred until the painful stimulus was applied, the second was the time from this point until the placebo was applied, and the third was this time point until the end. In the No Treatment group, there were two time series, one for the period before the pain was applied, and one after.

A major advantage of this method is that it allows us to examine changes in the parameters of the time series as a function of experimental stage and condition. This allowed us to estimate more precisely what changes occurred over time as a result of experimental procedure. The basic procedure is as above, except that parameters are estimated on a subset of the data, and cross-correlation functions are used to examine the changes between them.

\paragraph{Analysis of Placebo Response Data}
\label{sec:analys-plac-resp}

The first step was to classify participants as either placebo responders or non-responders. This was done simply by examining if their pain levels decreased following administration of placebo. If this happened, they were classified as placebo responders, and if not, they were classified as non-responders. Another approach taken was to examine the number of minutes spent with a pain rating of less than seven following administration of the placebo, and to model this as a Poisson variable using a generalised linear model. 

Because of the autocorrelation inherent in the pain ratings, general linear models were not entirely appropriate for this data. Therefore, the autocorrelation structure was determined for the pain ratings, and a generalised linear mixed model was fitted to the data using the IAT, explicit and physiological data as predictors. These models were carried out using stepwise, lasso, ridge and least angle regression methods, and validated using ten-fold cross validation. In addition, these models were compared against models using both factor scores and IRT ability estimates to determine the usefulness of this model based approach. 

\paragraph{Analysis of Physiological Data}
\label{sec:analys-phys-data}

The physiological data was collected (as described above) from  two different sources. The first was skin conductance, and the second was electro-cardiogram data. Both of these sources provided recordings at 1000Hz for the entire procedure (and normally a few seconds after the blood pressure gauge). The ECG data was analysed to extract heart rate, heart variability  and QT and RR intervals. In addition, both of these time series were examined using event history analysis (see Section \ref{sec:event-hist-analys}), which modelled the time series in three intervals (before administration of gauge, before administration of placebo, after administration till end) for the Deceptive and Open Placebo Groups and two series (before administration of gauge and afterwards) for the No Treatment group.

Finally, the relationships between physiological responses and the psychological (both implicit and explicit) variables collected, were examined. More details on the hypotheses regarding this can be found in the appropriate Chapter (\ref{cha:primary-research-2}).






\subsubsection{Analysis of Reaction Time Data}

Reaction time data has been studied by (mostly cognitive) psychologists for many years. The Implicit Association Test has been used in almost 300 published papers and reports (and doubtless many more times where the results were not published). However, with a few exceptions, there has been almost no overflow from one area of study to the other.

Classic work in examining the distributions of reaction time data was carried out by Ratcliff\cite{ratcliff1979group}\cite{ratcliff1993methods}. In the 1979 paper he suggested that a quantile based approach should be used for individual reaction time scores. This involves ranking each of the latencies for each individual participant, and using certain percentages of these as quantiles, which can then be used to estimate group distributions. This approach will be utilised in this research, and four quantiles will be used, as given that some conditions (Blocks one, two and four) have only twelve observations, and quartiles divide each of the block sizes (12 and 36) equally. These quartiles were then used to estimate group, block and condition level distributions for the reaction time data. 

The typical approach to analysis of IAT data goes as follows  \cite{Greenwald1998}: firstly, the data is checked for outliers. Outliers, in this case are defined as responses less than 300ms and greater than 3000ms. Any such outliers are recoded to 300 or 3000ms respectively. Following this procedure, a mean is taken of the items in each condition. These means along with standard deviations are reported. The response latencies are then log transformed (to reduce positive skew) and the IAT score is calculated as follows. 

Given the participants mean latency in each condition, their IAT score is the mean for the incompatible condition (i.e. White + Unpleasant) less the mean from the compatible condition (i.e. White + pleasant) divided by the average of the two within group standard deviations. This measure is typically called $D$, and was developed after extensive analysis of an extremely large sample of IAT responses\cite{Greenwald2003a}.In addition to the change of scoring procedure in the 2003 revisions, the threshold for outliers was also substantially widened to 10000 ms. Given that the sample used for this re-analysis was over one hundred thousand, this widening of the threshold was presumably based on experience with a much broader population than was used in many early IAT experiments (typically college students). 

There are a number of problems with this approach. Firstly, the approach throws away much information, more than once. In the first case, information regarding extreme responses is censored, in an unsystematic and theoretically unjustified manner. The breakpoints of 300 and 3000 milliseconds appear to have been chosen to allow for the use of the mean as a group value rather than for any principled reasons. 

It is arguable as to whether or not this censoring represents a good strategy, but certainly it is something which should have been examined in a principled fashion, which does not appear to have happened. The second issue relates again to the calculation of a mean.

While means are useful summary statistics, they are most optimal in situations where the distribution is unimodal and symmetric\cite{venables2002modern}. Reaction latency data are neither, so the choice of mean seems to have been made from familiarity rather than principle. 

The choice of log transform (while slightly more justifiable) is decidedly inferior to a more data driven approach. Again, the issue here is not that such choices in analysis were made, but rather that they have been made once and repeated many times in the later literature. Even those who have criticised the IAT \cite{Klauer2005,Mierke2003,Blanton2006} on methodological grounds appear to have ignored this issue. Possible resolutions of this issue are discussed below in Section \ref{sec:experimental-data-analysis}

Given the typical right skew observed in reaction time distributions, the median would seem to be a much better measure of central location than would the mean. This right skew typically occurs as there is a hard bound on how quickly a participant can respond, but no such bound (unless enforced by the procedure) in the maximum time taken to respond. Therefore, in this research, medians will be used for all reaction time based measures. The differences between the mean and median based approaches will be examined and reported, to determine if it makes any difference. 



\section{Results}
\label{sec:results}



<<importdata, echo=FALSE, results=hide>>=
setwd("./ExperimentDataforR/FullStudy/")
expmeasures <- read.csv("explicitmeasuresfixed.csv")
expmeasures[,"LOTR"] <- with(expmeasures, LOTR/6)
vasscores <- read.csv("VASscores.csv"       )
optiat <- read.csv("optiatres.csv" )
tcqiat <- read.csv("tcqiatres.csv")
setwd("../..")
@ 



<<loadpackages, echo=FALSE, results=hide>>=
require (randomForest)
require (cacheSweave)
require (gridExtra)
require(psych)
require(xtable)
require(arm)
require(ggplot2)
require(reshape2)
require(eRm)
require(ltm)
require(boot)
require(plyr)
require(caret)
require(survival)
source("func.R")
@ 

<<iatsort, echo=FALSE, results=hide>>=
tcqiatsorted <- tcqiat[,c("Participant", "Date", "Time", "Block", "Accurate", "Actual", "Real", "Truth", "Fake", "Illusory", "Inaccurate", "Lies", "Cream", "Injections", "Pills", "Surgery", "Acupuncture", "FlowerEssence", "Homeopathy", "Reiki", "Correct", "BlockTime")]
optiatsorted <- optiat[, c("Participant", "Date", "Time", "Block", "Me", "Mine", "Myself", "Theirs", "Them", "Themselves", "Better", "Happy", "Improving", "Succeeding", "Disimproving", "Failing", "Sad", "Worse", "Correct", "BlockTime")]
@ 

<<optiatscore, echo=FALSE, results=hide>>=
optiatsorted[,"Block"] <- with(optiatsorted, gsub(":", "", x=Block))
optiatscore.mean <- calcIatScores(optiatsorted,Code="Participant", method="mean", words=c("Me", "Mine", "Myself", "Theirs", "Them", "Themselves", "Better", "Happy", "Improving", "Succeeding", "Disimproving", "Failing", "Sad", "Worse"))
names(optiatscore.mean)[1] <- "Participant"
names(optiatscore.mean)[6] <- "OptIAT.Mean"
optstimblock3 <- optiatscore.mean[,grep("Block3.", x=names(optiatscore.mean))]
optstimblock5 <- optiatscore.mean[,grep("Block5.", x=names(optiatscore.mean))]
optiatscore.median <- calcIatScores(optiatsorted,Code="Participant", method="median", words=c("Me", "Mine", "Myself", "Theirs", "Them", "Themselves", "Better", "Happy", "Improving", "Succeeding", "Disimproving", "Failing", "Sad", "Worse"))
names(optiatscore.median)[1] <- "Participant"
names(optiatscore.median)[6] <- "OptIAT.Median"
optiatscore <- merge(optiatscore.mean[c(1,6)], optiatscore.median[,c(1,6)], by="Participant")
part.opt <- unique(optiat[,"Participant"])
part.tcq <- unique(tcqiat[,"Participant"])
optstimblock3 <- as.data.frame(cbind(part.opt, optstimblock3))
optstimblock5 <- as.data.frame(cbind(part.opt, optstimblock5))
@ 

\subsection{Analysis of IAT data}
\label{sec:analysis-iat-data}
The first step in the analysis of IAT data is to examine the differential impact of using the mean versus the median as the measure of central tendency for the calculation of IAT scores (the $D$ measure). The results for the Optimism IAT are shown in Figure \ref{fig:meanmedoptiatplot}. As can be seen from this figure, there were no major changes attributable to this difference (the correlation between the two scores was $r=0.91$). 

\begin{figure}
<<meanmedoptiatplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
meanmedoptiatplot <- ggplot(optiatscore, aes(x=OptIAT.Mean, y=OptIAT.Median))+geom_point()+geom_smooth(method="lm")
print(meanmedoptiatplot)
@   
  \caption{Scatterplot with Linear Regression Smooth of IAT score for Optimism IAT calculated with mean (x) and median (y)}
  \label{fig:meanmedoptiatplot}
\end{figure}



<<optiatboot, echo=FALSE, results=hide, cache=TRUE>>=
optcritblocks <- merge(optstimblock3, optstimblock5, by="part.opt")
Iatcalcopt <- function (data, indices=rep(0.00885, 113)) {
  d <- data[sample(indices, replace=TRUE),]
  b3mean <- apply(d[,2:length(optcritblocks)], 1, mean, na.rm=TRUE)
  b3sd <- apply(d[,2:length(optcritblocks)], 1, sd, na.rm=TRUE)
  b5mean <- apply(d[,2:length(optcritblocks)], 1, mean, na.rm=TRUE)
  b5sd <- apply(d[,2:length(optcritblocks)], 1, sd, na.rm=TRUE)
  ovsd <- (b3sd+b5sd)/2
  Iatscore <- (b5mean-b3mean)/ovsd
}
optboot <- boot(data=optcritblocks, statistic=Iatcalcopt, R=1000)
## optci <- boot.ci(optboot)
## optmeanconf <- as.data.frame(optci[["normal"]])
## names(optmeanconf) <- c("confidence level", "lower quantile", "upper quantile")
@ 



<<optmeanconfprint, echo=FALSE, results=tex>>=
## optmeanconf.xtab <- xtable(optmeanconf, label="tab:optmeanconfprint",
## caption="Summary of Bootstrapped (n=1000) Calculations of IAT Optimism Score")
##  print(optmeanconf.xtab)
@ 

% As can be seen from the Table \ref{tab:optmeanconfprint}, the mean score on the Optimism IAT is relatively unstable, even with quite a large sample. However, as shown in Figure \ref{fig:optbootplot}, the normal approximation used in the bootstrap seems appropriate. 






<<tcqtestblocks, echo=FALSE, results=hide>>=
tcqiatsorted[,"Block"] <- with(tcqiatsorted, gsub(":", "", x=Block))
tcqiatscore.mean <- calcIatScores(tcqiatsorted, Code="Participant", method="mean", words=c("Accurate", "Actual", "Real", "Truth", "Fake", "Illusory", "Inaccurate", "Lies", "Cream", "Injections", "Pills", "Surgery", "Acupuncture", "FlowerEssence", "Homeopathy", "Reiki"))
tcqstimblock3 <- tcqiatscore.mean[,grep("Block3.", x=names(tcqiatscore.mean))]
tcqstimblock5 <- tcqiatscore.mean[,grep("Block5.", x=names(tcqiatscore.mean))]
names(tcqiatscore.mean)[1] <- "Participant"
names(tcqiatscore.mean)[6] <- "TCQIAT.Mean"
tcqiatscore.median <- calcIatScores(tcqiatsorted, Code="Participant", method="median", words=c("Accurate", "Actual", "Real", "Truth", "Fake", "Illusory", "Inaccurate", "Lies", "Cream", "Injections", "Pills", "Surgery", "Acupuncture", "FlowerEssence", "Homeopathy", "Reiki"))
names(tcqiatscore.median)[1] <- "Participant"
names(tcqiatscore.median)[6] <- "TCQIAT.Median"
tcqiatscore <- merge(tcqiatscore.mean[,c(1,6)], tcqiatscore.median[,c(1,6)], by="Participant")
part.tcq <- unique(tcqiat[,"Participant"])
tcqstimblock3 <- as.data.frame(cbind(part.tcq, tcqstimblock3))
tcqstimblock5 <- as.data.frame(cbind(part.tcq, tcqstimblock5))
IATscores <- merge(optiatscore, tcqiatscore, by="Participant")
@

Next, we examine the difference between the mean and median scores for the TCQ IAT. As can be seen from Figure \ref{fig:meanmedtcqiat}, the same pattern as emerged from the Optimism IAT scores is apparent, with little difference between the two measures of central tendency ($r=0.89$). 

\begin{figure}
<<meanmedtcqiat, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
meanmedtcqpl <- ggplot(tcqiatscore, aes(x=TCQIAT.Median, y=TCQIAT.Mean))+geom_point()+geom_smooth(method="lm")
print(meanmedtcqpl)
@   
  \caption{Scatterplot of TCQIAT Median Scores against TCQ IAT mean scores with a linear regression smooth line}
  \label{fig:meanmedtcqiat}
\end{figure}



%%<<tcqiatboot, echo=FALSE, results=hide>>=
## tcqcritblocks <- merge(tcqiatblock3, tcqiatblock5, by="Participant")
## Iatcalctcq <- function (data=tcqcritblocks, indices=rep(0.00885, 113)) {
## d <- data[sample(indices, replace=TRUE),]
## b3mean <- apply(d[5:20], 1, mean, na.rm=TRUE)
## b3sd <- apply(d[5:20], 1, sd, na.rm=TRUE)
## b5mean <- apply(d[26:41], 1, mean, na.rm=TRUE)
## b5sd <- apply(d[,26:41], 1, sd, na.rm=TRUE)
## ovsd <- (b3sd+b5sd)/2
## Iatscore <- (b5mean-b3mean)/ovsd
##  }
## tcqboot <- boot(tcqcritblocks, Iatcalctcq, R=1000)
## tcqci <- boot.ci(tcqboot)
## tcqmeanconf <- as.data.frame(tcqci[["normal"]])
## names(tcqmeanconf) <- c("confidence level", "lower quantile", "upper quantile")
## tcqmeanconf.xtab <- xtable(tcqmeanconf, label="tab:tcqmeanconf", caption="TCQ IAT mean D scores bootstrap confidence estimates")
## tcqbootplot <- plot(tcqboot)
%%@ 

%%<<tcqmeanconf, echo=FALSE, results=tex>>=
## print(tcqmeanconf.xtab)
%%@ 

% As can be seen in Table \ref{tab:tcqmeanconf} the estimated mean IAT score on the treatment credibility IAT is also quite unstable, however as shown below in Figure, the normal approximation seems appropriate. 

%%<<tcqbootplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
## print(tcqbootplot)
%%@ 


%% <<tcqstimboot, echo=FALSE, results=hide>>=
## tcqmeanboot <- function (data, indices=rep(0.00885, 113)) {
##   d <- data[sample(indices, replace=TRUE),]
##  tcqmeans <- apply(d[,5:20], 2, mean, na.rm=TRUE)
## }
## tcqstimboot <- boot(tcqiatsorted, statistic=tcqmeanboot, R=1000)
## tcqstimbootci <- boot.ci(tcqstimboot)
## tcqstimbootplot <- plot(tcqstimboot)
%%@ 
 
The next question is whether or not the IAT\'s have been contaminated by method variance. This can be assessed in a preliminary fashion by examining the correlations between the Treatment Credibility and Optimism IAT. The correlation between the two mean scored IAT measures was ($r=0.003$), while the correlation between the two median scored IAT measures was r=0.08, thus showing that method variance does not appear to have contaminated the results. 


\begin{figure}
<<tcqstimblock3, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
tcqstimblock3.m <- melt(tcqstimblock3, id.vars="part.tcq")
tcqstimblock3.m[,"part.tcq"] <- with(tcqstimblock3.m, as.factor(part.tcq))
print(ggplot(tcqstimblock3.m, aes(x=log(value), colour=as.factor(part.tcq)))+geom_density()+theme(legend.position="none"))
@  
  \caption{TCQ IAT Stimuli Response Times in Block 3. Each density plot represents the distribution of each participants' response times. Plot is on log scale }
  \label{fig:tcqstimblock3}
\end{figure}

\begin{figure}
<<tcqstimblock3plot, echo=FALSE, fig=TRUE, eps=TRUE, pdf=TRUE, png=TRUE>>=
tcqstimblock3.real <- tcqstimblock3[,2:9]
tcqstimreal3.m <- melt(tcqstimblock3.real)
tcqstimblock5.real <- tcqstimblock5[,2:9]
tcqstimreal5.m <- melt(tcqstimblock5.real)
tcqstimreal3.m[,"variable"] <- with(tcqstimreal3.m, gsub("Block3\\.","",x=variable))
tcqstimreal5.m[,"variable"] <- with(tcqstimreal5.m, gsub("Block5\\.","",x=variable))
names(tcqstimreal3.m)[1] <- "Block3"
names(tcqstimreal5.m)[1] <- "Block5"

stimreal3.pl <- ggplot(tcqstimreal3.m, aes(x=Block3, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
stimreal5.pl <- ggplot(tcqstimreal5.m, aes(x=Block5, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
print(arrangeGrob(stimreal3.pl, stimreal5.pl))
@   
  \caption{Real versus Fake Stimuli, TCQ IAT. Boxplots of response times to each stimuli, Block 3 (top) and Block 5 (bottom)}
  \label{fig:tcqstimrealpl}
\end{figure}

As can be seen from Figure \ref{fig:tcqstimrealpl}, the majority of items were responded to relatively quickly in both categories. Of note, however, are the outliers which were words which were associated with fake treatments in Block 3 (where they were paired with conventional treatments) and words which were associated with real treatments (where they were paired with alternative treatments). This would seem to suggest that the words were in fact serving their intended purpose. 


As can be seen from Figure \ref{fig:tcqstimblock3} above, the distribution of participant response times was clearly not normal, being far too skewed to the right and heavy tailed. The log transformation helps matters, but the tails are still extremely long (a point further reinforced by the outliers seen in Figure \ref{fig:tcqstimrealpl} also). 
\begin{figure}
<<tcqstimconvaltpl, echo=FALSE, fig=TRUE, eps=TRUE, png=TRUE, pdf=TRUE>>=
tcqstim3convalt <- tcqstimblock3[,10:17]
tcqstim5convalt <- tcqstimblock5[,10:17]
tcqstim3conv.m <- melt(tcqstim3convalt)
tcqstim5conv.m <- melt(tcqstim5convalt)
tcqstim3conv.m[,"variable"] <- with(tcqstim3conv.m, gsub("Block3\\.", "", x=variable))
tcqstim5conv.m[,"variable"] <- with(tcqstim5conv.m, gsub("Block5\\.", "", x=variable))
names(tcqstim3conv.m)[1] <- "Block3"
names(tcqstim5conv.m)[1] <- "Block5"
tcqstimconv3pl <- ggplot(tcqstim3conv.m, aes(x=Block3, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
tcqstimconv5pl <- ggplot(tcqstim5conv.m, aes(x=Block5, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
print(arrangeGrob(tcqstimconv3pl, tcqstimconv5pl))
@   
  \caption{Boxplots for Conventional and Alternative Stimuli, TCQ IAT, Block 3 (top) Block 5 (bottom)}
  \label{fig:stimblockconvalt}
\end{figure}


As can be seen from Figure \ref{fig:stimblockconvalt} a similar pattern emerges from the conventional and alternative stimuli. Interestingly, it appears that response times were slower overall in Block 5, which may represent fatigue. However, the order of IAT's was counterbalanced, so one would expect to see the same pattern in the Optimism IAT's if this was the case. 

\begin{figure}
<<optstimblock3, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
optstimblock3.m <- melt(optstimblock3, id.vars="part.opt")
optstimblock3.m[,"part.opt"] <- with(optstimblock3.m, as.factor(part.opt))
names (optstimblock3.m) [3] <- "ResponseTime"
partstim3opt <- ggplot(optstimblock3.m, aes(x=log(ResponseTime), colour=as.factor(part.opt)))+geom_density()+theme(legend.position="none")
optstimblock5.m <- melt(optstimblock5, id.vars="part.opt")
optstimblock5.m[,"part.opt"] <- with(optstimblock5.m, as.factor(part.opt))
names (optstimblock5.m) [3] <- "ResponseTime"
partstim5opt <- ggplot(optstimblock5.m, aes(x=log(ResponseTime), colour=as.factor(part.opt)))+geom_density()+theme(legend.position="none")
print (arrangeGrob (partstim3opt, partstim5opt))
@   
  \caption{Density Plots for Response Times of Each Participant, Block 3 (top) and Block 5 (bottom) All response times are plotted on a log scale}
  \label{fig:optstimblock3part}
\end{figure}

As can be seen from Figure \ref{fig:optstimblock3part}, the distributions for the optimism IAT were relatively similar, except that they were overall shifted towards the right, indicating that response times were generally slower to each of these words. Note that one participant has almost the entirity of their distribution beyond the tails of most of the other participants, which given that this is a log-scale plot, indicates that something is very wrong here. Note additionally that the Block 5 results are much more variable than those from Block 3, which again may represent fatigue. 
\begin{figure}
<<optstimposneg, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
optstimblock3.real <- optstimblock3[,2:8]
optstimreal3.m <- melt(optstimblock3.real)
optstimblock5.real <- optstimblock5[,2:8]
optstimreal5.m <- melt(optstimblock5.real)
optstimreal3.m[,"variable"] <- with(optstimreal3.m, gsub("Block3\\.","",x=variable))
optstimreal5.m[,"variable"] <- with(optstimreal5.m, gsub("Block5\\.","",x=variable))
names(optstimreal3.m)[1] <- "Block3"
names(optstimreal5.m)[1] <- "Block5"

stimreal3.pl <- ggplot(optstimreal3.m, aes(x=Block3, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
stimreal5.pl <- ggplot(optstimreal5.m, aes(x=Block5, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
print(arrangeGrob(stimreal3.pl, stimreal5.pl))
@   
  \caption{Boxplots for Self/Other words in Optimism IAT, Block 3 (top) and Block 5 (bottom).}
  \label{fig:optstimmeyou}
\end{figure}

As can be seen from Figure \ref{fig:optstimmeyou}, the same pattern emerged for the optimism IAT in that the Block 5 scores were much more variable and overall participants responded slower to this block. 



<<optiatplotself, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
optstimblock3.posneg <- optstimblock3[,9:15]
optstimpos3.m <- melt(optstimblock3.posneg)
optstimblock5.posneg <- optstimblock5[,9:15]
optstimpos5.m <- melt(optstimblock5.posneg)
optstimpos3.m[,"variable"] <- with(optstimpos3.m, gsub("Block3\\.","",x=variable))
optstimpos5.m[,"variable"] <- with(optstimpos5.m, gsub("Block5\\.","",x=variable))
names(optstimpos3.m)[1] <- "Block3"
names(optstimpos5.m)[1] <- "Block5"

stimpos3.pl <- ggplot(optstimpos3.m, aes(x=Block3, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
stimpos5.pl <- ggplot(optstimpos5.m, aes(x=Block5, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
print(arrangeGrob(stimpos3.pl, stimpos5.pl))
@   
 \caption{Boxplots of Response Times to Positive and Negative Words, Optimism IAT Block 3 (top), Block 5 (bottom).}
 \label{fig:optiatplotpos}
\end{figure}



 As can be seen in Figure \ref{fig:optiatplotself}, the pattern of Block 5 responses tending to be slower was repeated. Note that disimproving appears to be the word with the highest mean latency, which is not surprising given its relatively unfamiliarity (compared to the other words, at least). 
 
 Next, the correlation between the different block times is assessed. As discussed in Chapter \ref{cha:literature-review}, some correlation would be expected given the nature of the IAT task, but the aim here is to quantify what effect, if any it would have had on the results. 
 
 \begin{figure}
<<tcqmeanresp, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
tcqiat.mean.resp <- ddply(tcqiatsorted, .(Block, Participant), summarise, Correlations=mean(BlockTime, na.rm=TRUE))
## tcqmeanresp.pl <- ggplot(tcqiat.mean.resp, aes(x=MeanResponseTime))+geom_histogram+facet_grid(.~Block)
## print(tcqmeanresp.pl)
tcqiat.mean.m <- melt(tcqiat.mean.resp, id.vars=c("Participant", "Block"))
tcq.iat.c <- dcast(tcqiat.mean.m, Participant+...~Block)
tcq.block.corr.pl <- plotmatrix(tcq.iat.c[,3:length(tcq.iat.c)])+geom_smooth(method="lm")
print(tcq.block.corr.pl)
@    
   \caption{Correlations between Block Scores for Treatment Credibility IAT with linear regression smooth line}
   \label{fig:tcqblockcorr}
 \end{figure}

As can be seen from Figure \ref{fig:tcqblockcorr}, the correlations are relatively low between most of the blocks, though somewhat higher between blocks 3 and 5. Table \ref{tab:tcqcormat} gives the exact Kendall\'s $\tau$ between each of the blocks. As can be seen the correlations hover between 0.3 and 0.4, which is in line with expectations prior to the experiment. 

<<cormatrixtcqiat, echo=FALSE, results=tex>>=
cormat <- corr.test(tcq.iat.c[,3:length(tcq.iat.c)], method="kendall")[["r"]]
print(xtable(cormat, label="tab:tcqcormat", caption="Correlations between the blocks of the treatment credibility IAT (Kendalls $\tau$. All correlations are significant at the p<0.001 level"))
@ 
 

Next, the same process is repeated for the Optimism IAT. 

<<tcqmeanresp, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
optiat.mean.resp <- ddply(optiatsorted, .(Block, Participant), summarise, Correlations=mean(BlockTime, na.rm=TRUE))
## optmeanresp.pl <- ggplot(optiat.mean.resp, aes(x=MeanResponseTime))+geom_histogram+facet_grid(.~Block)
## print(optmeanresp.pl)
optiat.mean.m <- melt(optiat.mean.resp, id.vars=c("Participant", "Block"))
opt.iat.c <- dcast(optiat.mean.m, Participant+...~Block)
opt.block.corr.pl <- plotmatrix(opt.iat.c[,3:length(opt.iat.c)])+geom_smooth(method="lm")
print(opt.block.corr.pl)
@    
   \caption{Correlations between Block Scores for Optimism IAT with linear regression smooth line}
   \label{fig:optblockcorr}
 \end{figure}

As shown in Figure \ref{fig:optblockcorr}, the correlations between blocks are moderate, though highest in blocks 3 and 5, as was seen for the Treatment Credibility IAT. Table \ref{tab:optcormat}. The correlations are a little higher than for the Treatment Credibility IAT, but still within an acceptable range. Its interesting to note that (with the exception of Block 5), the correlations are strongest between adjacent blocks, and drop off as the blocks move further apart, suggesting that the autocorrelation theory has some merit. 

<<cormatrixtcqiat, echo=FALSE, results=tex>>=
opt.cormat <- corr.test(opt.iat.c[,3:length(tcq.iat.c)], method="kendall")[["r"]]
print(xtable(opt.cormat, label="tab:optcormat", caption="Correlations between the blocks of the Optimism IAT (Kendalls $\tau$. All correlations are significant at the p<  0.001 level"))
@ 

The next question with regard to the IAT's is whether or not the non-critical blocks (that is, Blocks 1, 2 and 4) will be correlated. Given that these were administered in counterbalanced order and there was a small gap between them one would expect there to be much lower correlations between these blocks of the IAT's. These correlations (if present) should provide an index of general processing speed, and may be useful as predictor variables for some of the other measures. 

<<opttcqiatcorr, echo=FALSE, results=hide>>=
curnames <- names(opt.iat.c)
curnames.bl <- curnames[3:length(curnames)]
curnames.bl2 <- paste("Opt", curnames.bl, sep="")
curnames.d <- c(curnames[1:2], curnames.bl2)
names(opt.iat.c) <- curnames.d
curnames.tcq <- names(tcq.iat.c)
curnames.bl.tcq <- curnames[3:length(curnames)]
curnames.bl2.tcq <- paste("TCQ", curnames.bl.tcq, sep="")
curnames.d.tcq <- c(curnames.tcq[1:2], curnames.bl2.tcq)
names(tcq.iat.c) <- curnames.d.tcq
iat.block.merge <- merge(tcq.iat.c, opt.iat.c, by="Participant")
iat.block.merge2 <- iat.block.merge[, -1*c(2, 5,7,8,11,13)]
@ 

\begin{figure}
<<corriattcqopt, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
corr.iat.tcq.opt.pl <- plotmatrix(iat.block.merge2[,2:length(iat.block.merge2)])+geom_smooth(method="lm")+geom_smooth(method="lm")
print(corr.iat.tcq.opt.pl)
@   
  \caption{Correlations between Non Critical Blocks of Optimism and Treatment Credibility IAT}
  \label{fig:corriattcqopt}
\end{figure}

As can be seen from Figure \ref{fig:corriattcqopt}, there were correlations between the two IAT's. These correlations, while significant, were quite low ($r\bar =0.20$) which equates to about 4\% of the variance. Therefore the two implicit measures can be safely be regarded as not being contaminated by method variance . 



<<selfreportcleanup, echo=FALSE, results=hide>>=
names(vasscores)[1] <-  "Participant"
names(expmeasures)[1] <- "Participant"
vasscores.test <- vasscores[,1:8]
expmeasurescomp <- merge(vasscores.test, expmeasures)
Iatandexpmeasures <- merge(expmeasurescomp, IATscores, by="Participant")
iatexp <- Iatandexpmeasures[,14:23]
Iatandexpmeasures[,"meanconv"] <- with(Iatandexpmeasures, (Pill+Cream+Inj)/3)
Iatandexpmeasures[,"meanalt"] <- with(Iatandexpmeasures, (Acu+Hom+Rei)/3)
Iatandexpmeasures[,"convaltcomp"] <- with(Iatandexpmeasures, meanconv -meanalt)
## Iatandexpmeasures[,"Date"] <- with(Iatandexpmeasures, dmy(Date))
@
 
Next, the relationship between overall response time in each block (total time to complete the block, including interstimulus intervals) was examined in terms of the demographic variables. 

<<optiatdemo, echo=FALSE, results=tex>>=
optiatdemographics <- merge(expmeasures, optiatsorted, by="Participant")
opt.demo <- xtable(summary(lm(BlockTime~Block+Age+Gender, data=optiatdemographics)), label="tab:optblocktimedemo", caption="Summary of Linear Regression of Block Time by Block, Age and Gender")
print(opt.demo)
@  
As can be seen from Table \ref{tab:optblocktimedemo}, the major influence on Block Time comes from Block, which is as expected given that Blocks 3 and 5 had three times as many trials as the other blocks. However, there is also an effect of gender, with males tending to respond somewhat quicker than females. This is interesting, as there are typically no gender based effects on IAT's (except for those which measure gender attitudes). 

<<tcqiatdemo, echo=FALSE, results=tex>>=
tcqiatdemographics <- merge(expmeasures, tcqiatsorted, by="Participant")
tcq.demo <- xtable(summary(lm(BlockTime~Block+Age+Gender, data=optiatdemographics)), caption="Summary of Linear Regression on Age, Gender and Individual Block Times.", label="tab:tcqblocktimedemo" )
print(tcq.demo)
@ 

\begin{figure}
<<tcqiatgender, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
tcqiatgender <- ggplot(na.omit(Iatandexpmeasures), aes(x=Gender, y=TCQIAT.Mean))+geom_boxplot()
print(tcqiatgender)
@   
  \caption{Treatment Credibility IAT Scores by Gender}
  \label{fig:tcqiatgend}
\end{figure}



As can be seen from Figure \ref{fig:tcqiatgend} above, there were no significant differences ($t=-0.4973, p=0.6211$) between men and women in  the sample with regards to their scores on the Treatment Credibility Questionnaire. However, the variance was much higher for men, which was a pattern replicated in previous research into Treatment Credibility (using a self report instrument described in Chapter \ref{cha:tcq-thesis}).

\begin{figure}
<<optiatgender, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
optiatgender <- ggplot(na.omit(Iatandexpmeasures), aes(x=Gender, y=OptIAT.Mean))+geom_boxplot()
print(optiatgender)
@   
  \caption{Optimism IAT by Gender}
  \label{fig:optiatgend}
\end{figure}

 As can be seen from Figure \ref{fig:optiatgend}, there were no significant differences ($t=-0.8234, df=49.761, p=0.4142$) between men and women with regards to their score on the Optimism IAT. 

<<ordtestiatprep, echo=FALSE, results=hide>>=
part.opt <- unique(optiat[,"Participant"])
part.tcq <- unique(tcqiat[,"Participant"])
optblock3 <-  optstimblock3 
optblock5 <- optstimblock5
tcqblock3 <- tcqstimblock3
tcqblock5 <- tcqstimblock5
optiat.diff <- iatDiff(optblock3, optblock5)
tcqiat.diff <- iatDiff(tcqblock3, tcqblock5)

optdiffs.dich <- apply(optiat.diff, c(1,2), function (x) ifelse(x>0, 1, 0))
tcqdiffs.dich <- apply(tcqiat.diff, c(1,2), function (x) ifelse(x>0, 1, 0))
@ 
<<optiatirt, echo=FALSE, results=hide, cache=TRUE>>=
optiat.rasch <- RM(na.omit(optdiffs.dich))
optiat.ppar <- person.parameter(optiat.rasch)
optiat.elim <- stepwiseIt(optiat.rasch)
@ 

\begin{figure}
<<optpimap, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=

print(plotPImap(optiat.rasch))
@   
  \caption{Person Item Map of Difficulty parameters for Optimism IAT}
  \label{fig:optpimap}
\end{figure}


<<optiatrachprint, echo=FALSE, results=tex>>=
optrasch.df <- with(optiat.rasch, as.data.frame(cbind(etapar, se.eta, betapar, se.beta)))
print(xtable(optrasch.df, label="tab:optrasch", caption="Ability Estimates for Rasch Model Analysis of Optimism IAT"))
@ 

Table \ref{tab:optrasch} shows the estimated parameters for a Rasch model of the optimism IAT stimuli. Figure \ref{fig:optpimap} shows the Person Item Map for this model. This map shows that most of the stimuli were equivalent in difficulty, which indicates that this IAT is suitable for measuring implicit optimism in the general population, however, more discriminating stimuli would be necessary if the instrument was to be used in a clinical sample. Additionally, the map shows that the majority of participants showed latent traits of less than zero, suggesting that implicit optimism is rarer than explicit optimism. 


A process of stepwise elimination was carried out to eliminate items which did not fit the model. In this case, the stimulus ``Myself'' was the only one which had significant model misfit. 

<<tcqiatirt, echo=FALSE, results=hide, cache=TRUE>>=
tcqiat.rasch <- RM(tcqdiffs.dich)

tcqiat.elim <- stepwiseIt(tcqiat.rasch)
tcqiat.ppar <- person.parameter(tcqiat.rasch)
@ 

<<tcqiatability, echo=FALSE, results=tex>>=
tcqrasch.df <- with(tcqiat.rasch, as.data.frame(cbind(etapar, se.eta, betapar, se.beta)))
print(xtable(tcqrasch.df, label="tab:tcqiatrasch", caption="Ability Estimates for Rasch Model of Treatment Credibility IAT"))
@ 

\begin{figure}
<<tcqiatpiplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
sink("tmp.txt")
print(plotPImap(tcqiat.rasch))
sink(NULL)
## print(tcqiat.piplot)
@   
  \caption{Person Item Difficulty Map, Treatment Credibility IAT}
  \label{fig:tcqpimap}
\end{figure}

Table \ref{tab:tcqiatrasch}  shows the estimated abilities and their associated standard errors for the Treatment Credibility IAT. Figure \ref{fig:tcqpimap} the estimates of person and item abilities can be seen in graphical form. Again, it can be seen from Figure \ref{fig:tcqpimap} that a majority of the sample had latent traits of less than zero, but that the item estimates of difficulty were approximately equal.


\subsection{Implicit-Explicit Relationships}
\label{sec:impl-expl-relat}

Next, the relationships between the explicit and implicit measures were examined. 

<<corrtestimplexpl, echo=FALSE, results=tex>>=
corr.imp.exp <- corr.test(Iatandexpmeasures[,14:25], method="kendall")[["r"]]
print(xtable(corr.imp.exp, label="tab:corrimpexp", caption="Correlations between Implicit and Explicit Measures"), scalebox=0.7)
@ 

It can be seen from Table \ref{tab:corrimpexp} that the LOTR was only really correlated with the Acupuncture items and with the MAAS, the Conventional Treatment scales correlated within themselves, as did the Alternative treatment scales, while the two IAT measures showed no appreciable correlations with each other. The relationships between the IAT's and explicit measures were small, and surprisingly in the unpredicted direction (negative). Another surprise was that the direction of the correlation between the LOT-R and the MAAS was opposite to that observed in prior research. Possible reasons for these results are considered in the Discussion.  


\subsection{Explicit Measures}
\label{sec:explicit-measures}


\begin{figure}
<<lotrmaas, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
lotrmaas <- ggplot(Iatandexpmeasures, aes(x=LOTR, y=MAAS))+geom_point()+geom_smooth(method="lm")+facet_grid(.~Condition)
print(lotrmaas)
@   
  \caption{Scatterplot of LOT-R Scores by Condition, with a linear regression smooth}
  \label{fig:lotrmaas}
\end{figure}


The plot above in Figure \ref{fig:lotrmaas} shows that optimism and mindfulness were positively correlated with one another. Additionally this correlation appeared to be relatively stable across condition, though it appeared a little weaker in the Deceptive Placebo Group.  This is in contrast to the results found in a much larger scale study carried out earlier in the research. Note that one plausible explanation for this effect is that, in the experiment, the measures were administered in the opposite order - Optimism, followed by Mindfulness. It is possible that the completion of the mindfulness measure affected the way in which participants approached the Optimism measure. This theory is more fully discussed in Chapter \ref{cha:general-discussion}. 


\subsection{Relationships between Experimental Samples and Survey Samples}
\label{sec:relat-betw-exper}

Given the focus of this thesis on the integration of survey and experimental research, the next step was to examine the differences and similarities between the samples collected from the general population via survey and the experimental sample.


\begin{figure}
<<surveyoptimism, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
survey.opt.pl <- ggplot(hom.data, aes(x=optimism))+geom_density()
exp.opt.pl <- ggplot(expmeasures, aes(x=LOTR))+geom_density()

print(arrangeGrob ( survey.opt.pl, exp.opt.pl))
@   
  \caption{Density Plot of Optimism Scores in the Survey samples (top), and the experimental sample (bottom) }
  \label{fig:compoptimism}
\end{figure}


In Figure \ref{fig:compoptimism}  can be seen that the two plots are extremely different, with a much higher average optimism score in the experimental sample. To some extent, this is not unexpected given that the study was described as an investigation of painkilling drugs and there was an opportunity to win a smart-phone, so perhaps students with higher levels of optimism were more likely to agree to participate. 



Next, the differences in mindfulness levels between the two samples were assessed. 

\begin{figure} 
<<surveymindfulness, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
surv.mind.pl <- ggplot(hom.data, aes(x=mindfulness))+geom_density()
exp.mind.pl <- ggplot(Iatandexpmeasures, aes(x=MAAS))+geom_density()
print(arrangeGrob ( surv.mind.pl, exp.mind.pl))
@  
  \caption{Density Plot for Mindfulness Scores, Survey Sample (top), and Experimental Sample (bottom)}
  \label{fig:compmind}
\end{figure}

As can be seen from Figure \ref{fig:compmind}, the pattern was quite different for mindfulness levels (as measured by the MAAS) as the levels of mindfulness were higheer in the survey sample. Again, this may be due to the association of mindfulness with introversion, as introverts may have been less likely to respond to the email invitiation(s) to take part in the study. 


Finally, the treatment credibility questionnaire scores were examined to assess the differences between the survey and experimental samples. 

First, the differences between the two samples in terms of Pill credibility were examined. 

\begin{figure}
<<surveypill, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
surv.pill.pl <- ggplot(credtotals, aes(x=Pilltot))+geom_density()
exp.pill.pl <- ggplot(Iatandexpmeasures, aes(x=Pill))+geom_density()
print(arrangeGrob ( surv.pill.pl, exp.pill.pl))
@   
  \caption{Pill Credibility Density Plot, Survey Sample (sample 2) (top), and Experimental Sample (bottom)}
  \label{fig:comppill}
\end{figure}


As can be seen from Figure \ref{fig:comppill}, the general population sample was higher peaked, with less variation around the peak than was the experimental sample. In fact, the experimental sample seemed to be more variable than the survey sample, which could either be due to a true difference in the distributions or due to a greater uncertainty in the experimental sample due to the smaller sample size. 

Next, the difference between Cream Credibility scores were assessed. 

\begin{figure}
<<surveycream, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
surv.cream.pl <- ggplot(credtotals, aes(x=Creamtot))+geom_density()
exp.cream.pl <- ggplot(Iatandexpmeasures, aes(x=Cream))+geom_density()
print(arrangeGrob (surv.cream.pl, exp.cream.pl))
@   
  \caption{Density Plot for Distribution of Cream Credibility Scores, Survey Sample (sample 2) (top) and Experimental Cream Credibility Totals (bottom)}
  \label{fig:compcream}
\end{figure}



As shown in Figure \ref{fig:compcream}, the survey group tended to have a more positive view of painkilling creams. While the survey group is strongly peaked at the right of the plot, the experimental group were more evenly distributed, with a peak at the centre of the plot. This is interesting, as one might expect the experimental group to be more positive towards painkilling treatments in general, given that they had agreed to take part in  a study which examined the effects of a new analgesic.

Next, the credibility scores for injection painkilling treatments were examined between the survey and experimental groups. 

\begin{figure}
<<surveyinj, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
surv.inj.pl <- ggplot(credtotals, aes(x=Injtot))+geom_density()
exp.inj.pl <- ggplot(Iatandexpmeasures, aes(x=Inj))+geom_density()
print(arrangeGrob ( surv.inj.pl, exp.inj.pl))
@   
  \caption{Injection Credibility Density Plot, Survey Sample (sample two) (top), Experimental Sample (bottom)}
  \label{fig:compinj}
\end{figure}


Figure \ref{fig:compinj}  show that the Injection credibility scores were almost identical in their distributions between the two samples. 

Next, the Alternative treatment scores were examined between the two samples.
\begin{figure}
  
<<alttreatmentssurvey, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
surv.acu.pl <- ggplot(credtotals, aes(x=Acutot))+geom_density()
exp.acu.pl <- ggplot(Iatandexpmeasures, aes(x=Acu))+geom_density()
print(arrangeGrob ( surv.acu.pl, exp.acu.pl))
@   \caption{Density Plots for Acupuncture Scores, Survey sample (top) and Experimental Sample (bottom)}
  \label{fig:compacu}
\end{figure}


Figure ref{fig:compacu} shows that Acupuncture levels in the experimental sample were a little lower than those in the survey sample. 

\begin{figure}
  
<<alttreatmentssurvey, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
surv.hom.pl <- ggplot(credtotals, aes(x=Homtot))+geom_density()
exp.hom.pl <- ggplot(Iatandexpmeasures, aes(x=Hom))+geom_density()
print(arrangeGrob ( surv.hom.pl, exp.hom.pl))
@   \caption{Density Plots for Hompuncture Scores, Survey sample (top) and Experimental Sample (bottom)}
  \label{fig:comphom}
\end{figure}

Figure \ref{fig:comphom} shows the levels of homeopathy credibility in both survey and experimental samples. It can be seen that, contrary to the Acupuncture totals, the credibility scores for Homeopathy were slightly higher in the experimental sample than in the survey sample. 

\begin{figure}
  
<<alttreatmentssurvey, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
surv.rei.pl <- ggplot(credtotals, aes(x=Reitot))+geom_density()
exp.rei.pl <- ggplot(Iatandexpmeasures, aes(x=Rei))+geom_density()
print(arrangeGrob ( surv.rei.pl, exp.rei.pl))
@   \caption{Density Plots for Reipuncture Scores, Survey sample (top) and Experimental Sample (bottom)}
  \label{fig:comprei}
\end{figure}

Figure \ref{fig:comprei} shows the credibility totals for Reiki in the survey and experimental samples. While credibility totals were quite low in both samples, they were a little lower in the survey sample. 


\subsection{Randomisation Checks}
\label{sec:randomisation-checks}

Next, the comparability of the groups were assessed to ensure that the randomisation process had proved effective. 

<<randcheck1, echo=FALSE, results=tex>>=
tcq.xtab <- xtable(summary(aov(TCQIAT.Mean~Condition, data=Iatandexpmeasures)), label="tab:tcqiatcheck", caption="Summary of Anova of Treatment Credibility Scores by Condition")
opt.xtab <- xtable(summary(aov(OptIAT.Mean~Condition, data=Iatandexpmeasures)), label="tab:optiatcheck", caption="Summary of ANOVA of Optimism IAT scores by Condition")
conv.xtab <- xtable(summary(aov(meanconv~Condition, data=Iatandexpmeasures)))
alt.xtab <- xtable(summary(aov(meanalt~Condition, data=Iatandexpmeasures)))
chisq.gend <- with(Iatandexpmeasures, chisq.test(table(Gender, Condition)))
chisq.gend <- with(Iatandexpmeasures, chisq.test(table(Prime, Condition)))
@ 

<<tcqcheckprint, echo=FALSE, results=tex>>=
print(tcq.xtab)
@ 

<<optcheckprint, echo=FALSE, results=tex>>=
print(opt.xtab)
@ 

As can be seen from Table \ref{tab:tcqiatcheck} and Table \ref{tab:optiatcheck} the scores on the treatment credibility IAT or the optimism IAT did not differ by Condition. 

The results of a chi-square test showed that the gender of participants in each condition were equivalent, (p=\Sexpr{round ( chisq.gend[["p.value"]], 3)}.
In) addition, the priming manipulation was not significantly different across groups.



<<chisqcondition, echo=FALSE, results=tex>>=
Subsetiatandexp <- Iatandexpmeasures[with(Iatandexpmeasures,Condition!="No Treatment"),]
Subsetiatandexp <- droplevels(Subsetiatandexp)
chisq2 <- with(Subsetiatandexp, chisq.test(table(PlacResp, Condition)))
chisqtest2 <- as.data.frame(cbind(chisq2[["statistic"]], chisq2[["parameter"]], chisq2[["p.value"]], chisq2[["observed"]]))
names(chisqtest2)[1] <- "Chi Square"
names(chisqtest2)[2] <- "df"
names(chisqtest2)[3] <- "p Value"
chisq2.xtab <- xtable(chisqtest2)
print(chisq2.xtab)
@ 

As can be seen, there is no significant effect of condition on placebo response. However, as can also be seen from the table, this is not due to a lower than expected placebo response in the Deceptive Placebo group, but rather due to a higher than expected placebo response in the Open Placebo group. The next step was to examine if this result could be accounted for by the priming mechanism.

<<chisqtestprimeplac, echo=FALSE, results=tex>>=
chisq1 <- with(Iatandexpmeasures, chisq.test(table(Prime, Condition, PlacResp)))
chisqtest1 <- as.data.frame(cbind(chisq1[["statistic"]], chisq1[["parameter"]], chisq1[["p.value"]], chisq1[["observed"]]))
names(chisqtest1)[1] <- "Chi Square"
names(chisqtest1)[2] <- "df"
names(chisqtest1)[3] <- "p Value"
chisq1.xtab <- xtable(chisqtest1, label="tab:primeplacchi", caption="Chi Square for Relationship between Priming, Condition and Placebo Response")
print(chisq1.xtab)
@ 

In Table \ref{tab:primeplacchi} above, it can be seen that priming appears to be the driver of this effect, as the Placebo Response by Condition is extremely significant given the priming manipulation. Most of this improvement occurred in the Open Placebo group, which is an extremely interesting finding, one not reported in the literature before.  

This finding is much more clearly conveyed in Figure \ref{fig:placprimeplot}, where it can be seen that Participants were much more likely to respond to placebo following a priming intervention. Given that priming interventions typically take place outside conscious awareness, this suggests that there is at least some part of the placebo response which is amenable to non-conscious (or implicit) influences. 

\begin{figure}
<<placprimeplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
plac.prime.pl <- ggplot(Iatandexpmeasures, aes(x=PlacResp))+geom_histogram()+facet_grid(.~Prime)
print(plac.prime.pl)
@   
  \caption{Proportion of Placebo Response in Primed and Non-Primed Conditions}
  \label{fig:placprimeplot}
\end{figure}

\begin{figure}
<<pairsplot, echo=FALSE, Fig=TRUE>>=
print(plotmatrix(iatexp)+geom_smooth(method="lm")+geom_jitter())
@   
  \caption{Scatterplots and Density Plots for Self Report and Implicit Measures}
  \label{fig:pairsplotimpexp}
\end{figure}

In Figure \ref{fig:pairsplotimpexp} the relationships between the self report and implicit measures are shown. It can be seen that the credibility scores break into conventional and alternative groups, while the LOT-R and MAAS do not correlate hugely with any of the other measures (but do correlate quite well with themselves, as noted above).  


\subsection{Pain Ratings}
\label{sec:pain-ratings}

<<survivaldata, echo=FALSE, results=hide>>=
painratings <- vasscores[,c(9:53)]
napainratings <- apply(painratings,1, function (x)  sum(is.na(x)))
lengthsurv <- 46-napainratings
Iatandexpmeasures[,"lengthsurv"] <- lengthsurv
censor <- Iatandexpmeasures[,"Censored"]
censor2 <- ifelse(censor==c("No", "Left"), 1, 0)
Surv.data <- Surv(lengthsurv,censor2)
@ 




<<paints, echo=FALSE, results=hide>>=
painratings <- vasscores[,c(1,2,8:53)]
painratings.temp <- painratings[,2:length(painratings)]
painratings.trans <- t(painratings.temp)
colnames(painratings.trans) <- as.character(t(painratings[,1]))
painratings.trans <- as.data.frame(painratings.trans)
painratings.trans[,"Time"] <- 1:nrow(painratings.trans)
pain.cond <- ddply(painratings, .(Condition), summarise, PainRatings=apply(painratings[,4:48], 2, mean, na.rm=TRUE))
pain.cond.m <- melt(pain.cond, id.vars="Condition")
@ 







\subsection{Analysis of Pain Ratings}
 
<<meanpaingroup, echo=FALSE, results=hide>>=
deceptivepain <- painratings[with(painratings,Condition=="Treatment"),]
placebopain <- painratings[with(painratings,Condition=="Placebo"),]
notreatpain <- painratings[with(painratings,Condition=="No Treatment"),]
meandeceptivepain <- apply(deceptivepain[,4:48], 2, mean, na.rm=TRUE)
meanplacebopain <- apply(placebopain[,4:48], 2, mean, na.rm=TRUE)
meannotreatpain <- apply(notreatpain[,4:48], 2, mean, na.rm=TRUE)
meangrouppainratings <- cbind(meandeceptivepain, meanplacebopain, meannotreatpain)
meangrouppainratings <- as.data.frame(meangrouppainratings)
meangrouppainratings[,"Time"] <- 1:45
meddeceptivepain <- apply(deceptivepain[,4:48], 2, median, na.rm=TRUE)
medplacebopain <- apply(placebopain[,4:48], 2, median, na.rm=TRUE)
mednotreatpain <- apply(notreatpain[,4:48], 2, median, na.rm=TRUE)
medpainratings <- as.data.frame(cbind(meddeceptivepain, medplacebopain, mednotreatpain))
medpainratings[,"Time"] <- 1:45
dec.pain.mean <- apply(deceptivepain[,4:48], 2, mean, na.rm=TRUE)
open.pain.mean <- apply(placebopain[,4:48], 2, mean, na.rm=TRUE)
notreat.pain.mean <- apply(notreatpain[,4:48], 2, mean, na.rm=TRUE)

painbycond <- data.frame(Deceptive=dec.pain.mean, Open=open.pain.mean, NoTreat=notreat.pain.mean)
painbycond[,"Time"] <- 1:45
painbycond.m <- melt(painbycond, id.vars="Time")
@ 

\begin{figure}
<<tsmeanpainplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
meanpain.melt <- melt(meangrouppainratings, id="Time")
tspainplot <- ggplot(meanpain.melt, aes(x=Time, y=value, label=variable, colour=variable))+geom_smooth(span=0.2)
print(tspainplot)
@   
  \caption{Plot of Pain Responses by Condition Over Time}
  \label{fig:tsmeanpainplot}
\end{figure}



As can be seen from Figure \ref{fig:tsmeanpainplot}, the placebo group tended to report lower mean pain ratings across time. This was an extremely unexpected finding, and therefore the pain ratings were reanalysed using medians, to lessen the effects that outliers could be having on the results. The curves shown above used a locally weighted smoother (loess, span=0.2) to create the lines, given the substantial non linearity of the results. However, this plot does show that there was a significant difference between the placebo group and the two other conditions. 

\begin{figure}
<<medtspainplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
medpain.melt <- melt(medpainratings, id="Time")
medtspainplot <- ggplot(medpain.melt, aes(x=Time, y=value, group=variable, colour=variable))+geom_smooth(span=0.5)
print(medtspainplot)
@   
  \caption{Median Pain Ratings Over Time by Condition}
  \label{fig:medpainplot}
\end{figure}


As can be seen from Figure \ref{fig:medpainplot}, the results of the median pain ratings by group show exactly the same pattern as the mean pain ratings. This suggests that there is a real difference here, one that warrants further explanation. Again, a locally weighted smoother (loess, span=0.5) was used to fit the curves. 


\begin{figure}
<<medianpaintscorrelationplots, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
par(mfrow=c(2,2))
print(acf(na.omit(with(medpainratings,meddeceptivepain))))
print(acf(na.omit(with(medpainratings,medplacebopain))))
print(acf(na.omit(with(medpainratings,mednotreatpain))), main="Autocorrelation No Treatment Group")
@   
  \caption{Autocorrelation Plots for Median Pain Responses over Time by Condition}
  \label{fig:autocorrpainplot}
\end{figure}


As can be seen above, the autocorrelation plots for the three groups appear to be similiar, which means that a the same ARIMA model can be fit to them. The first three differences are significant, and a process of ARIMA model fitting indicates that an ARIMA(1,3,1) model has the best AIC and likelihood. This is information that needs to be incorportated into an overall model which will be fit to the data.

\begin{figure}
<<meanpaintscorrelationplots, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
plot.new()
par (mfrow=c (2,2))
sink ("tmp.txt")
print(acf(na.omit(with(meangrouppainratings, meannotreatpain))))
print(acf(na.omit(with(meangrouppainratings,  meandeceptivepain))))
print(acf(na.omit(with(meangrouppainratings,meanplacebopain))))
sink (NULL)
@   
  \caption{Mean Autocorrelation plots for pain ratings over time}
  \label{fig:meanautocorrplot}
\end{figure}


The mean pain ratings were also examined for autocorrelations, and as shown in Figure \ref{fig:meanautocorrplot}, the results were exactly the same as for the median pain ratings, indicating an ARIMA(1,3,1) model was the best fit for the data.


\subsection{Variables impacting the placebo response}
\label{sec:vari-impact-plac}
In this section, the covariates associated with response to placebo in this sample are examined, first graphically and then through a process of formal model fitting. 

<<tsimport, echo=FALSE, results=hide, eval=FALSE>>=
physfiles <- fileImport("ExperimentDataforR/FullStudy/PhysMeasures/Richieoutput", pattern=".txt$")
gsr.mat <- listToDf(physfiles, 1)

ecg.mat <- listToDf(physfiles, 2)
gsrnames <- colnames(gsr.mat)
ecgnames <- colnames(ecg.mat)
names(gsr.mat) <- NULL
gsr.mat.df <- as.data.frame(gsr.mat) #plot these, look at mean gsr scores as a predictor of placebo use variances also
gsr.mean <- colSums(gsr.mat.df, na.rm=TRUE)
gsr.mean2 <- as.data.frame(gsr.mean)
gsrnames2 <- gsub("GSR", "", gsrnames)
gsr.mean2[,"Participant"] <- gsrnames2
gsr.mat.t <- t(gsr.mat)

names(ecg.mat) <- NULL
ecg.mat.t <- t(ecg.mat)
gsr.df <- as.data.frame(gsr.mat.t)

gsr.df[,"Participant"] <- gsrnames2
## Iatandexpmeasures.phys <- merge(Iatandexpmeasures, gsr.mean2, by="Participant")
@


\begin{figure}
<<ggplothistcond, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
plac.resp.hist.pl <- ggplot(Iatandexpmeasures, aes(x=TCQIAT.Mean))+geom_density()+facet_grid(.~PlacResp)
print(plac.resp.hist.pl)
@   
  \caption{TCQIAT.Mean against Pain Responses Over Time}
  \label{fig:histresp}
\end{figure}


From Figure \ref{fig:histresp}, it can be seen that the participants who did respond to placebo had marginally higher Treatment Credibility IAT scores than those who did not. 






\begin{figure}
<<optiatcorrsurv, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
optiat.histcond <- ggplot(Iatandexpmeasures, aes(x=OptIAT.Mean))+geom_density()+facet_grid(.~PlacResp)
print(optiat.histcond)
@   
  \caption{Density Plots of Optimism IAT Scores by Condition}
  \label{fig:opthistcond}
\end{figure}


Again, it can be seen from Figure \ref{fig:opthistcond} that those who responded to placebo had higher optimism IAT scores than  those who did not, suggesting that something about the IAT is predictive of placebo response. 

\begin{figure}
<<creamhistcond, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
cream.hist.cond <- ggplot(Iatandexpmeasures, aes(x=Cream))+geom_density()+facet_grid(.~PlacResp)
print(cream.hist.cond)
@   
  \caption{Desnity Plot for Mean Cream Credibility Scores by Condition}
  \label{fig:creamhistcond}
\end{figure}


Figure  \ref{fig:creamhistcond} shows that there was a difference in the mean cream credibiliuty scores by whether or not a participant responded to placebo, but the difference was not significant ($t=-0.9545, df=53.766, p=0.3441$). 

\begin{figure}
<<ggplotplacyesno, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
placrespyes <- painratings[with(painratings,PlacResp=="Yes"),]
placrespno <- painratings[with(painratings,PlacResp=="No"),]
placresyesmean <- apply(placrespyes[4:48], 2, mean, na.rm=TRUE)
placresnomean <- apply(placrespno[4:48], 2, mean, na.rm=TRUE)
placresyesnopain <- as.data.frame(cbind(placresyesmean, placresnomean))
placresyesnopain$Time <- 1:45
placres.melt <- melt(placresyesnopain, id="Time")
placresplot <- ggplot(placres.melt, aes(x=Time, y=value, group=variable, colour=variable))+geom_line() +geom_smooth(method="loess")
print(placresplot)
@   
  \caption{Pain Ratings of Participants by Response to Placebo Across Time. Straight line is a loess smoother, the jagged line represents the actual pain levels}
  \label{fig:placyesno}
\end{figure}


A number of findings are apparent from the plot above in Figure \ref{fig:placyesno}. The placebo effect was approximately equivalent to a 15\% decrease in pain (read from the graph at the point the no response participants pain reached seven). This is a relatively large effect, and adds confidence to the significant results for modelling reported below. In addition, the participants who responded to placebo tended to remain in the experiment for a longer period of time (which is intuitively obvious). Below, formal model testing for the major hypotheses takes place. 

\subsection{Logistic Regressions on Placebo Response}
\label{sec:logist-regr-plac}
In order to examine whether or not the IAT scores were predictive of placebo response, a logistic regression model was used. Logistic regression was chosen for this as the response outcome was binary, and this method extends the linear regression model for binary outcomes. 


<<placmod1, echo=FALSE, results=tex>>=
placmod1 <- glm(PlacResp~TCQIAT.Mean, data=Iatandexpmeasures, family=binomial(link="logit"))
placmod1.xtab <- xtable(summary(placmod1),label="tab:placmod1", caption="Logistic Regression of Treatment Credibility IAT on Placebo Response")
print(placmod1.xtab)
@ 

As can be seen above in Table \ref{tab:placmod1}, the treatment credibility IAT was not a significant independent predictor of placebo response.

<<placmod2, echo=FALSE, results=tex>>=
placmod2 <- glm(PlacResp~OptIAT.Mean, data=Iatandexpmeasures, family=binomial(link="logit"))
placmod2.xtab <- xtable(summary(placmod2), label="tab:placmod2", caption="Regression of Optimism IAT on Placebo Response")
print(placmod2.xtab)
@ 

As shown in Table \ref{tab:placmod2} the Optimism IAT is not an independent significant predictor of placebo response either. 

<<placebolotr, echo=FALSE, results=tex>>=
placmod.lotr<- glm(PlacResp~LOTR, data=Iatandexpmeasures, family=binomial(link="logit"))
placmod.lotr.xtab <- xtable(summary(placmod.lotr), label="tab:placebolotr", caption="Regression of LOT-R Scores on Placebo Response")
print(placmod.lotr.xtab)
@ 

In addition, the Life Orientation scores are not a significant predictor of placebo response either, as shown in Table \ref{tab:placebolotr}.

However, when these three variables are placed into the model together, the result is significant.

<<placeboglm, echo=FALSE, results=tex>>=
placmod6<- glm(PlacResp~TCQIAT.Mean*OptIAT.Mean*LOTR, data=Iatandexpmeasures, family=binomial(link="logit"))
placmod.xtab <- xtable(summary(placmod6), label="tab:placmodinter", caption="Regression of TCQIAT, OptIAT and LOT-R on Placebo Response")
print(placmod.xtab)
@ 

As can be seen from Table \ref{tab:placmodinter} when interactions were allowed and the Life Orientation Test was added to the model, both IAT measures were significantly associated with placebo response, and all of the three variables interactions were also significant. 

In addition, this model explains approximately 27\% of the variance in placebo response (using a pseudo R-square of 1-(residual deviance/null deviance)).



\begin{figure}
  
<<ggplot3wayinter, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
iatexpno <- Iatandexpmeasures[with(Iatandexpmeasures,PlacResp=="No"),]
iatexpyes <- Iatandexpmeasures[with(Iatandexpmeasures,PlacResp=="Yes"),]
iatexpno <- na.omit(iatexpno)
iatexpyes <- na.omit(iatexpyes)
iatexpyesno <- rbind(iatexpyes, iatexpno)
lotriatplot <- ggplot(iatexpyesno, aes(x=LOTR, y=TCQIAT.Mean , colour=PlacResp, size=OptIAT.Mean))+geom_point()
print(lotriatplot)
@   
  \caption{Scatterplot of Optimism IAT, Treatment Credibility and LOT-R scores against mean survival time by Condition}
  \label{fig:3wayinter}
\end{figure}

 The plot above in Figure \ref{fig:3wayinter} indicates that there appears to be a non linear interaction between the three variables included in our final model. It can be seen that extremely high scores on both the treatment credibility questionnaire and the Life Orientation test appear to be associated with not responding to placebo, while moderate levels of all three variables appear to provide the greatest likelihood of placebo response. 
 \begin{figure}
<<lotrtcqsmoothplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
lotrtcqplot <- ggplot(iatexpyesno, aes(x=LOTR, y=TCQIAT.Mean , colour=PlacResp))+geom_point()
print(lotrtcqplot)
@     
   \caption{Scatterplot of LOT-R Scores against TCQ IAT Scores, Placebo Response is denoted using colour}
   \label{fig:lotrtcplot}
 \end{figure}


This is perhaps clearer in Figure \ref{fig:lotrtcqplot} where it can be seen that the majority of placebo response occurs at median levels of both self reported optimism and implicit treatment credibility. 

\begin{figure}
<<lotroptplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
lotroptplot <- ggplot(iatexpyesno, aes(x=LOTR, y=OptIAT.Mean , colour=PlacResp))+geom_point()
print(lotroptplot)
@   
  \caption{LOTR Scores against Optimism Scores, colour denotes placebo response}
  \label{fig:lotroptplot}
\end{figure}


From Figure \ref{fig:lotroptplot} it can be seen that scores of 0 or above on the optimism IAT (reflecting no to small difference in favour of positive stimuli) and high self reported optimism are associated with a greater likelihood of placebo response. 

\subsection{Machine Learning and the Placebo Response}
\label{sec:mach-learn-plac}

Given the confusing results obtained from linear statistical modelling of the placebo response, it was decided to fit a more flexible model, treating the placebo response as a two valued (Yes, No) classification task. A random forest model was used for this purpose. 

<<randomforestCV, echo=FALSE, results=hide, cache=TRUE>>=
train.ind <- sample(1:54, 40, replace=FALSE)
iatandexpfull <- Iatandexpmeasures[,c("PlacResp", "Age", "Gender", "Prime", "LOTR", "MAAS", "Pill", "Cream", "Inj", "Acu", "Hom", "Rei", "OptIAT.Mean", "OptIAT.Median", "TCQIAT.Mean", "TCQIAT.Median", "meanconv", "meanalt", "convaltcomp", "lengthsurv")]
iatandexpfull <- na.omit(iatandexpfull)
iatandexpfull.train.ind <- with(iatandexpfull,createDataPartition(PlacResp, p=0.8, list=FALSE))
iatandexpfull.train <- iatandexpfull[iatandexpfull.train.ind,]
iatandexpfull.test <- iatandexpfull[-iatandexpfull.train.ind,]
rf.train <- train(PlacResp~TCQIAT.Mean+meanconv+OptIAT.Mean+LOTR, data=iatandexpfull, method="rf", preProcess=c("center", "scale"))
rf.pred <- predict(rf.train, iatandexpfull.test)

var.used.count <- varUsed(rf.train[["finalModel"]])
var.used.count <- as.data.frame(var.used.count)
rownames(var.used.count) <- c("TCQIAT.Mean", "OptIAT.Mean", "meanconv", "LOTR")

## var.used.count.m[,"variable"] <- names(iatandexpfull)
## ggplot(var.used.count.m, aes(x=variable, y=value))+geom_histogram()+coord_flip()

@ 

<<rf2, echo=FALSE, results=hide, cache=TRUE>>=
iatandexpfull.train.ind2 <- with(iatandexpfull,createDataPartition(PlacResp, p=0.8, list=FALSE))
iatandexpfull.train2 <- iatandexpfull[iatandexpfull.train.ind2,]
iatandexpfull.test2 <- iatandexpfull[-iatandexpfull.train.ind2,]
rf.train2 <- train(PlacResp~., data=iatandexpfull.train2, method="rf", metric="Kappa", maximise=TRUE)
rf.pred2 <- predict(rf.train2, iatandexpfull.test2)
@ 

<<rfTestResults, echo=FALSE, results=tex>>=
test.xtab <- xtable(confusionMatrix(rf.pred, iatandexpfull.test$PlacResp)$table, label="tab:rfplac", caption="Random Forest Accuracy on Placebo Response (Test set)")
print(test.xtab)
@ 

As can be seen from Table \ref{tab:rfplac}, the random forest predicted the unseen data for the placebo response exceptionally well, with an accuracy of 1 ($95CI 0.7684-1$), and was significantly better than a random prediction ($p=0.002059$). This suggests that the major hypothesis of the thesis, that implicit measures would be a useful predictor of the placebo response was accurate. This subject is covered further in the discussion. 

Other models which were fit to the data included a naive bayes classifier ($Acc=0.5714$), a linear support vector machine ($Acc=0.6439$), an an evolutionary tree model ($Acc=0.7857$). Of these, only the evolutionary tree performed above chance when predicting positive placebo response (all models predicted no placebo response correctly). This may be, as the plots above in section \ref{sec:vari-impact-plac} showed, the relationship appeared to be nonlinear, in that moderate levels of the three predictor variables appeared to be more highly associated with positive response to placebo. Tree models allow the data to be split on particular levels of a predictor variable, and this flexibility allows placebos to be predicted more accurately. 

\begin{figure}
<<varImpPlot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
sink ("tmp.txt")
par(mfrow=c (1,1))
print(varImpPlot(rf.train[["finalModel"]]))
sink (NULL)
@   
  \caption{Variable Importance Plot for Random Forest Model on Placebo Response}
  \label{fig:varimprf}
\end{figure}

As can be seen from Figure \ref{fig:varimprf}, all three variables were important to the fit, however the TCQ IAT appeared to be the most important, followed by the mean credibility score for conventional treatments, followed by the Optimism IAT, followed by scores on the LOT-R.


<<evtree, echo=FALSE, results=hide, cache=TRUE>>=
evtree.train <- train(PlacResp~OptIAT.Mean+TCQIAT.Mean+meanconv, data=iatandexpfull.train, method="evtree", preProcess=c("center", "scale"))
evtree.pred <- predict(evtree.train, iatandexpfull.test)
evtree.confusionmat <- confusionMatrix(evtree.pred, iatandexpfull.test$PlacResp)
@ 

To have some kind of graphical representation of how the tree works, the evolutionary tree final model on the training set was plotted. As can be seen from Figure \ref{fig:evtreeplot}, the Treatment Credibility IAT was responsible for most of the predictive power of the model. As hypothesised earlier, it appears that a particular level of score on the TCQIAT was associated with a positive response to placebo. Note that the variables on the plot are scaled and centered, so this cannot be read off as the particular scores on the TCQ-IAT associated with placebo response. 

%% \begin{figure}
<<evtreeplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE, eval=FALSE>>=
print(plot(evtree.train$finalModel))
@   
%%   \caption{Evolutionary Tree plot for Prediction of Placebo Response}
%%   \label{fig:evtreeplot}
%% \end{figure}




\subsection{Analysis of Physiological Data}






<<gsrcollate, echo=FALSE, results=hide, eval=FALSE>>=
gsrwithpred <- merge(gsr.df, Iatandexpmeasures, by="Participant")
@ 

<<gsrbycond, echo=FALSE, results=hide, eval=FALSE>>=
gsrdeceptive <- gsrwithpred[with(gsrwithpred,Condition=="Treatment"), ]
gsropen <- gsrwithpred[with(gsrwithpred,Condition=="Placebo"), ]
gsrnotreat <- gsrwithpred[with(gsrwithpred,Condition=="No Treatment"), ]
gsrplacyes <- gsrwithpred[with(gsrwithpred,PlacResp=="Yes"),]
gsrplacno <- gsrwithpred[with(gsrwithpred,PlacResp=="No"),]
@ 

Before the analysis of GSR data was conducted, the mean and median GSR per group were scaled to ensure that they were directly comparable. Scaling was performed using a z-score method, where each observations value was subtracted from the mean and divided by the standard deviation of all the observations. 


<<gsrmeans, echo=FALSE, results=hide, eval=FALSE>>=
deceptivemeangsr <- lapply(gsrdeceptive[,2:31151], mean, na.rm=TRUE)
openmeangsr <- lapply(gsropen[,2:31151], mean, na.rm=TRUE)
notreatmeangsr <- lapply(gsrnotreat[,2:31151], mean, na.rm=TRUE)
meangsr <- as.data.frame(cbind(deceptivemeangsr,openmeangsr, notreatmeangsr))
meangsr <- lapply(meangsr, as.numeric )
meangsr <- as.data.frame(lapply(meangsr, scale))
meangsr[,"Time"] <- 1:nrow(meangsr)
@ 
\begin{figure}
<<meanplotgsr, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE, eval=FALSE>>=
par(mfrow=c(1,3))
plot(as.ts(na.omit(meangsr[,1:3])), main="Mean GSR over time")
@   
  \caption{Mean GSR Levels by Condition over Time}
  \label{fig:meangsr}
\end{figure}


The results shown above in Figure \ref{fig:meangsr} are quite unexpected, but fit with the strange pain ratings observed in this experiment. It can be seen that the Open Placebo group had the lowest GSR over time (middle plot), and that the deceptive pain group seemed to have the highest GSR throughout the experiment. The No Treatment group chart is perhaps the strangest, showing a steady upward trend until about 25 minutes into the experiment, and then falling steadily from there to reach (at 30000, or 45 minutes after the pain was induced) approximately the level which it began. This may have occurred due to habituation to the experimental environment, as they had no experimental manipulation during the course of the study which might have changed their GSR. In contrast, the Open Placebo group show a low GSR starting off which then rises slowly, dips and then rises again. The course of GSR in the Deceptive Placebo group looks like what would have been expected from the No Treatment group; i.e. a slow and steady rise throughout the experiment.

<<placmodgsr, echo=FALSE, results=tex, eval=FALSE>>=
placmod10 <- glm(PlacResp~gsr.mean, data=Iatandexpmeasures.phys, family=binomial(link="logit")) #extremely significant!
print(xtable(summary(placmod10), label="tab:placmodgsr", caption="Logistic Regression for the Impact of Mean GSR measurements on placebo response"))
@ 

As can be seen from Table \ref{tab:placmodgsr}, the mean level of skin response was significantly associated with the response to placebo. 

<<gsrmedians, echo=FALSE, results=hide, eval=FALSE>>=
deceptivemediangsr <- lapply(gsrdeceptive[,2:35151], median, na.rm=TRUE)
openmediangsr <- lapply(gsropen[,2:35151], median, na.rm=TRUE)
notreatmediangsr <- lapply(gsrnotreat[,2:35151], median, na.rm=TRUE)
medgsr <- as.data.frame(cbind(deceptivemediangsr, openmediangsr, notreatmediangsr))

medgsr <- lapply(medgsr, as.numeric)
medgsr <- as.data.frame(medgsr)
medgsr <- as.data.frame(lapply(medgsr, scale))
@ 
\begin{figure}
<<gsrmedianplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE, eval=FALSE>>=
plot(as.ts(na.omit(medgsr)), main="Median GSR Over Time", ylab="Time")
@   
  \caption{Median GSR by Condition Over Time}
  \label{fig:mediangsr}
\end{figure}


Again, the same pattern in GSR is seen in Figure \ref{fig:mediangsr} which suggests that the mean results shown in Figure \ref{fig:meangsr} are not the result of a small number of anomalous observations. 

<<ggplotgsr, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE, eval=FALSE>>=
meangsr.melt <- melt(meangsr, id="Time")
meangsrplot <- ggplot(meangsr.melt, aes(x=Time, y=value))+geom_line()+facet_grid(variable~.)
print(meangsrplot)
@ 


\subsection{Examining the Effect of Pain on GSR}
\label{sec:exam-effect-pain}

The next step in the analysis was to examine the impact of the particular painful stimuli administered on skin conductance. Some preliminary research has suggested that this may be a useful predictor of painful stimuli, and this question will be covered in the following section. However, in this section, the impact of the painful stimuli on skin conductance will be assessed. Note that due to experimenter error, not all participants had the band to induce pain at 300s into the experiment. This time of administration was recorded, and additionally as participants squeezed either more slowly or more quickly, there was some variability in these times. 

The first step in this analysis was to examine the size of these disparities. 

<<bandagequeeze, echo=FALSE, results=hide>>=
bandage.squeeze <- Iatandexpmeasures [,c ("Participant", "BandageOn", "SqueezStop")]
bandage.squeeze [,"SqueezingTime"] <- with (bandage.squeeze, SqueezStop-BandageOn)
@ 
 

\begin{figure}
<<bandonoff, echo=FALSE, fig=TRUE>>=
band.m <- melt (bandage.squeeze [,1:3], id.vars="Participant")
print (ggplot (band.m, aes (x=value, fill=variable))+geom_density (alpha=0.5))
@ 
  \caption{Density Plots for Time of Application of Bandage and Stopping Squeezing}
  \label{fig:onoffplot}
\end{figure}

Density Plots for the time of application of bandage and when the participants stopped squeezing are shown in Figure \ref{fig:onoffplot}. Note that the shape of the time of application is far more regular than that for the  stopping squeezing, suggesting that inter-participant variability was responsible for the majority of observed differences. 

\begin{figure}
<<squeezeplot, echo=FALSE, fig=TRUE>>=
squeeez.pl <- ggplot (bandage.squeeze, aes (x=SqueezingTime))+geom_density ()
print (squeeez.pl)
@ 
  \caption{Density Plot of Time Spent Squeezing the Hand Exerciser}
  \label{fig:squeezplot}
\end{figure}


As can be seen from Figure \ref{fig:squeezplot}, the majority of participants spent approximately between 30 and 50 seconds squeezing the hand exercisor, which seems reasonable given that they were asked to squeeze it for 40 seconds. 

<<physlen, echo=FALSE, results=hide>>=
physlen <- read.table ("./ExperimentDataforR/FullStudy/PhysMeasures/Richieoutput/FullTimePhys2.txt", header=FALSE)
names (physlen) <- c ("Time", "Participant")
physlen [,"Participant"] <- with (physlen, as.factor (Participant))
survlength.ratings <- Iatandexpmeasures [,c ("Participant","BandageOn", "lengthsurv")]
len.test <- merge (physlen, survlength.ratings, by="Participant")
len.test [,"Time"] <- with (len.test, Time/1000)
len.test [,"Diff"] <- with (len.test, Time-lengthsurv)
@ 

Another sanity check was then performed on the datasets to ensure that all data had been recorded correctly. This was the examination of the lengths of the physiological recordings versus the length of the recordings of pain ratings. While the physiological data should be more accurate here (as it was recorded automatically and at a high resolution) any major differences would be cause for elimination of the affected records. 


<<physpaindiffplot, echo=FALSE, fig=TRUE, eval=FALSE>>=
physpaindiff.pl <- ggplot (len.test, aes (x=Diff))+geom_density ()
print (physpaindiff.pl)
@ 

\subsection{Machine Learning and the Placebo Response}
\label{sec:mach-learn-plac}

Firstly, the data was split into two random sections, with 80\% of the data utilised to train the models, and 20\% held aside for a final validation of any model's predictive power. Additionally, the training set was split into ten sections, of which nine were used to train the model and one was used to test the model. This allowed for the accuracy of each model to be ranked in a preliminary fashion, though the ultimate arbiter was performance on the training set. 

<<testandtrain, echo=FALSE, results=hide>>=
require(doMC)
registerDoMC(2)
iatandexpfull <- Iatandexpmeasures[,c("PlacResp", "Age", "Gender", "Prime", "LOTR", "MAAS", "Pill", "Cream", "Inj", "Acu", "Hom", "Rei", "OptIAT.Mean", "OptIAT.Median", "TCQIAT.Mean", "TCQIAT.Median", "meanconv", "meanalt", "convaltcomp")]
iatandexpfull <- na.omit(iatandexpfull)
iatandexpfull.train.ind <- with(iatandexpfull,createDataPartition(PlacResp, p=0.8, list=FALSE))
iatandexpfull.train <- iatandexpfull[iatandexpfull.train.ind,]
iatandexpfull.test <- iatandexpfull[-iatandexpfull.train.ind,]
mytrain <- trainControl(method="repeatedcv", number=10, repeats=25)
@ 


<<rftrain, echo=FALSE, results=hide>>=
randf.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv, data=iatandexpfull.train, method="rf", trControl=mytrain, tuneGrid=data.frame(.mtry=1:10), preProcess=c("center", "scale"))
randf.pred <- predict(randf.train, iatandexpfull.test)
@ 

<<nbtrain, echo=FALSE, results=hide>>=
## nb.train <- train(PlacResp~., data=iatandexpfull.train, method="nb", trControl=mytrain, tuneGrid=data.frame(.usekernel=c(TRUE, FALSE), .fL=0:1), preProcess=c("center", "scale"))
nb.train.pc <- train(PlacResp~., data=iatandexpfull.train, method="nb", trControl=mytrain, tuneGrid=data.frame(.usekernel=c(TRUE, FALSE), .fL=0:1), preProcess=c("center", "scale", "pca"))
@

<<knntrain, echo=FALSE, results=hide>>=
knn.train.pc <- train(PlacResp~. , data=iatandexpfull.train, method="knn", trControl=mytrain, tuneGrid=data.frame(.k=1:10), 
                   preProcess=c("center", "scale", "pca"))
knn.train <- train(PlacResp~. , data=iatandexpfull.train, method="knn", trControl=mytrain, tuneGrid=data.frame(.k=1:10), 
                   preProcess=c("center", "scale"))
@ 

<<svmtrain, echo=FALSE, results=hide>>=
## svm.train.pc <- train(PlacResp~., data=iatandexpfull.train, method="svmLinear", trControl=mytrain, tuneGrid=data.frame(.C=1:10), preprocess=c("center", "scale", "pca"))
## svm.train <- train(PlacResp~., data=iatandexpfull.train, method="svmLinear", trControl=mytrain, tuneGrid=data.frame(.C=1:10), preprocess=c("center", "scale"))
@ 

<<svmradialtrain, echo=FALSE, results=hide>>=
#svmradial.train <- train(PlacResp~., data=iatandexpfull.train, method="lssvmRadial", trControl=mytrain, preProcess=c("center", "scale", "pca"))
@ 

<<gbmtrain, echo=FALSE, results=hide, eval=FALSE>>=
gbm.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+LOTR+meanconv, data=iatandexpfull.train, method="gbm", trControl=mytrain, tuneGrid=data.frame(.shrinkage=(c(0.1, 0.01, 0.001)), .n.trees=c(50,100,150), .interaction.depth=3))
@ 

<<gamboost, echo=FALSE, results=hide>>=
gamboost.train <- train(PlacResp~., data=iatandexpfull.train, method="gamboost", trControl=mytrain, tuneGrid=data.frame(.mstop=1:100, .prune=1:100))
@ 

<<glmboost, echo=FALSE, results=hide>>=
glmboost.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv, data=iatandexpfull.train, method="glmboost", trControl=mytrain, tuneGrid=data.frame(.mstop=1:100, .prune=1:100))
@ 

% <<logitboost, echo=FALSE, results=hide>>=
% logitboost.train <-  train(PlacResp~., data=iatandexpfull.train, method="logitBoost", trControl=mytrain, tuneGrid=data.frame(.nIter=seq(10,500, by=10)))
@ 

<<glmnettrain, echo=FALSE, results=hide>>=
glmnet.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv, data=iatandexpfull.train, method="glmnet", trControl=mytrain, tuneGrid=data.frame(.alpha=seq(0.1, 1, by=0.1), .lambda=seq(0.1, 1.0, by=0.1)))
@ 

<<blackboost, echo=FALSE, results=hide>>=
blackboost.train <- train(PlacResp~., data=iatandexpfull.train, method="blackboost", trControl=mytrain, tuneGrid=data.frame(.mstop=c(50,100,150), .maxdepth=1:3))
@ 

<<glmstep, echo=FALSE, results=hide>>=
glmstep.train <- train(PlacResp~., data=iatandexpfull.train, method="glmStepAIC", trControl=mytrain)
@ 

<<rpart, echo=FALSE, results=hide>>=
rpart.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv, data=iatandexpfull.train, method="rpart", trControl=mytrain, tuneGrid=data.frame(.cp=seq(0.01, 1, by=0.01)))
@ 

<<rpart2, echo=FALSE, results=hide>>=
rpart2.train <- train(PlacResp~., data=iatandexpfull.train, method="rpart2", trControl=mytrain, tuneGrid=data.frame(.maxdepth=1:10), preProcess=c("center", "scale"))
@ 

<<svmRadialCost, echo=FALSE, results=hide>>=
## svmradialcost.train.pc <- train(PlacResp~., data=iatandexpfull.train, method="svmRadialCost", trControl=mytrain, tuneGrid=data.frame(.C=1:10), preProcess=c("center", "scale", "pca"))
## svmradialcost.train <- train(PlacResp~., data=iatandexpfull.train, method="svmRadialCost", trControl=mytrain, tuneGrid=data.frame(.C=1:10), preProcess=c("center", "scale"))
@ 

<<ctree, echo=FALSE, results=hide>>=
ctree2.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv, data=iatandexpfull.train, method="ctree2", trControl=mytrain, tuneGrid=data.frame(.maxdepth=seq(0,10, by=1)), preProcess=c("center", "scale"))
@ 

<<evtree, echo=FALSE, results=hide>>=
evtree.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv, data=iatandexpfull.train, method="evtree", trControl=mytrain, tuneGrid=data.frame(.alpha=1:10), preProcess=c("center", "scale"))
@ 

<<cforest, echo=FALSE, results=hide>>=
cforest.train <- train(PlacResp~TCQIAT.Mean+OptIAT.Mean+meanconv, data=iatandexpfull.train, method="cforest", trControl=mytrain, tuneGrid=data.frame(.mtry=seq(1, 101, by=10)))
@ 

<<rferns, echo=FALSE, results=hide>>=
## rferns.train <- train(PlacResp~., data=iatandexpfull.train, method="rFerns", trControl=mytrain, tuneGrid=data.frame(.depth=1:10) )
@ 

<<pred, echo=FALSE, results=hide>>=
## nb.pc.pred <- predict(nb.train.pc, iatandexpfull.test)
## nb.pred <- predict(nb.train, iatandexpfull.test)
## knn.pc.pred <- predict(knn.train.pc, iatandexpfull.test)
## knn.pred <- predict(knn.train, iatandexpfull.test)
## svm.pc.pred <- predict(svm.train.pc, iatandexpfull.test)
## svm.pred <- predict(svm.train, iatandexpfull.test)
## gamboost.pred <- predict(gamboost.train, iatandexpfull.test)
glmboost.pred <- predict(glmboost.train, iatandexpfull.test)
## logitboost.pred <- predict(logitboost.train, iatandexpfull.test)
glmnet.pred <- predict(glmnet.train, iatandexpfull.test)
## blackboost.pred <- predict(blackboost.train, iatandexpfull.test)
## glmstep.pred <- predict(glmstep.train, iatandexpfull.test)
rpart.pred <- predict(rpart.train, iatandexpfull.test)
## rpart2.pred <- predict(rpart2.train, iatandexpfull.test)
## svmradialcost.pred <- predict(svmradialcost.train, iatandexpfull.test)
ctree.pred <- predict(ctree2.train, iatandexpfull.test)
## evtree.pred <- predict(evtree.train, iatandexpfull.test)
cforest.pred <- predict(cforest.train, iatandexpfull.test)
## rferns.pred <- predict(rferns.train, iatandexpfull.test)
@ 

<<confusionmats, echo=FALSE, results=hide>>=
randf.acc <-confusionMatrix(randf.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
## nb.acc <-confusionMatrix(nb.pc.pred, iatandexpfull.test[,"PlacResp"])
## knn.pc.acc <- confusionMatrix(knn.pc.pred, iatandexpfull.test[,"PlacResp"])
## knn.acc <- confusionMatrix(knn.pred, iatandexpfull.test[,"PlacResp"])
## svm.pc.acc <-confusionMatrix(svm.pc.pred, iatandexpfull.test[,"PlacResp"])
## svm.acc <- confusionMatrix(svm.pred, iatandexpfull.test[,"PlacResp"])
## gamboost.acc <- confusionMatrix(gamboost.pred, iatandexpfull.test[,"PlacResp"])
glmboost.acc <- confusionMatrix(glmboost.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
glmnet.acc <- confusionMatrix(glmnet.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
## blackboost.acc <- confusionMatrix(blackboost.pred, iatandexpfull.test[,"PlacResp"])
## glmstep.acc <- confusionMatrix(glmstep.pred, iatandexpfull.test[,"PlacResp"])
rpart.acc <- confusionMatrix(rpart.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
## rpart2.acc <- confusionMatrix(rpart2.pred, iatandexpfull.test[,"PlacResp"])
## svmradial.acc <- confusionMatrix(svmradialcost.pred, iatandexpfull.test[,"PlacResp"])
ctree.acc <- confusionMatrix(ctree.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
## evtree.acc <- confusionMatrix(evtree.pred, iatandexpfull.test[,"PlacResp"])
cforest.acc <- confusionMatrix(cforest.pred, iatandexpfull.test[,"PlacResp"], positive="Yes")
## rferns.acc <- confusionMatrix(rferns.pred, iatandexpfull.test[,"PlacResp"])
@ 


After an extensive model fitting process, only three models reached acceptable levels of accuracy. The first of these was the penalised regression methods used in the glmnet package. This model achieved an overall accuracy of 0.667 ($95CI 0.3489 - 0.9008$). The next algorithm which achieved acceptable accuracy was the conditional random forest model. These models form ensembles of trees to reduce variability and increase predictive power. This model achieved an accuracy of 0.75 ($95CI 0.4281 - 0.9451$), with a sensitivity of 0.8750 and a specificity of 0.5. This would suggest that while the model was excellent at predicting those who would respond to placebo, it was much worse (chance level) at predicting those who would not. The final model which was selected as being acceptably accurate was a glmboost algorithm which averages the results of many generalised linear models (logistic regression, in this case) and weights observations which were categorised incorrectly more highly in the next iteration of the algorithm. This model achieved an accuracy of 0.875, ($95CI 0.4281 -- 0.9421$), with a sensitivity of 0.875 and a specificity of 0.5. Again, note that the negative examples of placebo response are harder to predict than are the positive ones. The probability of correctly classifying a positive placebo response was 0.7778, while the probability of classifying a lack of placebo response was 0.6667. 


\section{Discussion} 

A number of caveats are in order here. Firstly, neither the optimism IAT or the treatment credibility IAT were independently predictive of placebo response. However, when the model included an interaction between them, all three of these variables were significant (as in the model shown above). This may indicate that there may be some irrelevant (from the conventional scoring perspective) feature of the IAT measures which was predictive of placebo response in this sample. This is a matter which can be teased out by future research. 

Secondly, given the number of models fitted, some were almost certain to come up as significant, and the three way interaction between the two IAT measures and the Life Orientation Test was not a hypothesis of the research. 

It might be questioned why the Condition variable was not included in the model - it was, but it was dropped as its presence caused errors in the model fit. A little thought explains why this is so - placebo response was only possible in 2 of 3 conditions, and Condition acted as a proxy for these, thus causing issues with the model fit (in essence, Condition was collinear with placebo response). The final model shown above was superior to the other models considered, in terms of AIC and other model fit indices. 


%%% Local Variables:
%%% TeX-master: "PlaceboMeasurementByMultipleMethods"
%%% End:
