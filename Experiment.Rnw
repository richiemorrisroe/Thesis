
\section{Introduction}
\subsection{Placebos, self reports and pain}

The focus of this section is on the definition of placebo, but the definition of placebo effects is without consequence if it is impossible to establish that a placebo effect has or has not occurred in a controlled setting. One of the largest problems with establishing this is when the outcome is a subjective one, such as pain (which is the domain where the placebo was examined in this thesis). Pain is a subjective experience, and equivalent stimulus levels may cause entirely different reactions  in two different people \cite{Kirsch1997}. 

This issue is often controlled for by calibration of the stimulus levels to the individual participant in an experiment. This allows the researcher to assess to what extent these pain levels are altered by the placebo treatment. Normally, pain intensity and unpleasantness levels are assessed on an eleven point Visual Analogue Scale (VAS)  and these ratings are used as the input data for statistical analysis of different groups. 

However, the use of these self report instruments carries with it a number of problems. Firstly, the issue of demand characteristics arises as a consequence of this. Demand characteristics refer to the subtle pressures faced by participants in experiments to live up to the expectations of the researcher \cite{weber1972subject} and were discussed in Section \ref{sec:plac-rand-trials}. The idea is that if a researcher really wants to demonstrate something, then he or she will communicate this subtly to the participants in the study, and they will (theoretically) respond in the manner in which the researcher prefers. This factor is one of the major reasons why tests of new drugs must blind the experimenter as well as the participants. 

Another large factor which affects placebo analgesia research is the issue of response biases. The problem is that if participants receive a treatment which they believe to be effective, then their thresholds of perception may be altered without any actual biochemical changes. This is the theory put forward by Allan \& Siegel's \cite{Allan2002} analysis of the placebo phenomenon in terms of signal detection theory. These two factors were proposed to account for the entirety of the placebo effect in pain by Hrobjarrstsson \& Goetzche \cite{hrobjartsson2001}. This interpretation of placebo seems less likely given that placebo effects have been observed on biochemical parameters, and also  given that placebo analgesia can be removed by the administration of naloxone \cite{Levine1979,benedetti2003a}. 

\subsection{Measurement Issues}
\label{sec:measurement-issues}

The measurement of placebo has improved greatly over the past fifty years since Beecher started examining the phenomenon. However, it still suffers from a number of problems.

The first problem is the consistent use of ANOVA and regression methods to examine differences between groups \cite{Colloca2008b,Pollo2001}. While there is nothing intrinsically wrong with this method (as long as it is not within person differences which are examined), it is a waste of statistical power. The issue here is that in placebo analgesia research (which this thesis made use of) typically only examines group differences in scores. The problem with this approach is that typically, pain scores are collected over time, and so the responses of one participant at time $t$ are not independent of the responses of this participant at $t+1$ (cf Chapter \ref{cha:methodology}). This violates the assumptions of ANOVA (and of linear regression). While a simple binary responder/non responder classification is possible, this approach throws away large amounts of information. One study which did use more appropriate methods was \cite{Bausell2005} who examined the effects of expectancies and acupuncture using survival analysis. 

More appropriate methods for these analyses are time series analyses and survival analysis, both of which are discussed in Chapter \ref{cha:methodology}. Additionally, in trials of functional bowel disorders, placebo effects correlate with the size of the trial, which suggests that regression to the mean effects are having an impact \cite{Enck2005a}. 

The next major problem with the current analytic strategies taken towards placebo is that most of the techniques used (ANOVA, linear models etc) assume that the mean is an appropriate measure of central tendency. However, given that placebo response can be modelled as binary (it either occurs or it does not) then the distribution would be expected to be bi-modal, in which case the mean can be very misleading. In reference to this point, a re-analysis of a number of placebo trials showed vast differences between the mean and median placebo responses \cite{McQuay1996}. 

Additionally, the additive model of drug and placebo interactions may be an issue. This is typically taken as true, which is a position which may have had validity before large scale computing resources were available, but this constraint does not apply any more \cite{Caspi2002}. The issue of additive placebo and drug interactions will be discussed further in Chapter \ref{cha:notes-towards-theory}. 


One final issue with current placebo research relates to expectancies. The first issue is that expectancies are typically measured with a simple one question scale. One of the aims of this thesis was to apply psychometric methods to the devlopment of a more sophisticated measure of treatment credibility and expectancies (see Chapter \ref{cha:tcq-thesis}). The other problem is that expectancies are assummed to be conscious, even though they appear to have far more in common with unconscious responses than they do with controlled processes. In the next section, the rationale behind this approach is explained. 



\subsection{Modeling Placebo}
\label{sec:modeling-placebo}

\section{Modelling Placebo by Multiple Methods}
\label{sec:modell-plac-mult}

Finally, this section will present the overall approach and rationale for the work carried out in the course of this thesis.

The placebo effect is a complex phenomenon. It has been established that it can be predicted by some variables which are typically measured using self report approaches. These variables include expectancies and optimism. Optimism was measured using the Life Orientation Test, Revised (LOT-R - see Chapter \ref{cha:health-for-thesis} for details on this instrument), and a new measure of expectancies and treatment credibility was developed (Chapter \ref{cha:tcq-thesis}). 

Additionally, these variables only explain a small proportion of the variance in the observed placebo response. It is the contention of this researcher that some of the residual variance can be predicted by the use of implicit measures (specifically the IAT). Therefore, two implicit measures, one of Treatment Credibility and the other of Optimism were developed (see Chapter \ref{cha:development-of-iats} for details). 

The relationship between the observed placebo response, and these explicit and implicit measures was of primary importance to this research, and so a measure of Mindfulness (Mindful Attention Awareness Scale) was also used. 

In order to make use of psychometric modelling, large samples of the self report instruments were collected in the same population from which the experimental participants were drawn. This allowed for factor score and IRT models to be built for each of the measures. 

In the experiment itself, (see Chapter \ref{cha:primary-research}) physiological (ECG and GSR) was collected in order to examine the physiological characteristics of placebo and to allow for another form of measurement to be included in the final model.

The collection of these various forms of data, along with a behavioural criterion (the observed placebo response) allowed for psychometric models (Structural equation models) to be examined to seperate out the effects of the various predictor type. In essence, this thesis seeks to marry the strengths of psychology in psychometric modelling to its complementary strengths in experimental design, with the aim of establishing these methods and measures relative usefulness in the prediction of placebo. Much more detailed descrptions of the major models applied are given in Chapter \ref{cha:notes-towards-theory}. 


\section{Methodology}

\subsection{Experimental Procedure}
<<healthenv, echo=FALSE, results=hide>>=
## load("healthforthesis.rda")
## load("tcq2.rda")
load("homdata.rda")
load("credtotals.rda")
## load("tcqthesis.rda")
@ 

\subsubsection{Time Series Analysis}

Time Series Analysis is an extremely widely used statistical tecnhique.  The models in use today in the social sciences tend to be ARIMA models and their descendents, and these were developed in the 1970's by Box and Jenkins\cite{box1970time}.  \footnote{ARIMA stands for Auto Regressive Integrated Moving Average models}, The features of these models are described below \cite{mccleary1980applied}.

The general framework for the models is as follows. Firstly, the time series process is assumed to be stochastic and a random walk. Given the observations at time $t$, the observation at $t+1$ is assumed to be generated by a random shock from a normal distribution with mean 0 and standard deviation 1 (the standard normal). This procedure is repeated over time, and the time series thus progresses and changes due to the compounded influence of all of these random shocks.

The series at time t can be modelled as follows: $Y_t=\phi Y_t+\phi Y_{t+1}\ldots+a_t$.

Time series analysis was used in this research mainly for the physiological data, though the pain ratings were also modelled as a function of time. This kind of analysis is important as it allows for the dependencies between successive scores to be examined, which is a factor often neglected in placebo research. 

\paragraph{Practical Model Building Strategy}

The following are general steps towards building an ARIMA model.

\begin{itemize}
\item Inspect the plots both of the series and the autocorrelations
\item Examine autocorrelations, differencing if necessary
\item Estimate parameters, ensuring that they are significantly different from zero, and within the bounds of invertibility
\item Examine residuals, if they are not white noise, repeat steps 1-4 until they are.
\end{itemize}

\paragraph{Event History Analysis}
\label{sec:event-hist-analys}
In this research, a particular form of time series analysis was used, which is known as event history analysis \cite{mccleary1980applied}. This type of analysis partitions the time series into two or more parts, based on whether or not a particular event has occurred. In the case of this research, there were either two or three parts to the analysis. In the Deceptive and Open Placebo groups (see Chapter \ref{cha:primary-research}), the first time series occcurred until the painful stimulus was applied, the second was the time from this point until the placebo was applied, and the third was this time point until the end. In the No Treatment group, there were two time series, one for the period before the pain was applied, and one after.

A major advantage of this method is that it allows us to examine changes in the parameters of the time series as a function of experimental stage and condition. This allowed us to estimate more precisely what changes occurred over time as a result of experimental procedure. The basic procedure is as above, except that parameters are estimated on a subset of the data, and cross-correlation functions are used to examine the changes between them.



\section{Results}
\label{sec:results}



<<importdata, echo=FALSE, results=hide>>=
setwd("./ExperimentDataforR/FullStudy/")
expmeasures <- read.csv("explicitmeasuresfixed.csv")
expmeasures[,"LOTR"] <- with(expmeasures, LOTR/6)
vasscores <- read.csv("VASscores.csv"       )
optiat <- read.csv("optiatres.csv" )
tcqiat <- read.csv("tcqiatres.csv")
setwd("../..")
@ 



<<loadpackages, echo=FALSE, results=hide>>=
require (randomForest)
require (cacheSweave)
require (gridExtra)
require(psych)
require(xtable)
require(arm)
require(ggplot2)
require(reshape2)
require(eRm)
require(ltm)
require(boot)
require(plyr)
require(caret)
require(survival)
source("func.R")
@ 

<<iatsort, echo=FALSE, results=hide>>=
tcqiatsorted <- tcqiat[,c("Participant", "Date", "Time", "Block", "Accurate", "Actual", "Real", "Truth", "Fake", "Illusory", "Inaccurate", "Lies", "Cream", "Injections", "Pills", "Surgery", "Acupuncture", "FlowerEssence", "Homeopathy", "Reiki", "Correct", "BlockTime")]
optiatsorted <- optiat[, c("Participant", "Date", "Time", "Block", "Me", "Mine", "Myself", "Theirs", "Them", "Themselves", "Better", "Happy", "Improving", "Succeeding", "Disimproving", "Failing", "Sad", "Worse", "Correct", "BlockTime")]
@ 

<<optiatscore, echo=FALSE, results=hide>>=
optiatsorted[,"Block"] <- with(optiatsorted, gsub(":", "", x=Block))
optiatscore.mean <- calcIatScores(optiatsorted,Code="Participant", method="mean", words=c("Me", "Mine", "Myself", "Theirs", "Them", "Themselves", "Better", "Happy", "Improving", "Succeeding", "Disimproving", "Failing", "Sad", "Worse"))
names(optiatscore.mean)[1] <- "Participant"
names(optiatscore.mean)[6] <- "OptIAT.Mean"
optstimblock3 <- optiatscore.mean[,grep("Block3.", x=names(optiatscore.mean))]
optstimblock5 <- optiatscore.mean[,grep("Block5.", x=names(optiatscore.mean))]
optiatscore.median <- calcIatScores(optiatsorted,Code="Participant", method="median", words=c("Me", "Mine", "Myself", "Theirs", "Them", "Themselves", "Better", "Happy", "Improving", "Succeeding", "Disimproving", "Failing", "Sad", "Worse"))
names(optiatscore.median)[1] <- "Participant"
names(optiatscore.median)[6] <- "OptIAT.Median"
optiatscore <- merge(optiatscore.mean[c(1,6)], optiatscore.median[,c(1,6)], by="Participant")
part.opt <- unique(optiat[,"Participant"])
part.tcq <- unique(tcqiat[,"Participant"])
optstimblock3 <- as.data.frame(cbind(part.opt, optstimblock3))
optstimblock5 <- as.data.frame(cbind(part.opt, optstimblock5))
@ 

\subsection{Analysis of IAT data}
\label{sec:analysis-iat-data}
The first step in the analysis of IAT data is to examine the differential impact of using the mean versus the median as the measure of central tendency for the calculation of IAT scores (the $D$ measure). The results for the Optimism IAT are shown in Figure \ref{fig:meanmedoptiatplot}. As can be seen from this figure, there were no major changes attributable to this difference (the correlation between the two scores was $r=0.91$). 

\begin{figure}
<<meanmedoptiatplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
meanmedoptiatplot <- ggplot(optiatscore, aes(x=OptIAT.Mean, y=OptIAT.Median))+geom_point()+geom_smooth(method="lm")
print(meanmedoptiatplot)
@   
  \caption{Scatterplot with Linear Regression Smooth of IAT score for Optimism IAT calculated with mean (x) and median (y)}
  \label{fig:meanmedoptiatplot}
\end{figure}



<<optiatboot, echo=FALSE, results=hide, cache=TRUE>>=
optcritblocks <- merge(optstimblock3, optstimblock5, by="part.opt")
Iatcalcopt <- function (data, indices=rep(0.00885, 113)) {
  d <- data[sample(indices, replace=TRUE),]
  b3mean <- apply(d[,2:length(optcritblocks)], 1, mean, na.rm=TRUE)
  b3sd <- apply(d[,2:length(optcritblocks)], 1, sd, na.rm=TRUE)
  b5mean <- apply(d[,2:length(optcritblocks)], 1, mean, na.rm=TRUE)
  b5sd <- apply(d[,2:length(optcritblocks)], 1, sd, na.rm=TRUE)
  ovsd <- (b3sd+b5sd)/2
  Iatscore <- (b5mean-b3mean)/ovsd
}
optboot <- boot(data=optcritblocks, statistic=Iatcalcopt, R=1000)
## optci <- boot.ci(optboot)
## optmeanconf <- as.data.frame(optci[["normal"]])
## names(optmeanconf) <- c("confidence level", "lower quantile", "upper quantile")
@ 



<<optmeanconfprint, echo=FALSE, results=tex>>=
## optmeanconf.xtab <- xtable(optmeanconf, label="tab:optmeanconfprint",
## caption="Summary of Bootstrapped (n=1000) Calculations of IAT Optimism Score")
##  print(optmeanconf.xtab)
@ 

% As can be seen from the Table \ref{tab:optmeanconfprint}, the mean score on the Optimism IAT is relatively unstable, even with quite a large sample. However, as shown in Figure \ref{fig:optbootplot}, the normal approximation used in the bootstrap seems appropriate. 

<<optbootplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
## print(plot(optboot))
@ 


<<optstimboot, echo=FALSE, results=hide>>=
## optmeanboot <- function (data, indices=rep(0.00885, 113)) {
##   d <- data[sample(indices, replace=TRUE),]
##   optmeans <- apply(d[,5:18], 2, mean, na.rm=TRUE)
## }
## optstimboot <- boot(optiatsorted, statistic=optmeanboot, R=1000)
## optstimbootci <- boot.ci(optstimboot)
## optstimbootplot <- plot(optstimboot)
@ 

<<tcqtestblocks, echo=FALSE, results=hide>>=
tcqiatsorted[,"Block"] <- with(tcqiatsorted, gsub(":", "", x=Block))
tcqiatscore.mean <- calcIatScores(tcqiatsorted, Code="Participant", method="mean", words=c("Accurate", "Actual", "Real", "Truth", "Fake", "Illusory", "Inaccurate", "Lies", "Cream", "Injections", "Pills", "Surgery", "Acupuncture", "FlowerEssence", "Homeopathy", "Reiki"))
tcqstimblock3 <- tcqiatscore.mean[,grep("Block3.", x=names(tcqiatscore.mean))]
tcqstimblock5 <- tcqiatscore.mean[,grep("Block5.", x=names(tcqiatscore.mean))]
names(tcqiatscore.mean)[1] <- "Participant"
names(tcqiatscore.mean)[6] <- "TCQIAT.Mean"
tcqiatscore.median <- calcIatScores(tcqiatsorted, Code="Participant", method="median", words=c("Accurate", "Actual", "Real", "Truth", "Fake", "Illusory", "Inaccurate", "Lies", "Cream", "Injections", "Pills", "Surgery", "Acupuncture", "FlowerEssence", "Homeopathy", "Reiki"))
names(tcqiatscore.median)[1] <- "Participant"
names(tcqiatscore.median)[6] <- "TCQIAT.Median"
tcqiatscore <- merge(tcqiatscore.mean[,c(1,6)], tcqiatscore.median[,c(1,6)], by="Participant")
part.tcq <- unique(tcqiat[,"Participant"])
tcqstimblock3 <- as.data.frame(cbind(part.tcq, tcqstimblock3))
tcqstimblock5 <- as.data.frame(cbind(part.tcq, tcqstimblock5))
IATscores <- merge(optiatscore, tcqiatscore, by="Participant")
@

Next, we examine the difference between the mean and median scores for the TCQ IAT. As can be seen from Figure \ref{fig:meanmedtcqiat}, the same pattern as emerged from the Optimism IAT scores is apparent, with little difference between the two measures of central tendency ($r=0.89$). 

\begin{figure}
<<meanmedtcqiat, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
meanmedtcqpl <- ggplot(tcqiatscore, aes(x=TCQIAT.Median, y=TCQIAT.Mean))+geom_point()+geom_smooth(method="lm")
print(meanmedtcqpl)
@   
  \caption{Scatterplot of TCQIAT Median Scores against TCQ IAT mean scores with a linear regression smooth line}
  \label{fig:meanmedtcqiat}
\end{figure}



%%<<tcqiatboot, echo=FALSE, results=hide>>=
## tcqcritblocks <- merge(tcqiatblock3, tcqiatblock5, by="Participant")
## Iatcalctcq <- function (data=tcqcritblocks, indices=rep(0.00885, 113)) {
## d <- data[sample(indices, replace=TRUE),]
## b3mean <- apply(d[5:20], 1, mean, na.rm=TRUE)
## b3sd <- apply(d[5:20], 1, sd, na.rm=TRUE)
## b5mean <- apply(d[26:41], 1, mean, na.rm=TRUE)
## b5sd <- apply(d[,26:41], 1, sd, na.rm=TRUE)
## ovsd <- (b3sd+b5sd)/2
## Iatscore <- (b5mean-b3mean)/ovsd
##  }
## tcqboot <- boot(tcqcritblocks, Iatcalctcq, R=1000)
## tcqci <- boot.ci(tcqboot)
## tcqmeanconf <- as.data.frame(tcqci[["normal"]])
## names(tcqmeanconf) <- c("confidence level", "lower quantile", "upper quantile")
## tcqmeanconf.xtab <- xtable(tcqmeanconf, label="tab:tcqmeanconf", caption="TCQ IAT mean D scores bootstrap confidence estimates")
## tcqbootplot <- plot(tcqboot)
%%@ 

%%<<tcqmeanconf, echo=FALSE, results=tex>>=
## print(tcqmeanconf.xtab)
%%@ 

% As can be seen in Table \ref{tab:tcqmeanconf} the estimated mean IAT score on the treatment credibility IAT is also quite unstable, however as shown below in Figure, the normal approximation seems appropriate. 

%%<<tcqbootplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
## print(tcqbootplot)
%%@ 


%% <<tcqstimboot, echo=FALSE, results=hide>>=
## tcqmeanboot <- function (data, indices=rep(0.00885, 113)) {
##   d <- data[sample(indices, replace=TRUE),]
##  tcqmeans <- apply(d[,5:20], 2, mean, na.rm=TRUE)
## }
## tcqstimboot <- boot(tcqiatsorted, statistic=tcqmeanboot, R=1000)
## tcqstimbootci <- boot.ci(tcqstimboot)
## tcqstimbootplot <- plot(tcqstimboot)
%%@ 
 
The next question is whether or not the IAT\'s have been contaminated by method variance. This can be assessed in a preliminary fashion by examining the correlations between the Treatment Credibility and Optimism IAT. The correlation between the two mean scored IAT measures was ($r=0.003$), while the correlation between the two median scored IAT measures was r=0.08, thus showing that method variance does not appear to have contaminated the results. 


\begin{figure}
<<tcqstimblock3, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
tcqstimblock3.m <- melt(tcqstimblock3, id.vars="part.tcq")
tcqstimblock3.m[,"part.tcq"] <- with(tcqstimblock3.m, as.factor(part.tcq))
print(ggplot(tcqstimblock3.m, aes(x=log(value), colour=as.factor(part.tcq)))+geom_density()+theme(legend.position="none"))
@  
  \caption{TCQ IAT Stimuli Response Times in Block 3. Each density plot represents the distribution of each participants' response times. Plot is on log scale }
  \label{fig:tcqstimblock3}
\end{figure}

\begin{figure}
<<tcqstimblock3plot, echo=FALSE, fig=TRUE, eps=TRUE, pdf=TRUE, png=TRUE>>=
tcqstimblock3.real <- tcqstimblock3[,2:9]
tcqstimreal3.m <- melt(tcqstimblock3.real)
tcqstimblock5.real <- tcqstimblock5[,2:9]
tcqstimreal5.m <- melt(tcqstimblock5.real)
tcqstimreal3.m[,"variable"] <- with(tcqstimreal3.m, gsub("Block3\\.","",x=variable))
tcqstimreal5.m[,"variable"] <- with(tcqstimreal5.m, gsub("Block5\\.","",x=variable))
names(tcqstimreal3.m)[1] <- "Block3"
names(tcqstimreal5.m)[1] <- "Block5"

stimreal3.pl <- ggplot(tcqstimreal3.m, aes(x=Block3, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
stimreal5.pl <- ggplot(tcqstimreal5.m, aes(x=Block5, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
print(arrangeGrob(stimreal3.pl, stimreal5.pl))
@   
  \caption{Real versus Fake Stimuli, TCQ IAT. Boxplots of response times to each stimuli, Block 3 (top) and Block 5 (bottom)}
  \label{fig:tcqstimrealpl}
\end{figure}

As can be seen from Figure \ref{fig:tcqstimrealpl}, the majority of items were responded to relatively quickly in both categories. Of note, however, are the outliers which were words which were associated with fake treatments in Block 3 (where they were paired with conventional treatments) and words which were associated with real treatments (where they were paired with alternative treatments). This would seem to suggest that the words were in fact serving their intended purpose. 


As can be seen from Figure \ref{fig:tcqstimblock3} above, the distribution of participant response times was clearly not normal, being far too skewed to the right and heavy tailed. The log transformation helps matters, but the tails are still extremely long (a point further reinforced by the outliers seen in Figure \ref{fig:tcqstimrealpl} also). 
\begin{figure}
<<tcqstimconvaltpl, echo=FALSE, fig=TRUE, eps=TRUE, png=TRUE, pdf=TRUE>>=
tcqstim3convalt <- tcqstimblock3[,10:17]
tcqstim5convalt <- tcqstimblock5[,10:17]
tcqstim3conv.m <- melt(tcqstim3convalt)
tcqstim5conv.m <- melt(tcqstim5convalt)
tcqstim3conv.m[,"variable"] <- with(tcqstim3conv.m, gsub("Block3\\.", "", x=variable))
tcqstim5conv.m[,"variable"] <- with(tcqstim5conv.m, gsub("Block5\\.", "", x=variable))
names(tcqstim3conv.m)[1] <- "Block3"
names(tcqstim5conv.m)[1] <- "Block5"
tcqstimconv3pl <- ggplot(tcqstim3conv.m, aes(x=Block3, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
tcqstimconv5pl <- ggplot(tcqstim5conv.m, aes(x=Block5, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
print(arrangeGrob(tcqstimconv3pl, tcqstimconv5pl))
@   
  \caption{Boxplots for Conventional and Alternative Stimuli, TCQ IAT, Block 3 (top) Block 5 (bottom)}
  \label{fig:stimblockconvalt}
\end{figure}


As can be seen from Figure \ref{fig:stimblockconvalt} a similar pattern emerges from the conventional and alternative stimuli. Interestingly, it appears that response times were slower overall in Block 5, which may represent fatigue. However, the order of IAT's was counterbalanced, so one would expect to see the same pattern in the Optimism IAT's if this was the case. 

\begin{figure}
<<optstimblock3, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
optstimblock3.m <- melt(optstimblock3, id.vars="part.opt")
optstimblock3.m[,"part.opt"] <- with(optstimblock3.m, as.factor(part.opt))
names (optstimblock3.m) [3] <- "ResponseTime"
partstim3opt <- ggplot(optstimblock3.m, aes(x=log(ResponseTime), colour=as.factor(part.opt)))+geom_density()+theme(legend.position="none")
optstimblock5.m <- melt(optstimblock5, id.vars="part.opt")
optstimblock5.m[,"part.opt"] <- with(optstimblock5.m, as.factor(part.opt))
names (optstimblock5.m) [3] <- "ResponseTime"
partstim5opt <- ggplot(optstimblock5.m, aes(x=log(ResponseTime), colour=as.factor(part.opt)))+geom_density()+theme(legend.position="none")
print (arrangeGrob (partstim3opt, partstim5opt))
@   
  \caption{Density Plots for Response Times of Each Participant, Block 3 (top) and Block 5 (bottom) All response times are plotted on a log scale}
  \label{fig:optstimblock3part}
\end{figure}

As can be seen from Figure \ref{fig:optstimblock3part}, the distributions for the optimism IAT were relatively similar, except that they were overall shifted towards the right, indicating that response times were generally slower to each of these words. Note that one participant has almost the entirity of their distribution beyond the tails of most of the other participants, which given that this is a log-scale plot, indicates that something is very wrong here. Note additionally that the Block 5 results are much more variable than those from Block 3, which again may represent fatigue. 
\begin{figure}
<<optstimposneg, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
optstimblock3.real <- optstimblock3[,2:8]
optstimreal3.m <- melt(optstimblock3.real)
optstimblock5.real <- optstimblock5[,2:8]
optstimreal5.m <- melt(optstimblock5.real)
optstimreal3.m[,"variable"] <- with(optstimreal3.m, gsub("Block3\\.","",x=variable))
optstimreal5.m[,"variable"] <- with(optstimreal5.m, gsub("Block5\\.","",x=variable))
names(optstimreal3.m)[1] <- "Block3"
names(optstimreal5.m)[1] <- "Block5"

stimreal3.pl <- ggplot(optstimreal3.m, aes(x=Block3, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
stimreal5.pl <- ggplot(optstimreal5.m, aes(x=Block5, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
print(arrangeGrob(stimreal3.pl, stimreal5.pl))
@   
  \caption{Boxplots for Self/Other words in Optimism IAT, Block 3 (top) and Block 5 (bottom).}
  \label{fig:optstimmeyou}
\end{figure}

As can be seen from Figure \ref{fig:optstimmeyou}, the same pattern emerged for the optimism IAT in that the Block 5 scores were much more variable and overall participants responded slower to this block. 



<<optiatplotself, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
optstimblock3.posneg <- optstimblock3[,9:15]
optstimpos3.m <- melt(optstimblock3.posneg)
optstimblock5.posneg <- optstimblock5[,9:15]
optstimpos5.m <- melt(optstimblock5.posneg)
optstimpos3.m[,"variable"] <- with(optstimpos3.m, gsub("Block3\\.","",x=variable))
optstimpos5.m[,"variable"] <- with(optstimpos5.m, gsub("Block5\\.","",x=variable))
names(optstimpos3.m)[1] <- "Block3"
names(optstimpos5.m)[1] <- "Block5"

stimpos3.pl <- ggplot(optstimpos3.m, aes(x=Block3, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
stimpos5.pl <- ggplot(optstimpos5.m, aes(x=Block5, y=value))+geom_boxplot()+theme (axis.text.x=element_text (angle=45, hjust=1))
print(arrangeGrob(stimpos3.pl, stimpos5.pl))
@   
 \caption{Boxplots of Response Times to Positive and Negative Words, Optimism IAT Block 3 (top), Block 5 (bottom).}
 \label{fig:optiatplotpos}
\end{figure}



 As can be seen in Figure \ref{fig:optiatplotself}, the pattern of Block 5 responses tending to be slower was repeated. Note that disimproving appears to be the word with the highest mean latency, which is not surprising given its relatively unfamiliarity (compared to the other words, at least). 
 
 Next, the correlation between the different block times is assessed. As discussed in Chapter \ref{cha:literature-review}, some correlation would be expected given the nature of the IAT task, but the aim here is to quantify what effect, if any it would have had on the results. 
 
 \begin{figure}
<<tcqmeanresp, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
tcqiat.mean.resp <- ddply(tcqiatsorted, .(Block, Participant), summarise, Correlations=mean(BlockTime, na.rm=TRUE))
## tcqmeanresp.pl <- ggplot(tcqiat.mean.resp, aes(x=MeanResponseTime))+geom_histogram+facet_grid(.~Block)
## print(tcqmeanresp.pl)
tcqiat.mean.m <- melt(tcqiat.mean.resp, id.vars=c("Participant", "Block"))
tcq.iat.c <- dcast(tcqiat.mean.m, Participant+...~Block)
tcq.block.corr.pl <- plotmatrix(tcq.iat.c[,3:length(tcq.iat.c)])+geom_smooth(method="lm")
print(tcq.block.corr.pl)
@    
   \caption{Correlations between Block Scores for Treatment Credibility IAT with linear regression smooth line}
   \label{fig:tcqblockcorr}
 \end{figure}

As can be seen from Figure \ref{fig:tcqblockcorr}, the correlations are relatively low between most of the blocks, though somewhat higher between blocks 3 and 5. Table \ref{tab:tcqcormat} gives the exact Kendall\'s $\tau$ between each of the blocks. As can be seen the correlations hover between 0.3 and 0.4, which is in line with expectations prior to the experiment. 

<<cormatrixtcqiat, echo=FALSE, results=tex>>=
cormat <- corr.test(tcq.iat.c[,3:length(tcq.iat.c)], method="kendall")[["r"]]
print(xtable(cormat, label="tab:tcqcormat", caption="Correlations between the blocks of the treatment credibility IAT (Kendalls $\tau$. All correlations are significant at the p<0.001 level"))
@ 
 

Next, the same process is repeated for the Optimism IAT. 

<<tcqmeanresp, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
optiat.mean.resp <- ddply(optiatsorted, .(Block, Participant), summarise, Correlations=mean(BlockTime, na.rm=TRUE))
## optmeanresp.pl <- ggplot(optiat.mean.resp, aes(x=MeanResponseTime))+geom_histogram+facet_grid(.~Block)
## print(optmeanresp.pl)
optiat.mean.m <- melt(optiat.mean.resp, id.vars=c("Participant", "Block"))
opt.iat.c <- dcast(optiat.mean.m, Participant+...~Block)
opt.block.corr.pl <- plotmatrix(opt.iat.c[,3:length(opt.iat.c)])+geom_smooth(method="lm")
print(opt.block.corr.pl)
@    
   \caption{Correlations between Block Scores for Optimism IAT with linear regression smooth line}
   \label{fig:optblockcorr}
 \end{figure}

As shown in Figure \ref{fig:optblockcorr}, the correlations between blocks are moderate, though highest in blocks 3 and 5, as was seen for the Treatment Credibility IAT. Table \ref{tab:optcormat}. The correlations are a little higher than for the Treatment Credibility IAT, but still within an acceptable range. Its interesting to note that (with the exception of Block 5), the correlations are strongest between adjacent blocks, and drop off as the blocks move further apart, suggesting that the autocorrelation theory has some merit. 

<<cormatrixtcqiat, echo=FALSE, results=tex>>=
opt.cormat <- corr.test(opt.iat.c[,3:length(tcq.iat.c)], method="kendall")[["r"]]
print(xtable(opt.cormat, label="tab:optcormat", caption="Correlations between the blocks of the Optimism IAT (Kendalls $\tau$. All correlations are significant at the p<  0.001 level"))
@ 

The next question with regard to the IAT's is whether or not the non-critical blocks (that is, Blocks 1, 2 and 4) will be correlated. Given that these were administered in counterbalanced order and there was a small gap between them one would expect there to be much lower correlations between these blocks of the IAT's. These correlations (if present) should provide an index of general processing speed, and may be useful as predictor variables for some of the other measures. 

<<opttcqiatcorr, echo=FALSE, results=hide>>=
curnames <- names(opt.iat.c)
curnames.bl <- curnames[3:length(curnames)]
curnames.bl2 <- paste("Opt", curnames.bl, sep="")
curnames.d <- c(curnames[1:2], curnames.bl2)
names(opt.iat.c) <- curnames.d
curnames.tcq <- names(tcq.iat.c)
curnames.bl.tcq <- curnames[3:length(curnames)]
curnames.bl2.tcq <- paste("TCQ", curnames.bl.tcq, sep="")
curnames.d.tcq <- c(curnames.tcq[1:2], curnames.bl2.tcq)
names(tcq.iat.c) <- curnames.d.tcq
iat.block.merge <- merge(tcq.iat.c, opt.iat.c, by="Participant")
iat.block.merge2 <- iat.block.merge[, -1*c(2, 5,7,8,11,13)]
@ 

\begin{figure}
<<corriattcqopt, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
corr.iat.tcq.opt.pl <- plotmatrix(iat.block.merge2[,2:length(iat.block.merge2)])+geom_smooth(method="lm")+geom_smooth(method="lm")
print(corr.iat.tcq.opt.pl)
@   
  \caption{Correlations between Non Critical Blocks of Optimism and Treatment Credibility IAT}
  \label{fig:corriattcqopt}
\end{figure}

As can be seen from Figure \ref{fig:corriattcqopt}, there were correlations between the two IAT's. These correlations, while significant, were quite low ($r\bar =0.20$) which equates to about 4\% of the variance. Therefore the two implicit measures can be safely be regarded as not being contaminated by method variance . 



<<selfreportcleanup, echo=FALSE, results=hide>>=
names(vasscores)[1] <-  "Participant"
names(expmeasures)[1] <- "Participant"
vasscores.test <- vasscores[,1:8]
expmeasurescomp <- merge(vasscores.test, expmeasures)
Iatandexpmeasures <- merge(expmeasurescomp, IATscores, by="Participant")
iatexp <- Iatandexpmeasures[,14:23]
Iatandexpmeasures[,"meanconv"] <- with(Iatandexpmeasures, (Pill+Cream+Inj)/3)
Iatandexpmeasures[,"meanalt"] <- with(Iatandexpmeasures, (Acu+Hom+Rei)/3)
Iatandexpmeasures[,"convaltcomp"] <- with(Iatandexpmeasures, meanconv -meanalt)
## Iatandexpmeasures[,"Date"] <- with(Iatandexpmeasures, dmy(Date))
@
 
Next, the relationship between overall response time in each block (total time to complete the block, including interstimulus intervals) was examined in terms of the demographic variables. 

<<optiatdemo, echo=FALSE, results=tex>>=
optiatdemographics <- merge(expmeasures, optiatsorted, by="Participant")
opt.demo <- xtable(summary(lm(BlockTime~Block+Age+Gender, data=optiatdemographics)), label="tab:optblocktimedemo", caption="Summary of Linear Regression of Block Time by Block, Age and Gender")
print(opt.demo)
@  
As can be seen from Table \ref{tab:optblocktimedemo}, the major influence on Block Time comes from Block, which is as expected given that Blocks 3 and 5 had three times as many trials as the other blocks. However, there is also an effect of gender, with males tending to respond somewhat quicker than females. This is interesting, as there are typically no gender based effects on IAT's (except for those which measure gender attitudes). 

<<tcqiatdemo, echo=FALSE, results=tex>>=
tcqiatdemographics <- merge(expmeasures, tcqiatsorted, by="Participant")
tcq.demo <- xtable(summary(lm(BlockTime~Block+Age+Gender, data=optiatdemographics)), caption="Summary of Linear Regression on Age, Gender and Individual Block Times.", label="tab:tcqblocktimedemo" )
print(tcq.demo)
@ 

\begin{figure}
<<tcqiatgender, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
tcqiatgender <- ggplot(na.omit(Iatandexpmeasures), aes(x=Gender, y=TCQIAT.Mean))+geom_boxplot()
print(tcqiatgender)
@   
  \caption{Treatment Credibility IAT Scores by Gender}
  \label{fig:tcqiatgend}
\end{figure}



As can be seen from Figure \ref{fig:tcqiatgend} above, there were no significant differences ($t=-0.4973, p=0.6211$) between men and women in  the sample with regards to their scores on the Treatment Credibility Questionnaire. However, the variance was much higher for men, which was a pattern replicated in previous research into Treatment Credibility (using a self report instrument described in Chapter \ref{cha:tcq-thesis}).

\begin{figure}
<<optiatgender, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
optiatgender <- ggplot(na.omit(Iatandexpmeasures), aes(x=Gender, y=OptIAT.Mean))+geom_boxplot()
print(optiatgender)
@   
  \caption{Optimism IAT by Gender}
  \label{fig:optiatgend}
\end{figure}

 As can be seen from Figure \ref{fig:optiatgend}, there were no significant differences ($t=-0.8234, df=49.761, p=0.4142$) between men and women with regards to their score on the Optimism IAT. 

<<ordtestiatprep, echo=FALSE, results=hide>>=
part.opt <- unique(optiat[,"Participant"])
part.tcq <- unique(tcqiat[,"Participant"])
optblock3 <-  optstimblock3 
optblock5 <- optstimblock5
tcqblock3 <- tcqstimblock3
tcqblock5 <- tcqstimblock5
optiat.diff <- iatDiff(optblock3, optblock5)
tcqiat.diff <- iatDiff(tcqblock3, tcqblock5)

optdiffs.dich <- apply(optiat.diff, c(1,2), function (x) ifelse(x>0, 1, 0))
tcqdiffs.dich <- apply(tcqiat.diff, c(1,2), function (x) ifelse(x>0, 1, 0))
@ 
<<optiatirt, echo=FALSE, results=hide, cache=TRUE>>=
optiat.rasch <- RM(na.omit(optdiffs.dich))
optiat.ppar <- person.parameter(optiat.rasch)
optiat.elim <- stepwiseIt(optiat.rasch)
@ 

\begin{figure}
<<optpimap, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=

print(plotPImap(optiat.rasch))
@   
  \caption{Person Item Map of Difficulty parameters for Optimism IAT}
  \label{fig:optpimap}
\end{figure}


<<optiatrachprint, echo=FALSE, results=tex>>=
optrasch.df <- with(optiat.rasch, as.data.frame(cbind(etapar, se.eta, betapar, se.beta)))
print(xtable(optrasch.df, label="tab:optrasch", caption="Ability Estimates for Rasch Model Analysis of Optimism IAT"))
@ 

Table \ref{tab:optrasch} shows the estimated parameters for a Rasch model of the optimism IAT stimuli. Figure \ref{fig:optpimap} shows the Person Item Map for this model. This map shows that most of the stimuli were equivalent in difficulty, which indicates that this IAT is suitable for measuring implicit optimism in the general population, however, more discriminating stimuli would be necessary if the instrument was to be used in a clinical sample. Additionally, the map shows that the majority of participants showed latent traits of less than zero, suggesting that implicit optimism is rarer than explicit optimism. 


A process of stepwise elimination was carried out to eliminate items which did not fit the model. In this case, the stimulus ``Myself'' was the only one which had significant model misfit. 

<<tcqiatirt, echo=FALSE, results=hide, cache=TRUE>>=
tcqiat.rasch <- RM(tcqdiffs.dich)

tcqiat.elim <- stepwiseIt(tcqiat.rasch)
tcqiat.ppar <- person.parameter(tcqiat.rasch)
@ 

<<tcqiatability, echo=FALSE, results=tex>>=
tcqrasch.df <- with(tcqiat.rasch, as.data.frame(cbind(etapar, se.eta, betapar, se.beta)))
print(xtable(tcqrasch.df, label="tab:tcqiatrasch", caption="Ability Estimates for Rasch Model of Treatment Credibility IAT"))
@ 

\begin{figure}
<<tcqiatpiplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
sink("tmp.txt")
print(plotPImap(tcqiat.rasch))
sink(NULL)
## print(tcqiat.piplot)
@   
  \caption{Person Item Difficulty Map, Treatment Credibility IAT}
  \label{fig:tcqpimap}
\end{figure}

Table \ref{tab:tcqiatrasch}  shows the estimated abilities and their associated standard errors for the Treatment Credibility IAT. Figure \ref{fig:tcqpimap} the estimates of person and item abilities can be seen in graphical form. Again, it can be seen from Figure \ref{fig:tcqpimap} that a majority of the sample had latent traits of less than zero, but that the item estimates of difficulty were approximately equal.


\subsection{Implicit-Explicit Relationships}
\label{sec:impl-expl-relat}

Next, the relationships between the explicit and implicit measures were examined. 

<<corrtestimplexpl, echo=FALSE, results=tex>>=
corr.imp.exp <- corr.test(Iatandexpmeasures[,14:25], method="kendall")[["r"]]
print(xtable(corr.imp.exp, label="tab:corrimpexp", caption="Correlations between Implicit and Explicit Measures"), scalebox=0.7)
@ 

It can be seen from Table \ref{tab:corrimpexp} that the LOTR was only really correlated with the Acupuncture items and with the MAAS, the Conventional Treatment scales correlated within themselves, as did the Alternative treatment scales, while the two IAT measures showed no appreciable correlations with each other. The relationships between the IAT's and explicit measures were small, and surprisingly in the unpredicted direction (negative). Another surprise was that the direction of the correlation between the LOT-R and the MAAS was opposite to that observed in prior research. Possible reasons for these results are considered in the Discussion.  


\subsection{Explicit Measures}
\label{sec:explicit-measures}


\begin{figure}
<<lotrmaas, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
lotrmaas <- ggplot(Iatandexpmeasures, aes(x=LOTR, y=MAAS))+geom_point()+geom_smooth(method="lm")+facet_grid(.~Condition)
print(lotrmaas)
@   
  \caption{Scatterplot of LOT-R Scores by Condition, with a linear regression smooth}
  \label{fig:lotrmaas}
\end{figure}


The plot above in Figure \ref{fig:lotrmaas} shows that optimism and mindfulness were positively correlated with one another. Additionally this correlation appeared to be relatively stable across condition, though it appeared a little weaker in the Deceptive Placebo Group.  This is in contrast to the results found in a much larger scale study carried out earlier in the research. Note that one plausible explanation for this effect is that, in the experiment, the measures were administered in the opposite order - Optimism, followed by Mindfulness. It is possible that the completion of the mindfulness measure affected the way in which participants approached the Optimism measure. This theory is more fully discussed in Chapter \ref{cha:general-discussion}. 


\subsection{Relationships between Experimental Samples and Survey Samples}
\label{sec:relat-betw-exper}

Given the focus of this thesis on the integration of survey and experimental research, the next step was to examine the differences and similarities between the samples collected from the general population via survey and the experimental sample.


\begin{figure}
<<surveyoptimism, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
survey.opt.pl <- ggplot(hom.data, aes(x=optimism))+geom_density()
exp.opt.pl <- ggplot(expmeasures, aes(x=LOTR))+geom_density()

print(arrangeGrob ( survey.opt.pl, exp.opt.pl))
@   
  \caption{Density Plot of Optimism Scores in the Survey samples (top), and the experimental sample (bottom) }
  \label{fig:compoptimism}
\end{figure}


In Figure \ref{fig:compoptimism}  can be seen that the two plots are extremely different, with a much higher average optimism score in the experimental sample. To some extent, this is not unexpected given that the study was described as an investigation of painkilling drugs and there was an opportunity to win a smart-phone, so perhaps students with higher levels of optimism were more likely to agree to participate. 



Next, the differences in mindfulness levels between the two samples were assessed. 

\begin{figure} 
<<surveymindfulness, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
surv.mind.pl <- ggplot(hom.data, aes(x=mindfulness))+geom_density()
exp.mind.pl <- ggplot(Iatandexpmeasures, aes(x=MAAS))+geom_density()
print(arrangeGrob ( surv.mind.pl, exp.mind.pl))
@  
  \caption{Density Plot for Mindfulness Scores, Survey Sample (top), and Experimental Sample (bottom)}
  \label{fig:compmind}
\end{figure}

As can be seen from Figure \ref{fig:compmind}, the pattern was quite different for mindfulness levels (as measured by the MAAS) as the levels of mindfulness were higheer in the survey sample. Again, this may be due to the association of mindfulness with introversion, as introverts may have been less likely to respond to the email invitiation(s) to take part in the study. 


Finally, the treatment credibility questionnaire scores were examined to assess the differences between the survey and experimental samples. 

First, the differences between the two samples in terms of Pill credibility were examined. 

\begin{figure}
<<surveypill, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
surv.pill.pl <- ggplot(credtotals, aes(x=Pilltot))+geom_density()
exp.pill.pl <- ggplot(Iatandexpmeasures, aes(x=Pill))+geom_density()
print(arrangeGrob ( surv.pill.pl, exp.pill.pl))
@   
  \caption{Pill Credibility Density Plot, Survey Sample (sample 2) (top), and Experimental Sample (bottom)}
  \label{fig:comppill}
\end{figure}


As can be seen from Figure \ref{fig:comppill}, the general population sample was higher peaked, with less variation around the peak than was the experimental sample. In fact, the experimental sample seemed to be more variable than the survey sample, which could either be due to a true difference in the distributions or due to a greater uncertainty in the experimental sample due to the smaller sample size. 

Next, the difference between Cream Credibility scores were assessed. 

\begin{figure}
<<surveycream, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
surv.cream.pl <- ggplot(credtotals, aes(x=Creamtot))+geom_density()
exp.cream.pl <- ggplot(Iatandexpmeasures, aes(x=Cream))+geom_density()
print(arrangeGrob (surv.cream.pl, exp.cream.pl))
@   
  \caption{Density Plot for Distribution of Cream Credibility Scores, Survey Sample (sample 2) (top) and Experimental Cream Credibility Totals (bottom)}
  \label{fig:compcream}
\end{figure}



As shown in Figure \ref{fig:compcream}, the survey group tended to have a more positive view of painkilling creams. While the survey group is strongly peaked at the right of the plot, the experimental group were more evenly distributed, with a peak at the centre of the plot. This is interesting, as one might expect the experimental group to be more positive towards painkilling treatments in general, given that they had agreed to take part in  a study which examined the effects of a new analgesic.

Next, the credibility scores for injection painkilling treatments were examined between the survey and experimental groups. 

\begin{figure}
<<surveyinj, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
surv.inj.pl <- ggplot(credtotals, aes(x=Injtot))+geom_density()
exp.inj.pl <- ggplot(Iatandexpmeasures, aes(x=Inj))+geom_density()
print(arrangeGrob ( surv.inj.pl, exp.inj.pl))
@   
  \caption{Injection Credibility Density Plot, Survey Sample (sample two) (top), Experimental Sample (bottom)}
  \label{fig:compinj}
\end{figure}


Figure \ref{fig:compinj}  show that the Injection credibility scores were almost identical in their distributions between the two samples. 

Next, the Alternative treatment scores were examined between the two samples.
\begin{figure}
  
<<alttreatmentssurvey, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
surv.acu.pl <- ggplot(credtotals, aes(x=Acutot))+geom_density()
exp.acu.pl <- ggplot(Iatandexpmeasures, aes(x=Acu))+geom_density()
print(arrangeGrob ( surv.acu.pl, exp.acu.pl))
@   \caption{Density Plots for Acupuncture Scores, Survey sample (top) and Experimental Sample (bottom)}
  \label{fig:compacu}
\end{figure}


Figure ref{fig:compacu} shows that Acupuncture levels in the experimental sample were a little lower than those in the survey sample. 

\begin{figure}
  
<<alttreatmentssurvey, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
surv.hom.pl <- ggplot(credtotals, aes(x=Homtot))+geom_density()
exp.hom.pl <- ggplot(Iatandexpmeasures, aes(x=Hom))+geom_density()
print(arrangeGrob ( surv.hom.pl, exp.hom.pl))
@   \caption{Density Plots for Hompuncture Scores, Survey sample (top) and Experimental Sample (bottom)}
  \label{fig:comphom}
\end{figure}

Figure \ref{fig:comphom} shows the levels of homeopathy credibility in both survey and experimental samples. It can be seen that, contrary to the Acupuncture totals, the credibility scores for Homeopathy were slightly higher in the experimental sample than in the survey sample. 

\begin{figure}
  
<<alttreatmentssurvey, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
surv.rei.pl <- ggplot(credtotals, aes(x=Reitot))+geom_density()
exp.rei.pl <- ggplot(Iatandexpmeasures, aes(x=Rei))+geom_density()
print(arrangeGrob ( surv.rei.pl, exp.rei.pl))
@   \caption{Density Plots for Reipuncture Scores, Survey sample (top) and Experimental Sample (bottom)}
  \label{fig:comprei}
\end{figure}

Figure \ref{fig:comprei} shows the credibility totals for Reiki in the survey and experimental samples. While credibility totals were quite low in both samples, they were a little lower in the survey sample. 


\subsection{Randomisation Checks}
\label{sec:randomisation-checks}

Next, the comparability of the groups were assessed to ensure that the randomisation process had proved effective. 

<<randcheck1, echo=FALSE, results=tex>>=
tcq.xtab <- xtable(summary(aov(TCQIAT.Mean~Condition, data=Iatandexpmeasures)), label="tab:tcqiatcheck", caption="Summary of Anova of Treatment Credibility Scores by Condition")
opt.xtab <- xtable(summary(aov(OptIAT.Mean~Condition, data=Iatandexpmeasures)), label="tab:optiatcheck", caption="Summary of ANOVA of Optimism IAT scores by Condition")
conv.xtab <- xtable(summary(aov(meanconv~Condition, data=Iatandexpmeasures)))
alt.xtab <- xtable(summary(aov(meanalt~Condition, data=Iatandexpmeasures)))
chisq.gend <- with(Iatandexpmeasures, chisq.test(table(Gender, Condition)))
chisq.gend <- with(Iatandexpmeasures, chisq.test(table(Prime, Condition)))
@ 

<<tcqcheckprint, echo=FALSE, results=tex>>=
print(tcq.xtab)
@ 

<<optcheckprint, echo=FALSE, results=tex>>=
print(opt.xtab)
@ 

As can be seen from Table \ref{tab:tcqiatcheck} and Table \ref{tab:optiatcheck} the scores on the treatment credibility IAT or the optimism IAT did not differ by Condition. 

The results of a chi-square test showed that the gender of participants in each condition were equivalent, (p=\Sexpr{round ( chisq.gend[["p.value"]], 3)}.
In) addition, the priming manipulation was not significantly different across groups.



<<chisqcondition, echo=FALSE, results=tex>>=
Subsetiatandexp <- Iatandexpmeasures[with(Iatandexpmeasures,Condition!="No Treatment"),]
Subsetiatandexp <- droplevels(Subsetiatandexp)
chisq2 <- with(Subsetiatandexp, chisq.test(table(PlacResp, Condition)))
chisqtest2 <- as.data.frame(cbind(chisq2[["statistic"]], chisq2[["parameter"]], chisq2[["p.value"]], chisq2[["observed"]]))
names(chisqtest2)[1] <- "Chi Square"
names(chisqtest2)[2] <- "df"
names(chisqtest2)[3] <- "p Value"
chisq2.xtab <- xtable(chisqtest2)
print(chisq2.xtab)
@ 

As can be seen, there is no significant effect of condition on placebo response. However, as can also be seen from the table, this is not due to a lower than expected placebo response in the Deceptive Placebo group, but rather due to a higher than expected placebo response in the Open Placebo group. The next step was to examine if this result could be accounted for by the priming mechanism.

<<chisqtestprimeplac, echo=FALSE, results=tex>>=
chisq1 <- with(Iatandexpmeasures, chisq.test(table(Prime, Condition, PlacResp)))
chisqtest1 <- as.data.frame(cbind(chisq1[["statistic"]], chisq1[["parameter"]], chisq1[["p.value"]], chisq1[["observed"]]))
names(chisqtest1)[1] <- "Chi Square"
names(chisqtest1)[2] <- "df"
names(chisqtest1)[3] <- "p Value"
chisq1.xtab <- xtable(chisqtest1, label="tab:primeplacchi", caption="Chi Square for Relationship between Priming, Condition and Placebo Response")
print(chisq1.xtab)
@ 

In Table \ref{tab:primeplacchi} above, it can be seen that priming appears to be the driver of this effect, as the Placebo Response by Condition is extremely significant given the priming manipulation. Most of this improvement occurred in the Open Placebo group, which is an extremely interesting finding, one not reported in the literature before.  

This finding is much more clearly conveyed in Figure \ref{fig:placprimeplot}, where it can be seen that Participants were much more likely to respond to placebo following a priming intervention. Given that priming interventions typically take place outside conscious awareness, this suggests that there is at least some part of the placebo response which is amenable to non-conscious (or implicit) influences. 

\begin{figure}
<<placprimeplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
plac.prime.pl <- ggplot(Iatandexpmeasures, aes(x=PlacResp))+geom_histogram()+facet_grid(.~Prime)
print(plac.prime.pl)
@   
  \caption{Proportion of Placebo Response in Primed and Non-Primed Conditions}
  \label{fig:placprimeplot}
\end{figure}

\begin{figure}
<<pairsplot, echo=FALSE, Fig=TRUE>>=
print(plotmatrix(iatexp)+geom_smooth(method="lm")+geom_jitter())
@   
  \caption{Scatterplots and Density Plots for Self Report and Implicit Measures}
  \label{fig:pairsplotimpexp}
\end{figure}

In Figure \ref{fig:pairsplotimpexp} the relationships between the self report and implicit measures are shown. It can be seen that the credibility scores break into conventional and alternative groups, while the LOT-R and MAAS do not correlate hugely with any of the other measures (but do correlate quite well with themselves, as noted above).  


\subsection{Pain Ratings}
\label{sec:pain-ratings}

<<survivaldata, echo=FALSE, results=hide>>=
painratings <- vasscores[,c(9:53)]
napainratings <- apply(painratings,1, function (x)  sum(is.na(x)))
lengthsurv <- 46-napainratings
Iatandexpmeasures[,"lengthsurv"] <- lengthsurv
censor <- Iatandexpmeasures[,"Censored"]
censor2 <- ifelse(censor==c("No", "Left"), 1, 0)
Surv.data <- Surv(lengthsurv,censor2)
@ 




<<paints, echo=FALSE, results=hide>>=
painratings <- vasscores[,c(1,2,8:53)]
painratings.temp <- painratings[,2:length(painratings)]
painratings.trans <- t(painratings.temp)
colnames(painratings.trans) <- as.character(t(painratings[,1]))
painratings.trans <- as.data.frame(painratings.trans)
painratings.trans[,"Time"] <- 1:nrow(painratings.trans)
pain.cond <- ddply(painratings, .(Condition), summarise, PainRatings=apply(painratings[,4:48], 2, mean, na.rm=TRUE))
pain.cond.m <- melt(pain.cond, id.vars="Condition")
@ 







\subsection{Analysis of Pain Ratings}
 
<<meanpaingroup, echo=FALSE, results=hide>>=
deceptivepain <- painratings[with(painratings,Condition=="Treatment"),]
placebopain <- painratings[with(painratings,Condition=="Placebo"),]
notreatpain <- painratings[with(painratings,Condition=="No Treatment"),]
meandeceptivepain <- apply(deceptivepain[,4:48], 2, mean, na.rm=TRUE)
meanplacebopain <- apply(placebopain[,4:48], 2, mean, na.rm=TRUE)
meannotreatpain <- apply(notreatpain[,4:48], 2, mean, na.rm=TRUE)
meangrouppainratings <- cbind(meandeceptivepain, meanplacebopain, meannotreatpain)
meangrouppainratings <- as.data.frame(meangrouppainratings)
meangrouppainratings[,"Time"] <- 1:45
meddeceptivepain <- apply(deceptivepain[,4:48], 2, median, na.rm=TRUE)
medplacebopain <- apply(placebopain[,4:48], 2, median, na.rm=TRUE)
mednotreatpain <- apply(notreatpain[,4:48], 2, median, na.rm=TRUE)
medpainratings <- as.data.frame(cbind(meddeceptivepain, medplacebopain, mednotreatpain))
medpainratings[,"Time"] <- 1:45
dec.pain.mean <- apply(deceptivepain[,4:48], 2, mean, na.rm=TRUE)
open.pain.mean <- apply(placebopain[,4:48], 2, mean, na.rm=TRUE)
notreat.pain.mean <- apply(notreatpain[,4:48], 2, mean, na.rm=TRUE)

painbycond <- data.frame(Deceptive=dec.pain.mean, Open=open.pain.mean, NoTreat=notreat.pain.mean)
painbycond[,"Time"] <- 1:45
painbycond.m <- melt(painbycond, id.vars="Time")
@ 

\begin{figure}
<<tsmeanpainplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
meanpain.melt <- melt(meangrouppainratings, id="Time")
tspainplot <- ggplot(meanpain.melt, aes(x=Time, y=value, label=variable, colour=variable))+geom_smooth(span=0.2)
print(tspainplot)
@   
  \caption{Plot of Pain Responses by Condition Over Time}
  \label{fig:tsmeanpainplot}
\end{figure}



As can be seen from Figure \ref{fig:tsmeanpainplot}, the placebo group tended to report lower mean pain ratings across time. This was an extremely unexpected finding, and therefore the pain ratings were reanalysed using medians, to lessen the effects that outliers could be having on the results. The curves shown above used a locally weighted smoother (loess, span=0.2) to create the lines, given the substantial non linearity of the results. However, this plot does show that there was a significant difference between the placebo group and the two other conditions. 

\begin{figure}
<<medtspainplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
medpain.melt <- melt(medpainratings, id="Time")
medtspainplot <- ggplot(medpain.melt, aes(x=Time, y=value, group=variable, colour=variable))+geom_smooth(span=0.5)
print(medtspainplot)
@   
  \caption{Median Pain Ratings Over Time by Condition}
  \label{fig:medpainplot}
\end{figure}


As can be seen from Figure \ref{fig:medpainplot}, the results of the median pain ratings by group show exactly the same pattern as the mean pain ratings. This suggests that there is a real difference here, one that warrants further explanation. Again, a locally weighted smoother (loess, span=0.5) was used to fit the curves. 


\begin{figure}
<<medianpaintscorrelationplots, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
par(mfrow=c(2,2))
print(acf(na.omit(with(medpainratings,meddeceptivepain))))
print(acf(na.omit(with(medpainratings,medplacebopain))))
print(acf(na.omit(with(medpainratings,mednotreatpain))), main="Autocorrelation No Treatment Group")
@   
  \caption{Autocorrelation Plots for Median Pain Responses over Time by Condition}
  \label{fig:autocorrpainplot}
\end{figure}


As can be seen above, the autocorrelation plots for the three groups appear to be similiar, which means that a the same ARIMA model can be fit to them. The first three differences are significant, and a process of ARIMA model fitting indicates that an ARIMA(1,3,1) model has the best AIC and likelihood. This is information that needs to be incorportated into an overall model which will be fit to the data.

\begin{figure}
<<meanpaintscorrelationplots, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
plot.new()
par (mfrow=c (2,2))
sink ("tmp.txt")
print(acf(na.omit(with(meangrouppainratings, meannotreatpain))))
print(acf(na.omit(with(meangrouppainratings,  meandeceptivepain))))
print(acf(na.omit(with(meangrouppainratings,meanplacebopain))))
sink (NULL)
@   
  \caption{Mean Autocorrelation plots for pain ratings over time}
  \label{fig:meanautocorrplot}
\end{figure}


The mean pain ratings were also examined for autocorrelations, and as shown in Figure \ref{fig:meanautocorrplot}, the results were exactly the same as for the median pain ratings, indicating an ARIMA(1,3,1) model was the best fit for the data.


\subsection{Variables impacting the placebo response}
\label{sec:vari-impact-plac}
In this section, the covariates associated with response to placebo in this sample are examined, first graphically and then through a process of formal model fitting. 

<<tsimport, echo=FALSE, results=hide>>=
physfiles <- fileImport("ExperimentDataforR/FullStudy/PhysMeasures/Richieoutput", pattern=".txt$")
gsr.mat <- listToDf(physfiles, 1)

ecg.mat <- listToDf(physfiles, 2)
gsrnames <- colnames(gsr.mat)
ecgnames <- colnames(ecg.mat)
names(gsr.mat) <- NULL
gsr.mat.df <- as.data.frame(gsr.mat) #plot these, look at mean gsr scores as a predictor of placebo use variances also
gsr.mean <- colSums(gsr.mat.df, na.rm=TRUE)
gsr.mean2 <- as.data.frame(gsr.mean)
gsrnames2 <- gsub("GSR", "", gsrnames)
gsr.mean2[,"Participant"] <- gsrnames2
gsr.mat.t <- t(gsr.mat)

names(ecg.mat) <- NULL
ecg.mat.t <- t(ecg.mat)
gsr.df <- as.data.frame(gsr.mat.t)

gsr.df[,"Participant"] <- gsrnames2
Iatandexpmeasures.phys <- merge(Iatandexpmeasures, gsr.mean2, by="Participant")
@


\begin{figure}
<<ggplothistcond, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
plac.resp.hist.pl <- ggplot(Iatandexpmeasures, aes(x=TCQIAT.Mean))+geom_density()+facet_grid(.~PlacResp)
print(plac.resp.hist.pl)
@   
  \caption{TCQIAT.Mean against Pain Responses Over Time}
  \label{fig:histresp}
\end{figure}


From Figure \ref{fig:histresp}, it can be seen that the participants who did respond to placebo had marginally higher Treatment Credibility IAT scores than those who did not. 






\begin{figure}
<<optiatcorrsurv, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
optiat.histcond <- ggplot(Iatandexpmeasures, aes(x=OptIAT.Mean))+geom_density()+facet_grid(.~PlacResp)
print(optiat.histcond)
@   
  \caption{Density Plots of Optimism IAT Scores by Condition}
  \label{fig:opthistcond}
\end{figure}


Again, it can be seen from Figure \ref{fig:opthistcond} that those who responded to placebo had higher optimism IAT scores than  those who did not, suggesting that something about the IAT is predictive of placebo response. 

\begin{figure}
<<creamhistcond, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
cream.hist.cond <- ggplot(Iatandexpmeasures, aes(x=Cream))+geom_density()+facet_grid(.~PlacResp)
print(cream.hist.cond)
@   
  \caption{Desnity Plot for Mean Cream Credibility Scores by Condition}
  \label{fig:creamhistcond}
\end{figure}


Figure  \ref{fig:creamhistcond} shows that there was a difference in the mean cream credibiliuty scores by whether or not a participant responded to placebo, but the difference was not significant ($t=-0.9545, df=53.766, p=0.3441$). 

\begin{figure}
<<ggplotplacyesno, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
placrespyes <- painratings[with(painratings,PlacResp=="Yes"),]
placrespno <- painratings[with(painratings,PlacResp=="No"),]
placresyesmean <- apply(placrespyes[4:48], 2, mean, na.rm=TRUE)
placresnomean <- apply(placrespno[4:48], 2, mean, na.rm=TRUE)
placresyesnopain <- as.data.frame(cbind(placresyesmean, placresnomean))
placresyesnopain$Time <- 1:45
placres.melt <- melt(placresyesnopain, id="Time")
placresplot <- ggplot(placres.melt, aes(x=Time, y=value, group=variable, colour=variable))+geom_line() +geom_smooth(method="loess")
print(placresplot)
@   
  \caption{Pain Ratings of Participants by Response to Placebo Across Time. Straight line is a loess smoother, the jagged line represents the actual pain levels}
  \label{fig:placyesno}
\end{figure}


A number of findings are apparent from the plot above in Figure \ref{fig:placyesno}. The placebo effect was approximately equivalent to a 15\% decrease in pain (read from the graph at the point the no response participants pain reached seven). This is a relatively large effect, and adds confidence to the significant results for modelling reported below. In addition, the participants who responded to placebo tended to remain in the experiment for a longer period of time (which is intuitively obvious). Below, formal model testing for the major hypotheses takes place. 

\subsection{Logistic Regressions on Placebo Response}
\label{sec:logist-regr-plac}
In order to examine whether or not the IAT scores were predictive of placebo response, a logistic regression model was used. Logistic regression was chosen for this as the response outcome was binary, and this method extends the linear regression model for binary outcomes. 


<<placmod1, echo=FALSE, results=tex>>=
placmod1 <- glm(PlacResp~TCQIAT.Mean, data=Iatandexpmeasures, family=binomial(link="logit"))
placmod1.xtab <- xtable(summary(placmod1),label="tab:placmod1", caption="Logistic Regression of Treatment Credibility IAT on Placebo Response")
print(placmod1.xtab)
@ 

As can be seen above in Table \ref{tab:placmod1}, the treatment credibility IAT was not a significant independent predictor of placebo response.

<<placmod2, echo=FALSE, results=tex>>=
placmod2 <- glm(PlacResp~OptIAT.Mean, data=Iatandexpmeasures, family=binomial(link="logit"))
placmod2.xtab <- xtable(summary(placmod2), label="tab:placmod2", caption="Regression of Optimism IAT on Placebo Response")
print(placmod2.xtab)
@ 

As shown in Table \ref{tab:placmod2} the Optimism IAT is not an independent significant predictor of placebo response either. 

<<placebolotr, echo=FALSE, results=tex>>=
placmod.lotr<- glm(PlacResp~LOTR, data=Iatandexpmeasures, family=binomial(link="logit"))
placmod.lotr.xtab <- xtable(summary(placmod.lotr), label="tab:placebolotr", caption="Regression of LOT-R Scores on Placebo Response")
print(placmod.lotr.xtab)
@ 

In addition, the Life Orientation scores are not a significant predictor of placebo response either, as shown in Table \ref{tab:placebolotr}.

However, when these three variables are placed into the model together, the result is significant.

<<placeboglm, echo=FALSE, results=tex>>=
placmod6<- glm(PlacResp~TCQIAT.Mean*OptIAT.Mean*LOTR, data=Iatandexpmeasures, family=binomial(link="logit"))
placmod.xtab <- xtable(summary(placmod6), label="tab:placmodinter", caption="Regression of TCQIAT, OptIAT and LOT-R on Placebo Response")
print(placmod.xtab)
@ 

As can be seen from Table \ref{tab:placmodinter} when interactions were allowed and the Life Orientation Test was added to the model, both IAT measures were significantly associated with placebo response, and all of the three variables interactions were also significant. 

In addition, this model explains approximately 27\% of the variance in placebo response (using a pseudo R-square of 1-(residual deviance/null deviance)).



\begin{figure}
  
<<ggplot3wayinter, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
iatexpno <- Iatandexpmeasures[with(Iatandexpmeasures,PlacResp=="No"),]
iatexpyes <- Iatandexpmeasures[with(Iatandexpmeasures,PlacResp=="Yes"),]
iatexpno <- na.omit(iatexpno)
iatexpyes <- na.omit(iatexpyes)
iatexpyesno <- rbind(iatexpyes, iatexpno)
lotriatplot <- ggplot(iatexpyesno, aes(x=LOTR, y=TCQIAT.Mean , colour=PlacResp, size=OptIAT.Mean))+geom_point()
print(lotriatplot)
@   
  \caption{Scatterplot of Optimism IAT, Treatment Credibility and LOT-R scores against mean survival time by Condition}
  \label{fig:3wayinter}
\end{figure}

 The plot above in Figure \ref{fig:3wayinter} indicates that there appears to be a non linear interaction between the three variables included in our final model. It can be seen that extremely high scores on both the treatment credibility questionnaire and the Life Orientation test appear to be associated with not responding to placebo, while moderate levels of all three variables appear to provide the greatest likelihood of placebo response. 
 \begin{figure}
<<lotrtcqsmoothplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
lotrtcqplot <- ggplot(iatexpyesno, aes(x=LOTR, y=TCQIAT.Mean , colour=PlacResp))+geom_point()
print(lotrtcqplot)
@     
   \caption{Scatterplot of LOT-R Scores against TCQ IAT Scores, Placebo Response is denoted using colour}
   \label{fig:lotrtcplot}
 \end{figure}


This is perhaps clearer in Figure \ref{fig:lotrtcqplot} where it can be seen that the majority of placebo response occurs at median levels of both self reported optimism and implicit treatment credibility. 

\begin{figure}
<<lotroptplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
lotroptplot <- ggplot(iatexpyesno, aes(x=LOTR, y=OptIAT.Mean , colour=PlacResp))+geom_point()
print(lotroptplot)
@   
  \caption{LOTR Scores against Optimism Scores, colour denotes placebo response}
  \label{fig:lotroptplot}
\end{figure}


From Figure \ref{fig:lotroptplot} it can be seen that scores of 0 or above on the optimism IAT (reflecting no to small difference in favour of positive stimuli) and high self reported optimism are associated with a greater likelihood of placebo response. 

\subsection{Machine Learning and the Placebo Response}
\label{sec:mach-learn-plac}

Given the confusing results obtained from linear statistical modelling of the placebo response, it was decided to fit a more flexible model, treating the placebo response as a two valued (Yes, No) classification task. A random forest model was used for this purpose. 

<<randomforestCV, echo=FALSE, results=hide, cache=TRUE>>=
train.ind <- sample(1:54, 40, replace=FALSE)
iatandexpfull <- Iatandexpmeasures.phys[,c("PlacResp", "Age", "Gender", "Prime", "LOTR", "MAAS", "Pill", "Cream", "Inj", "Acu", "Hom", "Rei", "OptIAT.Mean", "OptIAT.Median", "TCQIAT.Mean", "TCQIAT.Median", "meanconv", "meanalt", "convaltcomp", "lengthsurv", "gsr.mean")]
iatandexpfull <- na.omit(iatandexpfull)
iatandexpfull.train.ind <- with(iatandexpfull,createDataPartition(PlacResp, p=0.8, list=FALSE))
iatandexpfull.train <- iatandexpfull[iatandexpfull.train.ind,]
iatandexpfull.test <- iatandexpfull[-iatandexpfull.train.ind,]
rf.train <- train(PlacResp~TCQIAT.Mean+meanconv+OptIAT.Mean+LOTR, data=iatandexpfull, method="rf", preProcess=c("center", "scale"))
rf.pred <- predict(rf.train, iatandexpfull.test)

var.used.count <- varUsed(rf.train[["finalModel"]])
var.used.count <- as.data.frame(var.used.count)
rownames(var.used.count) <- c("TCQIAT.Mean", "OptIAT.Mean", "meanconv", "LOTR")

## var.used.count.m[,"variable"] <- names(iatandexpfull)
## ggplot(var.used.count.m, aes(x=variable, y=value))+geom_histogram()+coord_flip()

@ 

<<rf2, echo=FALSE, results=hide, cache=TRUE>>=
iatandexpfull.train.ind2 <- with(iatandexpfull,createDataPartition(PlacResp, p=0.8, list=FALSE))
iatandexpfull.train2 <- iatandexpfull[iatandexpfull.train.ind2,]
iatandexpfull.test2 <- iatandexpfull[-iatandexpfull.train.ind2,]
rf.train2 <- train(PlacResp~., data=iatandexpfull.train2, method="rf", metric="Kappa", maximise=TRUE)
rf.pred2 <- predict(rf.train2, iatandexpfull.test2)
@ 

<<rfTestResults, echo=FALSE, results=tex>>=
test.xtab <- xtable(confusionMatrix(rf.pred, iatandexpfull.test$PlacResp)$table, label="tab:rfplac", caption="Random Forest Accuracy on Placebo Response (Test set)")
print(test.xtab)
@ 

As can be seen from Table \ref{tab:rfplac}, the random forest predicted the unseen data for the placebo response exceptionally well, with an accuracy of 1 ($95CI 0.7684-1$), and was significantly better than a random prediction ($p=0.002059$). This suggests that the major hypothesis of the thesis, that implicit measures would be a useful predictor of the placebo response was accurate. This subject is covered further in the discussion. 

Other models which were fit to the data included a naive bayes classifier ($Acc=0.5714$), a linear support vector machine ($Acc=0.6439$), an an evolutionary tree model ($Acc=0.7857$). Of these, only the evolutionary tree performed above chance when predicting positive placebo response (all models predicted no placebo response correctly). This may be, as the plots above in section \ref{sec:vari-impact-plac} showed, the relationship appeared to be nonlinear, in that moderate levels of the three predictor variables appeared to be more highly associated with positive response to placebo. Tree models allow the data to be split on particular levels of a predictor variable, and this flexibility allows placebos to be predicted more accurately. 

\begin{figure}
<<varImpPlot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
sink ("tmp.txt")
par(mfrow=c (1,1))
print(varImpPlot(rf.train[["finalModel"]]))
sink (NULL)
@   
  \caption{Variable Importance Plot for Random Forest Model on Placebo Response}
  \label{fig:varimprf}
\end{figure}

As can be seen from Figure \ref{fig:varimprf}, all three variables were important to the fit, however the TCQ IAT appeared to be the most important, followed by the mean credibility score for conventional treatments, followed by the Optimism IAT, followed by scores on the LOT-R.


<<evtree, echo=FALSE, results=hide, cache=TRUE>>=
evtree.train <- train(PlacResp~OptIAT.Mean+TCQIAT.Mean+meanconv, data=iatandexpfull.train, method="evtree", preProcess=c("center", "scale"))
evtree.pred <- predict(evtree.train, iatandexpfull.test)
evtree.confusionmat <- confusionMatrix(evtree.pred, iatandexpfull.test$PlacResp)
@ 

To have some kind of graphical representation of how the tree works, the evolutionary tree final model on the training set was plotted. As can be seen from Figure \ref{fig:evtreeplot}, the Treatment Credibility IAT was responsible for most of the predictive power of the model. As hypothesised earlier, it appears that a particular level of score on the TCQIAT was associated with a positive response to placebo. Note that the variables on the plot are scaled and centered, so this cannot be read off as the particular scores on the TCQ-IAT associated with placebo response. 

%% \begin{figure}
<<evtreeplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE, eval=FALSE>>=
print(plot(evtree.train$finalModel))
@   
%%   \caption{Evolutionary Tree plot for Prediction of Placebo Response}
%%   \label{fig:evtreeplot}
%% \end{figure}




\subsection{Analysis of Physiological Data}






<<gsrcollate, echo=FALSE, results=hide>>=
gsrwithpred <- merge(gsr.df, Iatandexpmeasures, by="Participant")
@ 

<<gsrbycond, echo=FALSE, results=hide>>=
gsrdeceptive <- gsrwithpred[with(gsrwithpred,Condition=="Treatment"), ]
gsropen <- gsrwithpred[with(gsrwithpred,Condition=="Placebo"), ]
gsrnotreat <- gsrwithpred[with(gsrwithpred,Condition=="No Treatment"), ]
gsrplacyes <- gsrwithpred[with(gsrwithpred,PlacResp=="Yes"),]
gsrplacno <- gsrwithpred[with(gsrwithpred,PlacResp=="No"),]
@ 

Before the analysis of GSR data was conducted, the mean and median GSR per group were scaled to ensure that they were directly comparable. Scaling was performed using a z-score method, where each observations value was subtracted from the mean and divided by the standard deviation of all the observations. 


<<gsrmeans, echo=FALSE, results=hide>>=
deceptivemeangsr <- lapply(gsrdeceptive[,2:31151], mean, na.rm=TRUE)
openmeangsr <- lapply(gsropen[,2:31151], mean, na.rm=TRUE)
notreatmeangsr <- lapply(gsrnotreat[,2:31151], mean, na.rm=TRUE)
meangsr <- as.data.frame(cbind(deceptivemeangsr,openmeangsr, notreatmeangsr))
meangsr <- lapply(meangsr, as.numeric )
meangsr <- as.data.frame(lapply(meangsr, scale))
meangsr[,"Time"] <- 1:nrow(meangsr)
@ 
\begin{figure}
<<meanplotgsr, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
par(mfrow=c(1,3))
plot(as.ts(na.omit(meangsr[,1:3])), main="Mean GSR over time")
@   
  \caption{Mean GSR Levels by Condition over Time}
  \label{fig:meangsr}
\end{figure}


The results shown above in Figure \ref{fig:meangsr} are quite unexpected, but fit with the strange pain ratings observed in this experiment. It can be seen that the Open Placebo group had the lowest GSR over time (middle plot), and that the deceptive pain group seemed to have the highest GSR throughout the experiment. The No Treatment group chart is perhaps the strangest, showing a steady upward trend until about 25 minutes into the experiment, and then falling steadily from there to reach (at 30000, or 45 minutes after the pain was induced) approximately the level which it began. This may have occurred due to habituation to the experimental environment, as they had no experimental manipulation during the course of the study which might have changed their GSR. In contrast, the Open Placebo group show a low GSR starting off which then rises slowly, dips and then rises again. The course of GSR in the Deceptive Placebo group looks like what would have been expected from the No Treatment group; i.e. a slow and steady rise throughout the experiment.

<<placmodgsr, echo=FALSE, results=tex>>=
placmod10 <- glm(PlacResp~gsr.mean, data=Iatandexpmeasures.phys, family=binomial(link="logit")) #extremely significant!
print(xtable(summary(placmod10), label="tab:placmodgsr", caption="Logistic Regression for the Impact of Mean GSR measurements on placebo response"))
@ 

As can be seen from Table \ref{tab:placmodgsr}, the mean level of skin response was significantly associated with the response to placebo. 

<<gsrmedians, echo=FALSE, results=hide>>=
deceptivemediangsr <- lapply(gsrdeceptive[,2:35151], median, na.rm=TRUE)
openmediangsr <- lapply(gsropen[,2:35151], median, na.rm=TRUE)
notreatmediangsr <- lapply(gsrnotreat[,2:35151], median, na.rm=TRUE)
medgsr <- as.data.frame(cbind(deceptivemediangsr, openmediangsr, notreatmediangsr))

medgsr <- lapply(medgsr, as.numeric)
medgsr <- as.data.frame(medgsr)
medgsr <- as.data.frame(lapply(medgsr, scale))
@ 
\begin{figure}
<<gsrmedianplot, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
plot(as.ts(na.omit(medgsr)), main="Median GSR Over Time", ylab="Time")
@   
  \caption{Median GSR by Condition Over Time}
  \label{fig:mediangsr}
\end{figure}


Again, the same pattern in GSR is seen in Figure \ref{fig:mediangsr} which suggests that the mean results shown in Figure \ref{fig:meangsr} are not the result of a small number of anomalous observations. 

<<ggplotgsr, echo=FALSE, fig=TRUE,pdf=TRUE, eps=TRUE, png=TRUE>>=
meangsr.melt <- melt(meangsr, id="Time")
meangsrplot <- ggplot(meangsr.melt, aes(x=Time, y=value))+geom_line()+facet_grid(variable~.)
print(meangsrplot)
@ 


\subsection{Examining the Effect of Pain on GSR}
\label{sec:exam-effect-pain}

The next step in the analysis was to examine the impact of the particular painful stimuli administered on skin conductance. Some preliminary research has suggested that this may be a useful predictor of painful stimuli, and this question will be covered in the following section. However, in this section, the impact of the painful stimuli on skin conductance will be assessed. Note that due to experimenter error, not all participants had the band to induce pain at 300s into the experiment. This time of administration was recorded, and additionally as participants squeezed either more slowly or more quickly, there was some variability in these times. 

The first step in this analysis was to examine the size of these disparities. 

<<bandagequeeze, echo=FALSE, results=hide>>=
bandage.squeeze <- Iatandexpmeasures [,c ("Participant", "BandageOn", "SqueezStop")]
bandage.squeeze [,"SqueezingTime"] <- with (bandage.squeeze, SqueezStop-BandageOn)
@ 
 

\begin{figure}
<<bandonoff, echo=FALSE, fig=TRUE>>=
band.m <- melt (bandage.squeeze [,1:3], id.vars="Participant")
print (ggplot (band.m, aes (x=value, fill=variable))+geom_density (alpha=0.5))
@ 
  \caption{Density Plots for Time of Application of Bandage and Stopping Squeezing}
  \label{fig:onoffplot}
\end{figure}

Density Plots for the time of application of bandage and when the participants stopped squeezing are shown in Figure \ref{fig:onoffplot}. Note that the shape of the time of application is far more regular than that for the  stopping squeezing, suggesting that inter-participant variability was responsible for the majority of observed differences. 

\begin{figure}
<<squeezeplot, echo=FALSE, fig=TRUE>>=
squeeez.pl <- ggplot (bandage.squeeze, aes (x=SqueezingTime))+geom_density ()
print (squeeez.pl)
@ 
  \caption{Density Plot of Time Spent Squeezing the Hand Exerciser}
  \label{fig:squeezplot}
\end{figure}


As can be seen from Figure \ref{fig:squeezplot}, the majority of participants spent approximately between 30 and 50 seconds squeezing the hand exercisor, which seems reasonable given that they were asked to squeeze it for 40 seconds. 

<<physlen, echo=FALSE, results=hide>>=
physlen <- read.table ("./ExperimentDataforR/FullStudy/PhysMeasures/Richieoutput/FullTimePhys2.txt", header=FALSE)
names (physlen) <- c ("Time", "Participant")
physlen [,"Participant"] <- with (physlen, as.factor (Participant))
survlength.ratings <- Iatandexpmeasures [,c ("Participant","BandageOn", "lengthsurv")]
len.test <- merge (physlen, survlength.ratings, by="Participant")
len.test [,"Time"] <- with (len.test, Time/1000)
len.test [,"Diff"] <- with (len.test, Time-lengthsurv)
@ 

Another sanity check was then performed on the datasets to ensure that all data had been recorded correctly. This was the examination of the lengths of the physiological recordings versus the length of the recordings of pain ratings. While the physiological data should be more accurate here (as it was recorded automatically and at a high resolution) any major differences would be cause for elimination of the affected records. 


<<physpaindiffplot, echo=FALSE, fig=TRUE, eval=FALSE>>=
physpaindiff.pl <- ggplot (len.test, aes (x=Diff))+geom_density ()
print (physpaindiff.pl)
@ 

\section{Discussion} 

A number of caveats are in order here. Firstly, neither the optimism IAT or the treatment credibility IAT were independently predictive of placebo response. However, when the model included an interaction between them, all three of these variables were significant (as in the model shown above). This may indicate that there may be some irrelevant (from the conventional scoring perspective) feature of the IAT measures which was predictive of placebo response in this sample. This is a matter which can be teased out by future research. 

Secondly, given the number of models fitted, some were almost certain to come up as significant, and the three way interaction between the two IAT measures and the Life Orientation Test was not a hypothesis of the research. 

It might be questioned why the Condition variable was not included in the model - it was, but it was dropped as its presence caused errors in the model fit. A little thought explains why this is so - placebo response was only possible in 2 of 3 conditions, and Condition acted as a proxy for these, thus causing issues with the model fit (in essence, Condition was collinear with placebo response). The final model shown above was superior to the other models considered, in terms of AIC and other model fit indices. 


%%% Local Variables:
%%% TeX-master: "PlaceboMeasurementByMultipleMethods"
%%% End:
